{"title": "Stable Diffusion", "page_id": 71642695, "revision_id": 1157318594, "revision_timestamp": "2023-05-27T21:23:37Z", "content": "{{Short description|Image-generating machine learning model}}\n{{Infobox software\n| name = Stable Diffusion\n| logo = \n| logo caption = \n| screenshot = A photograph of an astronaut riding a horse 2022-08-28.png\n| screenshot size = 250px\n| caption = An image generated by Stable Diffusion based on the text prompt \"a photograph of an astronaut riding a horse\"\n| author = Runway, CompVis, and Stability AI\n| developer = Stability AI\n| released = August 22, 2022\n| latest release version = 2.1 (model)<ref name=\"release2.1\">{{cite web|url=https://stability.ai/blog/stablediffusion2-1-release7-dec-2022|title=Stable Diffusion v2.1 and DreamStudio Updates 7-Dec 22|website=stability.ai|archive-date=December 10, 2022|archive-url=https://web.archive.org/web/20221210062732/https://stability.ai/blog/stablediffusion2-1-release7-dec-2022|url-status=live}}</ref>\n| latest release date = December 7, 2022\n| repo = {{url|https://github.com/Stability-AI/stablediffusion}}\n| programming language = [[Python (programming language)|Python]]<ref>{{cite web |author1 = Ryan O'Connor | title = How to Run Stable Diffusion Locally to Generate Images | url = https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/ | access-date = May 4, 2023 | date = August 23, 2022}}</ref>\n| operating system = Any that support [[CUDA]] [[Compute kernel|kernel]]s\n| genre = [[Text-to-image model]]\n| license = Creative ML OpenRAIL-M\n| website = \n}}\n'''Stable Diffusion''' is a [[deep learning]], [[text-to-image model]] released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as [[inpainting]], outpainting, and generating image-to-image translations guided by a [[prompt engineering|text prompt]].<ref name=\":0\">{{Cite web|title=Diffuse The Rest - a Hugging Face Space by huggingface|url=https://huggingface.co/spaces/huggingface/diffuse-the-rest|access-date=2022-09-05|website=huggingface.co|archive-date=2022-09-05|archive-url=https://web.archive.org/web/20220905141431/https://huggingface.co/spaces/huggingface/diffuse-the-rest|url-status=live }}</ref> It was developed by the start-up Stability AI in collaboration with a number of academic researchers and non-profit organizations.<ref name=\"stable-diffusion-launch\"/>\n\nStable Diffusion is a [[Latent variable model|latent]] [[diffusion model]], a kind of deep generative [[neural network]]. Its code and model weights have been released [[Source-available software|publicly]],<ref name=\"stable-diffusion-github\"/> and it can run on most consumer hardware equipped with a modest [[Graphics processing unit|GPU]] with at least 8&nbsp;GB [[Video random access memory|VRAM]]. This marked a departure from previous proprietary text-to-image models such as [[DALL-E]] and [[Midjourney]] which were accessible only via [[cloud service]]s.<ref name=\"pcworld\">{{cite web|title=The new killer app: Creating AI art will absolutely crush your PC|url=https://www.pcworld.com/article/916785/creating-ai-art-local-pc-stable-diffusion.html|access-date=2022-08-31|website=PCWorld|archive-date=2022-08-31|archive-url=https://web.archive.org/web/20220831065139/https://www.pcworld.com/article/916785/creating-ai-art-local-pc-stable-diffusion.html|url-status=live }}</ref><ref name=\"verge\"/>\n\n==Development==\nThe development of Stable Diffusion was funded and shaped by the start-up company Stability AI.<ref name=verge/><ref name=CNN-Getty/><ref name=\"MIT-LAION\"/>\nThe technical license for the model was released by the CompVis group at [[Ludwig Maximilian University of Munich]].<ref name=verge/> Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion.<ref name=\"stable-diffusion-launch\"/> Stability AI also credited [[EleutherAI]] and [[LAION]] (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.<ref name=\"stable-diffusion-launch\"/>\n\nIn October 2022, Stability AI raised US$101&nbsp;million in a round led by [[Lightspeed Venture Partners]] and [[Coatue Management]].<ref>{{Cite web|last=Wiggers|first=Kyle|title=Stability AI, the startup behind Stable Diffusion, raises $101M|url=https://techcrunch.com/2022/10/17/stability-ai-the-startup-behind-stable-diffusion-raises-101m/|access-date=2022-10-17|website=Techcrunch|date=17 October 2022|language=en}}</ref>\n\n==Technology==\n[[File:Stable Diffusion architecture.png|thumb|right|upright=1.3|Diagram of the latent diffusion architecture used by Stable Diffusion]]\n[[File:X-Y plot of algorithmically-generated AI art of European-style castle in Japan demonstrating DDIM diffusion steps.png|right|thumb|300px|The denoising process used by Stable Diffusion. The model generates images by iteratively denoising random noise until a configured number of steps have been reached, guided by the CLIP text encoder pretrained on concepts along with the attention mechanism, resulting in the desired image depicting a representation of the trained concept.]]\n===Architecture===\nStable Diffusion uses a kind of [[diffusion model]] (DM), called a latent diffusion model (LDM) developed by the CompVis group at [[LMU Munich]].<ref name=\"paper\" /><ref name=\"stable-diffusion-github\">{{cite web|title=Stable Diffusion Repository on GitHub|url=https://github.com/CompVis/stable-diffusion|publisher=CompVis - Machine Vision and Learning Research Group, LMU Munich|access-date=17 September 2022|date=17 September 2022}}</ref> Introduced in 2015, diffusion models are trained with the objective of removing successive applications of [[Gaussian noise]] on training images, which can be thought of as a sequence of [[denoising autoencoder]]s. Stable Diffusion consists of 3 parts: the [[variational autoencoder]] (VAE), [[U-Net]], and an optional text encoder.<ref name=\":02\">{{Cite web|last=Alammar|first=Jay|title=The Illustrated Stable Diffusion|url=https://jalammar.github.io/illustrated-stable-diffusion/|access-date=2022-10-31|website=jalammar.github.io}}</ref> The VAE encoder compresses the image from pixel space to a smaller dimensional [[latent space]], capturing a more fundamental semantic meaning of the image.<ref name=paper/> Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.<ref name=\":02\" /> The U-Net block, composed of a [[Residual neural network|ResNet]] backbone, [[Noise reduction|denoises]] the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.<ref name=\":02\" /> The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a [[Attention (machine learning)|cross-attention mechanism]].<ref name=\":02\" /> For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.<ref name=\"stable-diffusion-github\" /> Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.<ref name=\"stable-diffusion-launch\"/><ref name=paper/>\n\n=== Training data ===\nStable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from [[Common Crawl]] data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted \"aesthetic\" score (e.g. subjective visual quality).<ref name=\"Waxy\">{{Cite web|last=Baio|first=Andy|date=2022-08-30|title=Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion's Image Generator|url=https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/|access-date=2022-11-02|website=Waxy.org|language=en-US}}</ref> The dataset was created by [[LAION]], a German non-profit which receives funding from Stability AI.<ref name=\"Waxy\" /><ref>{{Cite web|title=This artist is dominating AI-generated art. And he's not happy about it.|url=https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/|access-date=2022-11-02|website=MIT Technology Review|language=en}}</ref> The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+.<ref name=\"Waxy\" /> A third-party analysis of the model's training data identified that out of a smaller subset of 12&nbsp;million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with [[Pinterest]] taking up 8.5% of the subset, followed by websites such as [[WordPress]], [[Blogspot]], [[Flickr]], [[DeviantArt]] and [[Wikimedia Commons]].<ref>{{Cite web|last=Ivanovs|first=Alex|date=2022-09-08|title=Stable Diffusion: Tutorials, Resources, and Tools|url=https://stackdiary.com/stable-diffusion-resources/|access-date=2022-11-02|website=Stack Diary|language=en-US}}</ref><ref name=\"Waxy\" />\n\n=== Training procedures ===\nThe model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600&nbsp;million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them.<ref>{{Citation|last=Schuhmann|first=Christoph|title=CLIP+MLP Aesthetic Score Predictor|date=2022-11-02|url=https://github.com/christophschuhmann/improved-aesthetic-predictor|access-date=2022-11-02}}</ref><ref name=\"Waxy\" /><ref name=\"LAION-Aesthetics\">{{Cite web|title=LAION-Aesthetics {{!}} LAION|url=https://laion.ai/blog/laion-aesthetics|access-date=2022-09-02|website=laion.ai|language=en|archive-date=2022-08-26|archive-url=https://web.archive.org/web/20220826121216/https://laion.ai/blog/laion-aesthetics/|url-status=live }}</ref> The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a [[watermark]] with greater than 80% probability.<ref name=\"Waxy\" /> Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.<ref name=\":5\">{{cite arXiv|last1=Ho|first1=Jonathan|last2=Salimans|first2=Tim|date=2022-07-25|title=Classifier-Free Diffusion Guidance|class=cs.LG|eprint=2207.12598 }}</ref>\n\nThe model was trained using 256 [[Ampere (microarchitecture)|Nvidia A100]] GPUs on [[Amazon Web Services]] for a total of 150,000 GPU-hours, at a cost of $600,000.<ref>{{Cite web|last=Mostaque|first=Emad|date=August 28, 2022|title=Cost of construction|url=https://twitter.com/emostaque/status/1563870674111832066|access-date=2022-09-06|website=Twitter|language=en|archive-date=2022-09-06|archive-url=https://web.archive.org/web/20220906155426/https://twitter.com/EMostaque/status/1563870674111832066|url-status=live }}</ref><ref name=\"stable-diffusion-model-card-1-4\"/><ref>{{Cite web|last=Wiggers|first=Kyle|date=2022-08-12|title=A startup wants to democratize the tech behind DALL-E 2, consequences be damned|url=https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/|access-date=2022-11-02|website=TechCrunch|language=en-US}}</ref>\n\n=== Limitations ===\nStable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512\u00d7512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its \"expected\" 512\u00d7512 resolution;<ref name=\"diffusers\">{{Cite web|title=Stable Diffusion with \ud83e\udde8 Diffusers|url=https://huggingface.co/blog/stable_diffusion|access-date=2022-10-31|website=huggingface.co}}</ref> the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768\u00d7768 resolution.<ref name=\"release2.0\">{{cite web|url=https://stability.ai/blog/stable-diffusion-v2-release|title=Stable Diffusion 2.0 Release|website=stability.ai|archive-date=December 10, 2022|archive-url=https://web.archive.org/web/20221210062729/https://stability.ai/blog/stable-diffusion-v2-release|url-status=live}}</ref> Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database.<ref>{{Cite web|title=LAION|url=https://laion.ai/|access-date=2022-10-31|website=laion.ai|language=en}}</ref> The model is insufficiently trained to understand human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model.<ref>{{Cite web|date=2022-08-24|title=Generating images with Stable Diffusion|url=https://blog.paperspace.com/generating-images-with-stable-diffusion/|access-date=2022-10-31|website=Paperspace Blog|language=en}}</ref>\n\nAccessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating [[anime]] characters (\"waifu diffusion\"),<ref>{{Cite web|title=hakurei/waifu-diffusion \u00b7 Hugging Face|url=https://huggingface.co/hakurei/waifu-diffusion|access-date=2022-10-31|website=huggingface.co}}</ref> new data and further training are required. [[fine-tuning (machine learning)|Fine-tuned]] adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging<ref>{{cite arXiv|first1=Pierre|last1=Chambon|first2=Christian|last2=Bluethgen|first3=Curtis P.|last3=Langlotz|first4=Akshay|last4=Chaudhari|date=2022-10-09|title=Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains|class=cs.CV|eprint=2210.04133}}</ref> to [[Riffusion|algorithmically-generated music]].<ref>{{cite web|author=Seth Forsgren|author2=Hayk Martiros|url=https://www.riffusion.com/about|title=Riffusion - Stable diffusion for real-time music generation|website=Riffusion|archive-url=https://web.archive.org/web/20221216092717/https://www.riffusion.com/about|archive-date=December 16, 2022|url-status=live}}</ref> However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30&nbsp;GB of [[VRAM]],<ref>{{Citation|last=Mercurio|first=Anthony|title=Waifu Diffusion|date=2022-10-31|url=https://github.com/harubaru/waifu-diffusion/blob/6bf942eb6368ebf6bcbbb24b6ba8197bda6582a0/docs/en/training/README.md|access-date=2022-10-31}}</ref> which exceeds the usual resource provided in such consumer GPUs as [[Nvidia]]'s [[GeForce 30 series]], which has only about 12&nbsp;GB.<ref>{{Cite web|last=Smith|first=Ryan|title=NVIDIA Quietly Launches GeForce RTX 3080 12GB: More VRAM, More Power, More Money|url=https://www.anandtech.com/show/17204/nvidia-quietly-launches-geforce-rtx-3080-12gb-more-vram-more-power-more-money|access-date=2022-10-31|website=www.anandtech.com}}</ref>\n\nThe creators of Stable Diffusion acknowledge the potential for [[algorithmic bias]], as the model was primarily trained on images with English descriptions.<ref name=\"stable-diffusion-model-card-1-4\">{{Cite web|title=CompVis/stable-diffusion-v1-4 \u00b7 Hugging Face|url=https://huggingface.co/CompVis/stable-diffusion-v1-4|access-date=2022-11-02|website=huggingface.co}}</ref> As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.<ref name=\"stable-diffusion-model-card-1-4\" />\n\n=== End-user fine-tuning ===\nTo address the limitations of the model's initial training, end-users may opt to implement additional training to [[fine-tuning (machine learning)|fine-tune]] generation outputs to match more specific use-cases. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:\n*An \"embedding\" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt.<ref>{{cite web|author=Dave James|date=October 28, 2022|url=https://www.pcgamer.com/nvidia-rtx-4090-stable-diffusion-training-aharon-kahana/|title=I thrashed the RTX 4090 for 8 hours straight training Stable Diffusion to paint like my uncle Hermann|website=[[PC Gamer]]|archive-url=https://web.archive.org/web/20221109154310/https://www.pcgamer.com/nvidia-rtx-4090-stable-diffusion-training-aharon-kahana/|archive-date=November 9, 2022|url-status=live}}</ref> Embeddings are based on the \"textual inversion\" concept developed by researchers from [[Tel Aviv University]] in 2022 with support from [[Nvidia]], where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.<ref>{{cite arXiv|first1=Rinon|last1=Gal|first2=Yuval|last2=Alaluf|first3=Yuval|last3=Atzmon|first4=Or|last4=Patashnik|first5=Amit H.|last5=Bermano|first6=Gal|last6=Chechik|first7=Daniel|last7=Cohen-Or|date=2022-08-02|title=An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion|class=cs.CV|eprint=2208.01618}}</ref>\n*A \"hypernetwork\" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by [[NovelAI]] developer Kurumuz in 2021, originally intended for text-generation [[Transformer (machine learning model)|transformer models]]. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.<ref>{{cite web|date=October 11, 2022|url=https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac|title=NovelAI Improvements on Stable Diffusion|website=NovelAI|archive-url=https://archive.vn/x9zHS|archive-date=October 27, 2022|url-status=live}}</ref>\n*[[DreamBooth]] is a deep learning generation model developed by researchers from [[Google|Google Research]] and [[Boston University]] in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject.<ref>{{cite web|author=Yuki Yamashita|date=September 1, 2022|url=https://www.itmedia.co.jp/news/articles/2209/01/news041.html|title=\u611b\u72ac\u306e\u5408\u6210\u753b\u50cf\u3092\u751f\u6210\u3067\u304d\u308bAI \u6587\u7ae0\u3067\u6307\u793a\u3059\u308b\u3060\u3051\u3067\u30b3\u30b9\u30d7\u30ec \u7c73Google\u304c\u958b\u767a|website=ITmedia Inc.|language=ja|archive-url=https://web.archive.org/web/20220831232021/https://www.itmedia.co.jp/news/articles/2209/01/news041.html|archive-date=August 31, 2022|url-status=live}}</ref>\n\n== Capabilities ==\nThe Stable Diffusion model supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.<ref name=\"stable-diffusion-github\"/> Existing images can be re-drawn by the model to incorporate new elements described by a text prompt (a process known as \"guided image synthesis\"<ref>{{cite arXiv|date=August 2, 2021|first1=Chenlin|last1=Meng|first2=Yutong|last2=He|first3=Yang|last3=Song|first4=Jiaming|last4=Song|first5=Jiajun|last5=Wu|first6=Jun-Yan|last6=Zhu|first7=Stefano|last7=Ermon|title=SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations|class=cs.CV|eprint=2108.01073}}</ref>) through its diffusion-denoising mechanism.<ref name=\"stable-diffusion-github\"/> In addition, the model also allows the use of prompts to partially alter existing images via [[inpainting]] and outpainting, when used with an appropriate user interface that supports such features, of which numerous different open source implementations exist.<ref name=\"webui_showcase\">{{cite web|url=https://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase|title=Stable Diffusion web UI|website=GitHub|date=10 November 2022 }}</ref>\n\nStable Diffusion is recommended to be run with 10&nbsp;GB or more VRAM, however users with less VRAM may opt to load the weights in [[float16]] precision instead of the default [[float32]] to tradeoff model performance with lower VRAM usage.<ref name=\"diffusers\" />\n\n=== Text to image generation ===\n{{multiple image\n| direction         = vertical\n| align             = right\n| total_width       = 200\n| image1            = Algorithmically-generated landscape artwork of forest with Shinto shrine.png\n| image2            = Algorithmically-generated landscape artwork of forest with Shinto shrine using negative prompt for green trees.png\n| image3            = Algorithmically-generated landscape artwork of forest with Shinto shrine using negative prompt for round stones.png\n| footer            = Demonstration of the effect of negative prompts on image generation\n*'''Top''': no negative prompt\n*'''Centre''': \"green trees\"\n*'''Bottom''': \"round stones, round rocks\"\n}}\nThe text to image sampling script within Stable Diffusion, known as \"txt2img\", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt.<ref name=\"stable-diffusion-github\" /> Generated images are tagged with an invisible [[digital watermark]] to allow users to identify an image as generated by Stable Diffusion,<ref name=\"stable-diffusion-github\" /> although this watermark loses its efficacy if the image is resized or rotated.<ref>{{Citation|title=invisible-watermark|date=2022-11-02|url=https://github.com/ShieldMnt/invisible-watermark/blob/9802ce3e0c3a5ec43b41d503f156717f0c739584/README.md|publisher=Shield Mountain|access-date=2022-11-02}}</ref>\n\nEach txt2img generation will involve a specific [[Random seed|seed value]] which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image.<ref name=\"diffusers\" /> Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects.<ref name=\"diffusers\" /> Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt.<ref name=\":5\" /> More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.<ref name=\"diffusers\" />\n\nAdditional text2img features are provided by [[Frontend and backend|front-end]] implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets.<ref>{{Cite web|title=stable-diffusion-tools/emphasis at master \u00b7 JohannesGaessler/stable-diffusion-tools|url=https://github.com/JohannesGaessler/stable-diffusion-tools|access-date=2022-11-02|website=GitHub|language=en}}</ref> An alternative method of adjusting weight to parts of the prompt are \"negative prompts\". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.<ref name=\"webui_showcase\" /><ref name=\"release2.1\"/>\n\n=== Image modification ===\nStable Diffusion also includes another sampling script, \"img2img\", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.<ref name=\"stable-diffusion-github\" />\n\nThe ability of img2img to add noise to the original image makes it potentially useful for [[data anonymization]] and [[data augmentation]], in which the visual features of image data are changed and anonymized.<ref name=\":1\">{{cite arXiv|last1=Luzi|first1=Lorenzo|last2=Siahkoohi|first2=Ali|last3=Mayer|first3=Paul M.|last4=Casco-Rodriguez|first4=Josue|last5=Baraniuk|first5=Richard|date=2022-10-21|title=Boomerang: Local sampling on image manifolds using diffusion models|class=cs.CV|eprint=2210.12100 }}</ref> The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image.<ref name=\":1\" /> Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to [[JPEG]] and [[WebP]], the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.<ref>{{Cite web|last=B\u00fchlmann|first=Matthias|date=2022-09-28|title=Stable Diffusion Based Image Compression|url=https://pub.towardsai.net/stable-diffusion-based-image-compresssion-6f1f0a399202|access-date=2022-11-02|website=Medium|language=en}}</ref>\n\nAdditional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided [[Layers (digital image editing)#Layer mask|layer mask]], which fills the masked space with newly generated content based on the provided prompt.<ref name=\"webui_showcase\" /> A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0.<ref name=\"release2.0\"/> Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.<ref name=\"webui_showcase\" />\n\nA depth-guided model, named \"depth2img\", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the [[depth map|depth]] of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.<ref name=\"release2.0\"/>\n\n=== ControlNet ===\nControlNet<ref name=\"controlnet-paper\">{{Cite arXiv|title=Adding Conditional Control to Text-to-Image Diffusion Models|last=Zhang|first=Lvmin|date=10 February 2023|class=cs.CV |eprint=2302.05543 }}</ref> is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" copy learns the desired condition, while the \"locked\" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The \"zero convolution\" is a 1\u00d71 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.\n\n==Usage and controversy==\nStable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals. The freedom provided to users over image usage has caused controversy over the ethics of ownership, as Stable Diffusion and other generative models are trained from copyrighted images without the owner\u2019s consent.<ref name=\":13\">{{Cite web|last=Cai|first=Kenrick|title=Startup Behind AI Image Generator Stable Diffusion Is In Talks To Raise At A Valuation Up To $1 Billion|url=https://www.forbes.com/sites/kenrickcai/2022/09/07/stability-ai-funding-round-1-billion-valuation-stable-diffusion-text-to-image/|access-date=2022-10-31|website=Forbes|language=en}}</ref>\n\nAs [[Style (visual arts)|visual style]]s and [[Composition (visual arts)|composition]]s are not subject to copyright, it is often interpreted that users of Stable Diffusion who generate images of artworks should not be considered to be infringing upon the copyright of visually similar works.<ref name=\"automaton\" /> However, individuals depicted in generated images may be protected by [[personality rights]] if their likeness is used,<ref name=\"automaton\">{{cite web|date=August 24, 2022|url=https://automaton-media.com/articles/newsjp/20220824-216074/|title=\u9ad8\u6027\u80fd\u753b\u50cf\u751f\u6210AI\u300cStable Diffusion\u300d\u7121\u6599\u30ea\u30ea\u30fc\u30b9\u3002\u300ckawaii\u300d\u307e\u3067\u3082\u7406\u89e3\u3057\u5275\u9020\u3059\u308b\u753b\u50cf\u751f\u6210AI|website=Automaton Media|language=ja}}</ref> and [[intellectual property]] such as recognizable brand logos still remain protected by copyright. Nonetheless, visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.<ref name=\"MIT-LAION\" />\n\nStable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.<ref name=\"bijapan\">{{cite web|author=Ryo Shimizu|date=August 26, 2022|url=https://www.businessinsider.jp/post-258369|title=Midjourney\u3092\u8d85\u3048\u305f\uff1f \u7121\u6599\u306e\u4f5c\u753bAI\uff62 #StableDiffusion \uff63\u304c\uff62AI\u3092\u6c11\u4e3b\u5316\u3057\u305f\uff63\u3068\u65ad\u8a00\u3067\u304d\u308b\u7406\u7531|website=Business Insider Japan|language=ja}}</ref> Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, Emad Mostaque, argues that \"[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology\",<ref name=\"verge\" /> and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences.<ref name=\"verge\" /> In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis.<ref name=\"verge\" /><ref name=\"bijapan\" /> This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.<ref name=\":13\"/>\n\n==Litigation==\nIn January of 2023, three artists: [[Sarah Andersen]], Kelly McKernan, and Karla Ortiz filed a [[copyright infringement]] lawsuit against Stability AI, [[Midjourney]], and [[DeviantArt]], claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.<ref>[https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart James Vincent \"AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit\" The Verge, 16 January, 2023.]</ref> The same month, Stability AI was also sued by [[Getty Images]] for using its images in the training data.<ref name=CNN-Getty>{{Cite web |last=Korn |first=Jennifer |date=2023-01-17 |title=Getty Images suing the makers of popular AI art tool for allegedly stealing photos |url=https://www.cnn.com/2023/01/17/tech/getty-images-stability-ai-lawsuit/index.html |access-date=2023-01-22 |website=CNN |language=en}}</ref>\n\n== License ==\nUnlike models like [[DALL-E]], Stable Diffusion makes its [[Source-available software|source code available]],<ref name=\"stability\">{{cite web|title=Stable Diffusion Public Release|url=https://stability.ai/blog/stable-diffusion-public-release|url-status=live|archive-url=https://web.archive.org/web/20220830210535/https://stability.ai/blog/stable-diffusion-public-release|archive-date=2022-08-30|access-date=2022-08-31|website=Stability.Ai}}</ref><ref name=\"stable-diffusion-github\" /> along with the model (pretrained weights). It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M).<ref>{{Cite web |title=From RAIL to Open RAIL: Topologies of RAIL Licenses |url=https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses |access-date=2023-02-20 |website=Responsible AI Licenses (RAIL) |date=18 August 2022 |language=en-US}}</ref> The licence prohibits certain use cases, including crime, [[libel]], [[harassment]], [[doxing]], \"exploiting ... minors\", giving medical advice, automatically creating legal obligations, producing legal evidence, and \"discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] [[Anti-discrimination law|legally protected characteristics or categories]]\".<ref name=\"washingtonpost\">{{cite news|date=2022-08-30|title=Ready or not, mass video deepfakes are coming|newspaper=The Washington Post|url=https://www.washingtonpost.com/technology/2022/08/30/deep-fake-video-on-agt/|url-status=live|access-date=2022-08-31|archive-url=https://web.archive.org/web/20220831115010/https://www.washingtonpost.com/technology/2022/08/30/deep-fake-video-on-agt/|archive-date=2022-08-31}}</ref><ref>{{Cite web|title=License - a Hugging Face Space by CompVis|url=https://huggingface.co/spaces/CompVis/stable-diffusion-license|url-status=live|archive-url=https://web.archive.org/web/20220904215616/https://huggingface.co/spaces/CompVis/stable-diffusion-license|archive-date=2022-09-04|access-date=2022-09-05|website=huggingface.co}}</ref> The user owns the rights to their generated output images, and is free to use them commercially.<ref>{{cite web|author=Katsuo Ishida|date=August 26, 2022|title=\u8a00\u8449\u3067\u6307\u793a\u3057\u305f\u753b\u50cf\u3092\u51c4\u3044AI\u304c\u63cf\u304d\u51fa\u3059\u300cStable Diffusion\u300d \uff5e\u753b\u50cf\u306f\u5546\u7528\u5229\u7528\u3082\u53ef\u80fd|url=https://forest.watch.impress.co.jp/docs/review/1434893.html|website=Impress Corporation|language=ja}}</ref>\n\n==See also==\n* [[15.ai]]\n* [[Artificial intelligence art]]\n* [[Craiyon]]\n* [[Imagen (Google Brain)]]\n* [[Synthography]]\n* [[Hugging Face]]\n\n==References==\n{{reflist|refs=\n<ref name=\"MIT-LAION\">{{cite web\n|work=MIT Technology Review\n|last=Heikkil\u00e4|first=Melissa\n|date=16 September 2022\n|title=This artist is dominating AI-generated art. And he's not happy about it.\n|url=https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/\n}}</ref>\n<ref name=\"paper\">{{cite conference|last1=Rombach|last2=Blattmann|last3=Lorenz|last4=Esser|last5=Ommer|title=High-Resolution Image Synthesis with Latent Diffusion Models|conference=International Conference on Computer Vision and Pattern Recognition (CVPR)|pages=10684\u201310695|date=June 2022|location=New Orleans, LA|url=https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf|arxiv=2112.10752}}</ref>\n<ref name=\"stable-diffusion-launch\">{{cite web|url=https://stability.ai/blog/stable-diffusion-announcement|title=Stable Diffusion Launch Announcement|website=Stability.Ai|access-date=2022-09-06|archive-date=2022-09-05|archive-url=https://web.archive.org/web/20220905105009/https://stability.ai/blog/stable-diffusion-announcement|url-status=live}}</ref>\n<ref name=\"verge\">{{cite web\n|work=The Verge\n|last=Vincent|first=James\n|date=15 September 2022\n|title=Anyone can use this AI art generator \u2014 that's the risk\n|url=https://www.theverge.com/2022/9/15/23340673/ai-image-generation-stable-diffusion-explained-ethics-copyright-data\n}}</ref>\n}}\n\n==External links==\n{{Commons category}}\n*[https://huggingface.co/spaces/stabilityai/stable-diffusion Stable Diffusion Demo]\n*[https://poloclub.github.io/diffusion-explainer/ Interactive Explanation of Stable Diffusion]\n{{Differentiable computing}}\n[[Category:Deep learning software applications]]\n[[Category:Text-to-image generation]]\n[[Category:Unsupervised learning]]\n[[Category:Art controversies]]\n[[Category:Works involved in plagiarism controversies]]\n[[Category:2022 software]]\n[[Category:Open-source artificial intelligence]]"}