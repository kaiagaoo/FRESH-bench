{"title": "DALL-E", "page_id": 66303034, "revision_id": 1224460772, "revision_timestamp": "2024-05-18T15:21:51Z", "content": "{{Short description|Image-generating deep-learning model}}\n{{Use dmy dates|date=June 2022}}\n{{Infobox software\n| name = DALL\u00b7E\n| logo = DALL-E 2 Signature.svg\n| logo caption = Watermark present on DALL\u00b7E images generated on OpenAI's {{URL|https://labs.openai.com}}\n| screenshot = DALL\u00b7E 2023-10.png\n| caption = An image generated by DALL\u00b7E 3 with GPT-4 based on the text prompt \"A modern architectural building with large glass windows, situated on a cliff overlooking a serene ocean at sunset\"\n| author = \n| developer = [[OpenAI]]\n| released = {{start date and age|df=y|2021|1|5}}\n| latest release version = DALL\u00b7E 3\n| latest release date = {{start date and age|df=y|2023|8|10}}\n| repo = \n| programming language = \n| operating system = \n| genre = [[Text-to-image model]]\n| license = \n| website = {{URL|https://labs.openai.com/}}\n}}\n{{Artificial intelligence}}\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n'''DALL\u00b7E''', '''DALL\u00b7E 2''', and '''DALL\u00b7E 3''' are [[text-to-image model]]s developed by [[OpenAI]] using [[deep learning]] methodologies to generate [[digital image]]s from [[natural language]] descriptions known as \"[[Prompt (natural language)|prompts]]\".\n\nThe first version of DALL-E was announced in January 2021. In the following year, its successor DALL-E 2 was released. DALL\u00b7E 3 was released natively into [[ChatGPT]] for ChatGPT Plus and ChatGPT Enterprise customers in October 2023,<ref name=\"FSKdR\" /> with availability via OpenAI's API<ref name=\"jh5eF\" /> and \"Labs\" platform provided in early November.<ref name=\"Gt7o0\" /> Microsoft implemented the model in Bing's Image Creator tool and plans to implement it into their Designer app.<ref name=\"jKa67\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n== History and background ==\nDALL\u00b7E was revealed by OpenAI in a blog post on 5 January 2021, and uses a version of [[GPT-3]]<ref name=\"vb\" /> modified to generate images.\n\nOn 6 April 2022, OpenAI announced DALL\u00b7E 2, a successor designed to generate more realistic images at higher resolutions that \"can combine concepts, attributes, and styles\".<ref name=\"NJ4qD\" /> On 20 July 2022, DALL\u00b7E 2 entered into a beta phase with invitations sent to 1 million waitlisted individuals;<ref name=\":3\" /> users could generate a certain number of images for free every month and may purchase more.<ref name=\":4\" /> Access had previously been restricted to pre-selected users for a research preview due to concerns about [[Ethics of artificial intelligence|ethics]] and safety.<ref name=\"bYeQq\" /><ref name=\"Xa1jy\" /> On 28 September 2022, DALL\u00b7E 2 was opened to everyone and the waitlist requirement was removed.<ref name=\"BEbJV\" /> In September 2023, OpenAI announced their latest image model, DALL\u00b7E 3, capable of understanding \"significantly more nuance and detail\" than previous iterations.<ref name=\":5\" /> In early November 2022, OpenAI released DALL\u00b7E 2 as an [[API]], allowing developers to integrate the model into their own applications. [[Microsoft]] unveiled their implementation of DALL\u00b7E 2 in their Designer app and Image Creator tool included in [[Microsoft Bing|Bing]] and [[Microsoft Edge]].<ref name=\"jodXw\" /> The API operates on a cost-per-image basis, with prices varying depending on image resolution. Volume discounts are available to companies working with OpenAI's enterprise team.<ref name=\"hhWp9\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n\nThe software's name is a [[portmanteau]] of the names of animated robot [[Pixar]] character [[WALL-E (character)|WALL-E]] and the Spanish surrealist artist [[Salvador Dal\u00ed]].<ref name=\"tc\" /><ref name=\"vb\" />\n\nIn February 2024, OpenAI began adding watermarks to DALL-E generated images, containing metadata in the C2PA (Coalition for Content Provenance and Authenticity) standard promoted by the [[Content Authenticity Initiative]].<ref>{{Cite web |last=Growcoot |first=Matt |date=2024-02-08 |title=AI Images Generated on DALL-E Now Contain the Content Authenticity Tag |url=https://petapixel.com/2024/02/08/ai-images-generated-on-dall-e-now-contain-the-content-authenticity-tag/ |access-date=2024-04-04 |website=[[PetaPixel]] |language=en}}</ref>\n\n== Technology ==\nThe first [[generative pre-trained transformer]] (GPT) model was initially developed by OpenAI in 2018,<ref name=\"gpt1paper\" /> using a [[Transformer (machine learning model)|Transformer]] architecture. The first iteration, GPT-1,<ref name=\"cKHhM\" /> was scaled up to produce [[GPT-2]] in 2019;<ref name=\"gpt2paper\" /> in 2020, it was scaled up again to produce [[GPT-3]], with 175 billion parameters.<ref name=\"gpt3paper\" /><ref name=\"vb\" /><ref name=\"dallepaper\" />\n\nDALL\u00b7E's model is a [[multimodal interaction|multimodal]] implementation of GPT-3<ref name=\"impact\" /> with 12 billion parameters<ref name=\"vb\" /> which \"swaps text for pixels,\" trained on text\u2013image pairs from the Internet.<ref name=\"mittr\" /> In detail, the input to the Transformer model is a sequence of tokenized image caption followed by tokenized image patches. The image caption is in English, tokenized by [[byte pair encoding]] (vocabulary size 16384), and can be up to 256 tokens long. Each image is a 256\u00d7256 RGB image, divided into 32\u00d732 patches of 4\u00d74 each. Each patch is then converted by a discrete [[variational autoencoder]] to a token (vocabulary size 8192).\n\nDALL\u00b7E was developed and announced to the public in conjunction with CLIP (Contrastive Language-Image Pre-training).<ref name=\"mittr\" /> CLIP is a separate model based on [[zero-shot learning]] that was trained on 400 million pairs of images with text captions [[Web scraping|scraped]] from the Internet.<ref name=\"vb\" /><ref name=\"mittr\" /><ref name=\"9FmC7\" /> Its role is to \"understand and rank\" DALL\u00b7E's output by predicting which caption from a list of 32,768 captions randomly selected from the dataset (of which one was the correct answer) is most appropriate for an image. This model is used to filter a larger initial list of images generated by DALL\u00b7E to select the most appropriate outputs.<ref name=\"tc\" /><ref name=\"mittr\" />\n\nDALL\u00b7E 2 uses 3.5 billion parameters, a smaller number than its predecessor.<ref name=\":2\" /> DALL\u00b7E 2 uses a [[diffusion model]] conditioned on CLIP image embeddings, which, during inference, are generated from CLIP text embeddings by a prior model.<ref name=\":2\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n=== Contrastive Language-Image Pre-training (CLIP) ===\nContrastive Language-Image Pre-training<ref name=\"uko4t\" /> is a technique for training a pair of models. One model takes in a piece of text and outputs a single vector. Another takes in an image and outputs a single vector.\n\nTo train such a pair of models, one would start by preparing a large dataset of image-caption pairs, then sample batches of size <math>N</math>. Let the outputs from the text and image models be respectively <math>v_1, ..., v_N, w_1, ..., w_N </math>. The loss incurred on this batch is:<math display=\"block\">-\\sum_{i} \\ln\\frac{e^{v_i \\cdot w_i}}{\\sum_j e^{v_i \\cdot w_j}} -\\sum_{j} \\ln\\frac{e^{v_j \\cdot w_j}}{\\sum_i e^{v_i \\cdot w_j}} </math>In words, it is the total sum of cross-entropy loss across every column and every row of the matrix <math>[v_i \\cdot w_j]_{i, j}</math>.\n\nThe models released were trained on a dataset \"WebImageText,\" containing 400 million pairs of image-captions. The total number of words is similar to WebText, which contains about 40 GB of text.\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n\n== Capabilities ==\nDALL\u00b7E can generate imagery in multiple styles, including [[photorealistic]] imagery, [[paintings]], and [[emoji]].<ref name=\"vb\" /> It can \"manipulate and rearrange\" objects in its images,<ref name=\"vb\" /> and can correctly place design elements in novel compositions without explicit instruction. Thom Dunn writing for ''[[Boing Boing|BoingBoing]]'' remarked that \"For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL\u00b7E often draws the handkerchief, hands, and feet in plausible locations.\"<ref name=\"boing\" /> DALL\u00b7E showed the ability to \"fill in the blanks\" to infer appropriate details without specific prompts, such as adding Christmas imagery to prompts commonly associated with the celebration,<ref name=\"extreme\" /> and appropriately placed shadows to images that did not mention them.<ref name=\"engadget\" /> Furthermore, DALL\u00b7E exhibits a broad understanding of visual and design trends.{{Citation needed|date=July 2022}}\n\nDALL\u00b7E can produce images for a wide variety of arbitrary descriptions from various viewpoints<ref name=\":0\" /> with only rare failures.<ref name=\"tc\" /> Mark Riedl, an associate professor at the [[Georgia Tech]] School of Interactive Computing, found that DALL-E could blend concepts (described as a key element of human [[creativity]]).<ref name=\"cnbc\" /><ref name=\"bbc\" />\n\nIts visual reasoning ability is sufficient to solve [[Raven's Progressive Matrices|Raven's Matrices]] (visual tests often administered to humans to measure intelligence).<ref name=\"dale\" /><ref name=\"fdfao\" />\n\n[[File:DALL-E 3 AI image of an avocado speaking.png|thumb|An image of accurate text generated by DALL\u00b7E 3 based on the text prompt \"An illustration of an avocado sitting in a therapist's chair, saying 'I just feel so empty inside' with a pit-sized hole in its center. The therapist, a spoon, scribbles notes\"]]\nDALL\u00b7E 3 follows complex prompts with more accuracy and detail than its predecessors, and is able to generate more coherent and accurate text.<ref name=\"UZbGX\" /><ref name=\":5\" /> DALL\u00b7E 3 is integrated into ChatGPT Plus.<ref name=\":5\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n=== Image modification ===\n{{multiple image\n| align             = \n| width             = 130\n| image1            = DALL-E 2 variation 1.png\n| image2            = DALL-E 2 variation 2.png\n| footer            = Two \"variations\" of ''[[Girl With a Pearl Earring]]'' generated with DALL\u00b7E 2\n}}\nGiven an existing image, DALL\u00b7E 2 can produce \"variations\" of the image as individual outputs based on the original, as well as edit the image to modify or expand upon it. DALL\u00b7E 2's \"inpainting\" and \"outpainting\" use context from an image to fill in missing areas using a [[Artistic medium|medium]] consistent with the original, following a given prompt.\n\nFor example, this can be used to insert a new subject into an image, or expand an image beyond its original borders.<ref name=\"UOAWM\" /> According to OpenAI, \"Outpainting takes into account the image\u2019s existing visual elements \u2014 including shadows, reflections, and textures \u2014 to maintain the context of the original image.\"<ref name=\"6BZCE\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n\n=== Technical limitations ===\nDALL\u00b7E 2's language understanding has limits. It is sometimes unable to distinguish \"A yellow book and a red vase\" from \"A red book and a yellow vase\" or \"A panda making latte art\" from \"Latte art of a panda\".<ref name=\"uZZnn\" /> It generates images of \"an astronaut riding a horse\" when presented with the prompt \"a horse riding an astronaut\".<ref name=\"l0sAF\" /> It also fails to generate the correct images in a variety of circumstances. Requesting more than three objects, negation, numbers, and [[Conjunction (grammar)|connected sentences]] may result in mistakes, and object features may appear on the wrong object.<ref name=\":0\" /> Additional limitations include handling text \u2014 which, even with legible lettering, almost invariably results in dream-like gibberish \u2014 and its limited capacity to address scientific information, such as astronomy or medical imagery.<ref name=\"QJCbe\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n\n== Ethical concerns ==\nDALL\u00b7E 2's reliance on public datasets influences its results and leads to [[algorithmic bias]] in some cases, such as generating higher numbers of men than women for requests that do not mention gender.<ref name=\"yASFB\" /> DALL\u00b7E 2's training data was filtered to remove violent and sexual imagery, but this was found to increase bias in some cases such as reducing the frequency of women being generated.<ref name=\":1\" /> OpenAI hypothesize that this may be because women were more likely to be sexualized in training data which caused the filter to influence results.<ref name=\":1\" /> In September 2022, OpenAI confirmed to ''[[The Verge]]'' that DALL\u00b7E invisibly inserts phrases into user prompts to address bias in results; for instance, \"black man\" and \"Asian woman\" are inserted into prompts that do not specify gender or race.<ref name=\"QIhyO\" />\n\nA concern about DALL\u00b7E 2 and similar image generation models is that they could be used to propagate [[deepfake]]s and other forms of misinformation.<ref name=\"Taylor\" /><ref name=\"wired2\" /> As an attempt to mitigate this, the software rejects prompts involving public figures and uploads containing human faces.<ref name=\"vice\" /> Prompts containing potentially objectionable content are blocked, and uploaded images are analyzed to detect offensive material.<ref name=\"docs\" /> A disadvantage of prompt-based filtering is that it is easy to bypass using alternative phrases that result in a similar output. For example, the word \"blood\" is filtered, but \"ketchup\" and \"red liquid\" are not.<ref name=\"w7Syr\" /><ref name=\"docs\" />\n\nAnother concern about DALL\u00b7E 2 and similar models is that they could cause [[technological unemployment]] for artists, photographers, and graphic designers due to their accuracy and popularity.<ref name=\"X3g6f\" /><ref name=\"74NV7\" /> DALL\u00b7E 3 is designed to block users from generating art in the style of currently-living artists.<ref name=\":5\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n\n== Reception ==\n[[File:DALL-E radish.jpg|thumb|upright=1.35|Images generated by DALL\u00b7E upon the prompt: \"an illustration of a baby daikon radish in a tutu walking a dog\"]]\nMost coverage of DALL\u00b7E focuses on a small subset of \"surreal\"<ref name=\"mittr\" /> or \"quirky\"<ref name=\"cnbc\" /> outputs. DALL-E's output for \"an illustration of a baby daikon radish in a tutu walking a dog\" was mentioned in pieces from ''Input'',<ref name=\"input\" /> [[NBC]],<ref name=\"nbc\" /> ''[[Nature (journal)|Nature]]'',<ref name=\"nature\" /> and other publications.<ref name=\"vb\" /><ref name=\"wired\" /><ref name=\"cnn\" /> Its output for \"an armchair in the shape of an avocado\" was also widely covered.<ref name=\"mittr\" /><ref name=\"bbc\" />\n\n''[[ExtremeTech]]'' stated \"you can ask DALL\u00b7E for a picture of a phone or vacuum cleaner from a specified period of time, and it understands how those objects have changed\".<ref name=\"extreme\" /> ''[[Engadget]]'' also noted its unusual capacity for \"understanding how telephones and other objects change over time\".<ref name=\"engadget\" />\n\nAccording to ''[[MIT Technology Review]]'', one of OpenAI's objectives was to \"give language models a better grasp of the everyday concepts that humans use to make sense of things\".<ref name=\"mittr\" />\n\nWall Street investors have had a positive reception of DALL\u00b7E 2, with some firms thinking it could represent a turning point for a future multi-trillion dollar industry. By mid-2019, OpenAI had already received over $1 billion in funding from [[Microsoft]] and Khosla Ventures,<ref name=\"39hV1\" /><ref name=\"BoHAr\" /><ref name=\"SeRsU\" /> and in January 2023, following the launch of DALL\u00b7E 2 and ChatGPT, received an additional $10 billion in funding from Microsoft.<ref name=\"DmeBW\" />\n\nJapan's [[anime]] community has had a negative reaction to DALL\u00b7E 2 and similar models.<ref name=\"dhGoa\" /><ref name=\"1ejYV\" /><ref name=\"r9oIC\" /> Two arguments are typically presented by artists against the software. The first is that AI art is not art because it is not created by a human with intent. \"The juxtaposition of AI-generated images with their own work is degrading and undermines the time and skill that goes into their art. AI-driven image generation tools have been heavily criticized by artists because they are trained on human-made art scraped from the web.\"<ref name=\":3\" /> The second is the trouble with [[copyright law]] and data text-to-image models are trained on. OpenAI has not released information about what dataset(s) were used to train DALL\u00b7E 2, inciting concern from some that the work of artists has been used for training without permission. Copyright laws surrounding these topics are inconclusive at the moment.<ref name=\":4\" />\n\nAfter integrating DALL\u00b7E 3 into Bing Chat and ChatGPT, Microsoft and OpenAI faced criticism for excessive content filtering, with critics saying DALL\u00b7E had been \"lobotomized.\"<ref name=\"WindowsCentral\" /> The flagging of images generated by prompts such as \"man breaks server rack with sledgehammer\" was cited as evidence. Over the first days of its launch, filtering was reportedly increased to the point where images generated by some of Bing's own suggested prompts were being blocked.<ref name=\"WindowsCentral\" /><ref name=\"TechRadar\" /> ''[[TechRadar]]'' argued that leaning too heavily on the side of caution could limit DALL\u00b7E's value as a creative tool.<ref name=\"TechRadar\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n== Open-source implementations ==\nSince OpenAI has not released [[source code]] for any of the three models, there have been several attempts to create [[Open source|open-source]] models offering similar capabilities.<ref name=\"VentureBeat3\" /><ref name=\"SWLwd\" /> Released in 2022 on [[Hugging Face]]'s Spaces platform, Craiyon (formerly DALL\u00b7E Mini until a name change was requested by OpenAI in June 2022) is an AI model based on the original DALL\u00b7E that was trained on unfiltered data from the Internet. It attracted substantial media attention in mid-2022, after its release due to its capacity for producing humorous imagery.<ref name=\"CNETmini\" /><ref name=\"DailyDotmini\" /><ref name=\"Polygonmini\" />\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->\n==See also==\n* [[Artificial intelligence art]]\n* [[DeepDream]]\n* [[Imagen (Google Brain)]]\n* [[Midjourney]]\n* [[Stable Diffusion]]\n* [[Prompt engineering]]\n\n==References==\n{{reflist|refs=\n<ref name=\"impact\">{{Cite arXiv |eprint = 2102.02503 |last1 = Tamkin |first1 = Alex |last2 = Brundage |first2 = Miles |last3 = Clark |first3 = Jack |last4 = Ganguli |first4 = Deep |title = Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models |year = 2021 |class = cs.CL}}</ref>\n<ref name=\"gpt1paper\">{{Cite web\n  | url          = https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n  | title        = Improving Language Understanding by Generative Pre-Training\n  | last1        = Radford\n  | first1       = Alec\n  | last2        = Narasimhan\n  | first2       = Karthik\n  | last3        = Salimans\n  | first3       = Tim\n  | last4        = Sutskever\n  | first4       = Ilya\n  | pages        = 12\n  | publisher    = [[OpenAI]]\n  | date         = 11 June 2018\n  | access-date  = 23 January 2021\n  | archive-date = 26 January 2021\n  | archive-url  = https://web.archive.org/web/20210126024542/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n  | url-status   = live\n  }}</ref>\n<ref name=\"gpt2paper\">{{cite journal\n  | url          = https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n  | title        = Language models are unsupervised multitask learners\n  | last1        = Radford\n  | first1       = Alec\n  | last2        = Wu\n  | first2       = Jeffrey\n  | last3        = Child\n  | first3       = Rewon\n  | last4        = Luan\n  | first4       = David\n  | last5        = Amodei\n  | first5       = Dario\n  | last6        = Sutskever\n  | first6       = Ilua\n  | volume       = 1\n  | issue        = 8\n  | date         = 14 February 2019\n  | access-date  = 19 December 2020\n  | quote        = \n  | website      = cdn.openai.com\n  | archive-date = 6 February 2021\n  | archive-url  = https://web.archive.org/web/20210206183945/https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n  | url-status   = live\n  }}</ref>\n<ref name=\"gpt3paper\">{{cite arXiv | last1 = Brown | first1 = Tom B. | last2 = Mann | first2 = Benjamin | last3 = Ryder | first3 = Nick | last4 = Subbiah | first4 = Melanie | last5 = Kaplan | first5 = Jared | last6 = Dhariwal | first6 = Prafulla | last7 = Neelakantan | first7 = Arvind | last8 = Shyam | first8 = Pranav | last9 = Sastry | first9 = Girish | last10 = Askell | first10 = Amanda | last11 = Agarwal | first11 = Sandhini | last12 = Herbert-Voss | first12 = Ariel | last13 = Krueger | first13 = Gretchen | last14 = Henighan | first14 = Tom | last15 = Child | first15 = Rewon | last16 = Ramesh | first16 = Aditya | last17 = Ziegler | first17 = Daniel M. | last18 = Wu | first18 = Jeffrey | last19 = Winter | first19 = Clemens | last20 = Hesse | first20 = Christopher | last21 = Chen | first21 = Mark | last22 = Sigler | first22 = Eric | last23 = Litwin | first23 = Mateusz | last24 = Gray | first24 = Scott | last25 = Chess | first25 = Benjamin | last26 = Clark | first26 = Jack | last27 = Berner | first27 = Christopher | last28 = McCandlish | first28 = Sam | last29 = Radford | first29 = Alec | last30 = Sutskever | first30 = Ilya | last31 = Amodei | first31 = Dario | title = Language Models are Few-Shot Learners | eprint = 2005.14165 | date = July 22, 2020 | class = cs.CL}}</ref>\n<ref name=\"dallepaper\">{{cite arXiv |last1 = Ramesh |first1 = Aditya |last2 = Pavlov |first2 = Mikhail |last3 = Goh |first3 = Gabriel |last4 = Gray |first4 = Scott |last5 = Voss |first5 = Chelsea |last6 = Radford |first6 = Alec |last7 = Chen |first7 = Mark |last8 = Sutskever |first8 = Ilya |eprint = 2102.12092 |title = Zero-Shot Text-to-Image Generation |class = cs.LG |date = 24 February 2021}}</ref>\n<ref name=\"tc\">{{cite web\n  | url          = https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n  | title        = OpenAI's DALL-E creates plausible images of literally anything you ask it to\n  | last         = Coldewey\n  | first        = Devin\n  | website      = \n  | publisher    = \n  | date         = 5 January 2021\n  | access-date  = 5 January 2021\n  | quote        = \n  | archive-date = 6 January 2021\n  | archive-url  = https://web.archive.org/web/20210106075542/https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n  | url-status   = live\n  }}</ref>\n<ref name=\"vb\">{{cite web\n  | url          = https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n  | title        = OpenAI debuts DALL-E for generating images from text\n  | last         = Johnson\n  | first        = Khari\n  | website      = \n  | publisher    = VentureBeat\n  | date         = 5 January 2021\n  | access-date  = 5 January 2021\n  | quote        = \n  | archive-date = 5 January 2021\n  | archive-url  = https://web.archive.org/web/20210105221534/https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n  | url-status   = live\n  }}</ref>\n<ref name=\"mittr\">{{cite web\n  | url          = https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/\n  | title        = This avocado armchair could be the future of AI\n  | last         = Heaven\n  | first        = Will Douglas\n  | website      = \n  | publisher    = MIT Technology Review\n  | date         = 5 January 2021\n  | access-date  = 5 January 2021\n  | quote        = \n  | archive-date = 5 January 2021\n  | archive-url  = https://web.archive.org/web/20210105193658/https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/\n  | url-status   = live\n  }}</ref>\n<ref name=\"boing\">{{cite web\n  | url          = https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n  | title        = This AI neural network transforms text captions into art, like a jellyfish Pikachu\n  | last         = Dunn\n  | first        = Thom\n  | website      = \n  | publisher    = [[BoingBoing]]\n  | date         = 10 February 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 22 February 2021\n  | archive-url  = https://web.archive.org/web/20210222001459/https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"nature\">{{cite web\n  | url          = https://www.nature.com/immersive/d41586-021-00095-y/index.html\n  | title        = Tardigrade circus and a tree of life \u2014 January's best science images\n  | last         = Stove\n  | first        = Emma\n  | website      = \n  | publisher    = [[Nature (journal)|Nature]]\n  | date         = 5 February 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 8 March 2021\n  | archive-url  = https://web.archive.org/web/20210308032636/https://www.nature.com/immersive/d41586-021-00095-y/index.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"extreme\">{{cite news\n  | url          = https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n  | title        = OpenAI's 'DALL-E' Generates Images From Text Descriptions\n  | last         = Whitwam\n  | first        = Ryan\n  | website      = \n  | newspaper    = ExtremeTech\n  | date         = 6 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 28 January 2021\n  | archive-url  = https://web.archive.org/web/20210128064428/https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n  | url-status   = live\n  }}</ref>\n<ref name=\"engadget\">{{cite web\n  | url          = https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n  | title        = OpenAI's DALL-E app generates images from just a description\n  | last         = Dent\n  | first        = Steve\n  | website      = \n  | publisher    = [[Engadget]]\n  | date         = 6 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 27 January 2021\n  | archive-url  = https://web.archive.org/web/20210127225652/https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"input\">{{cite web\n  | url          = https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n  | title        = This AI turns text into surreal, suggestion-driven art\n  | last         = Kasana\n  | first        = Mehreen\n  | website      = \n  | publisher    = Input\n  | date         = 7 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 29 January 2021\n  | archive-url  = https://web.archive.org/web/20210129211643/https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n  | url-status   = live\n  }}</ref>\n<ref name=\"cnbc\">{{cite web\n  | url          = https://www.cnbc.com/2021/01/08/openai-shows-off-dall-e-image-generator-after-gpt-3.html\n  | title        = Why everyone is talking about an image generator released by an Elon Musk-backed A.I. lab\n  | last         = Shead\n  | first        = Sam\n  | website      = \n  | publisher    = [[CNBC]]\n  | date         = 8 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 16 July 2022\n  | archive-url  = https://web.archive.org/web/20220716230354/https://www.cnbc.com/2021/01/08/openai-shows-off-dall-e-image-generator-after-gpt-3.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"dale\">{{cite web\n  | url          = https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n  | title        = Here's how OpenAI's magical DALL-E image generator works\n  | last         = Markowitz\n  | first        = Dale\n  | website      = \n  | publisher    = [[TheNextWeb]]\n  | date         = 10 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 23 February 2021\n  | archive-url  = https://web.archive.org/web/20210223162340/https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n  | url-status   = live\n  }}</ref>\n<ref name=\"nbc\">{{cite web\n  | url          = https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n  | title        = Here's DALL-E: An algorithm learned to draw anything you tell it\n  | last         = Ehrenkranz\n  | first        = Melanie\n  | website      = \n  | publisher    = [[NBC News]]\n  | date         = 27 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 20 February 2021\n  | archive-url  = https://web.archive.org/web/20210220164655/https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n  | url-status   = live\n  }}</ref>\n<ref name=\"bbc\">{{cite web\n  | url          = https://www.bbc.com/news/technology-55559463\n  | title        = AI draws dog-walking baby radish in a tutu\n  | last         = Wakefield\n  | first        = Jane\n  | website      = \n  | publisher    = [[British Broadcasting Corporation]]\n  | date         = 6 January 2021\n  | access-date  = 3 March 2021\n  | quote        = \n  | archive-date = 2 March 2021\n  | archive-url  = https://web.archive.org/web/20210302170623/https://www.bbc.com/news/technology-55559463\n  | url-status   = live\n  }}</ref>\n<ref name=\"cnn\">{{cite web\n  | url          = https://www.cnn.com/2021/01/08/tech/artificial-intelligence-openai-images-from-text/index.html\n  | title        = A radish in a tutu walking a dog? This AI can draw it really well\n  | last         = Metz\n  | first        = Rachel\n  | website      = \n  | publisher    = CNN\n  | date         = 2 February 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 16 July 2022\n  | archive-url  = https://web.archive.org/web/20220716171727/https://www.cnn.com/2021/01/08/tech/artificial-intelligence-openai-images-from-text/index.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"wired\">{{cite magazine\n  | url          = https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n  | title        = This AI Could Go From 'Art' to Steering a Self-Driving Car\n  | last         = Knight\n  | first        = Will\n  | magazine     = Wired\n  | date         = 26 January 2021\n  | access-date  = 2 March 2021\n  | quote        = \n  | archive-date = 21 February 2021\n  | archive-url  = https://web.archive.org/web/20210221010223/https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n  | url-status   = live\n  }}</ref>\n<ref name=\"CNETmini\">{{cite web\n  | url          = https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n  | title        = Everything to Know About Dall-E Mini, the Mind-Bending AI Art Creator\n  | last         = Carson\n  | first        = Erin\n  | website      = [[CNET]]\n  | publisher    = \n  | date         = 14 June 2022\n  | access-date  = 15 June 2022\n  | quote        = \n  | archive-date = 15 June 2022\n  | archive-url  = https://web.archive.org/web/20220615085705/https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n  | url-status   = live\n  }}</ref>\n<ref name=\"DailyDotmini\">{{cite web\n  | url          = https://www.dailydot.com/unclick/dall-e-mini-memes/\n  | title        = AI program DALL-E mini prompts some truly cursed images\n  | last         = Schroeder\n  | first        = Audra\n  | website      = [[Daily Dot]]\n  | publisher    = \n  | date         = 9 June 2022\n  | access-date  = 15 June 2022\n  | quote        = \n  | archive-date = 10 June 2022\n  | archive-url  = https://web.archive.org/web/20220610212300/https://www.dailydot.com/unclick/dall-e-mini-memes/\n  | url-status   = live\n  }}</ref>\n<ref name=\"Polygonmini\">{{cite web\n  | url          = https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n  | title        = People are using DALL-E mini to make meme abominations \u2014 like pug Pikachu\n  | last         = Diaz\n  | first        = Ana\n  | website      = [[Polygon (website)|Polygon]]\n  | publisher    = \n  | date         = 15 June 2022\n  | access-date  = 15 June 2022\n  | quote        = \n  | archive-date = 15 June 2022\n  | archive-url  = https://web.archive.org/web/20220615151753/https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n  | url-status   = live\n  }}</ref>\n<ref name=\"VentureBeat3\">{{cite web\n  | url          = https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n  | title        = How DALL-E 2 could solve major computer vision challenges\n  | last         = Sahar Mor\n  | first        = Stripe\n  | website      = [[VentureBeat]]\n  | publisher    = \n  | date         = 16 April 2022\n  | access-date  = 15 June 2022\n  | quote        = \n  | archive-date = 24 May 2022\n  | archive-url  = https://web.archive.org/web/20220524133956/https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n  | url-status   = live\n  }}</ref>\n<ref name=\"WindowsCentral\">{{cite web\n  | url          = https://www.windowscentral.com/software-apps/bing/bing-dall-e-3-image-creation-was-great-for-a-few-days-but-now-microsoft-has-predictably-lobotomized-it\n  | title        = Bing Dall-E 3 image creation was great for a few days, but now Microsoft has predictably lobotomized it\n  | last         = Corden\n  | first        = Jez\n  | website      = Windows Central\n  | date         = 8 October 2023\n  | access-date  = 11 October 2023\n  | archive-date = 10 October 2023\n  | archive-url  = https://web.archive.org/web/20231010185641/https://www.windowscentral.com/software-apps/bing/bing-dall-e-3-image-creation-was-great-for-a-few-days-but-now-microsoft-has-predictably-lobotomized-it\n  | url-status   = live\n  }}</ref>\n<ref name=\"TechRadar\">{{cite web\n  | url          = https://www.techradar.com/computing/artificial-intelligence/microsoft-reins-in-bing-ais-image-creator-and-the-results-dont-make-much-sense\n  | title        = Microsoft reins in Bing AI's Image Creator \u2013 and the results don't make much sense\n  | last         = Allan\n  | first        = Darren\n  | website      = [[TechRadar]]\n  | date         = 9 October 2023\n  | access-date  = 11 October 2023\n  | archive-date = 10 October 2023\n  | archive-url  = https://web.archive.org/web/20231010075911/https://www.techradar.com/computing/artificial-intelligence/microsoft-reins-in-bing-ais-image-creator-and-the-results-dont-make-much-sense\n  | url-status   = live\n  }}</ref>\n<ref name=\":3\">{{Cite web\n  | date         = 2022-07-20\n  | title        = DALL\u00b7E Now Available in Beta\n  | url          = https://openai.com/blog/dall-e-now-available-in-beta/\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20220720162939/https://openai.com/blog/dall-e-now-available-in-beta/\n  | archive-date = 20 July 2022\n  | access-date  = 2022-07-20\n  | website      = OpenAI\n  | language     = en\n  }}</ref>\n<ref name=\":4\">{{Cite news\n  | last         = Allyn\n  | first        = Bobby\n  | date         = 2022-07-20\n  | title        = Surreal or too real? Breathtaking AI tool DALL\u00b7E takes its images to a bigger stage\n  | language     = en\n  | work         = NPR\n  | url          = https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test\n  | url-status   = live\n  | access-date  = 2022-07-20\n  | archive-url  = https://web.archive.org/web/20220720170255/https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test\n  | archive-date = 20 July 2022\n  }}</ref>\n<ref name=\":5\">{{Cite web\n  | title        = DALL\u00b7E 3\n  | url          = https://openai.com/dall-e-3/\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20230920225833/https://openai.com/dall-e-3\n  | archive-date = 20 September 2023\n  | access-date  = 2023-09-21\n  | website      = OpenAI\n  | language     = en-US\n  }}</ref>\n<ref name=\":2\">{{Cite arXiv |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |date=2022-04-12 |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |class=cs.CV |eprint=2204.06125}}</ref>\n<ref name=\":0\">{{cite arXiv |eprint=2204.13807 |class=cs.CV |first1=Gary |last1=Marcus |first2=Ernest |last2=Davis |title=A very preliminary analysis of DALL-E 2 |date=2022-05-02 |last3=Aaronson |first3=Scott}}</ref>\n<ref name=\":1\">{{Cite web\n  | date         = 2022-06-28\n  | title        = DALL\u00b7E 2 Pre-Training Mitigations\n  | url          = https://openai.com/blog/dall-e-2-pre-training-mitigations/\n  | access-date  = 2022-07-18\n  | website      = OpenAI\n  | language     = en\n  | archive-date = 19 July 2022\n  | archive-url  = https://web.archive.org/web/20220719044249/https://openai.com/blog/dall-e-2-pre-training-mitigations/\n  | url-status   = live\n  }}</ref>\n<ref name=\"Taylor\">{{cite news\n  | last         = Taylor\n  | first        = Josh\n  | title        = From Trump Nevermind babies to deep fakes: DALL-E and the ethics of AI art\n  | url          = https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art\n  | website      = The Guardian\n  | date         = 18 June 2022\n  | accessdate   = 2 August 2022\n  | archive-date = 6 July 2022\n  | archive-url  = https://web.archive.org/web/20220706125540/https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art\n  | url-status   = live\n  }}</ref>\n<ref name=\"wired2\">{{cite magazine\n  | last1        = Knight\n  | first1       = Will\n  | title        = When AI Makes Art, Humans Supply the Creative Spark\n  | url          = https://www.wired.com/story/when-ai-makes-art/\n  | magazine     = Wired\n  | date         = 13 July 2022\n  | accessdate   = 2 August 2022\n  | archive-date = 2 August 2022\n  | archive-url  = https://web.archive.org/web/20220802162402/https://www.wired.com/story/when-ai-makes-art/\n  | url-status   = live\n  }}</ref>\n<ref name=\"vice\">{{cite news\n  | title        = DALL-E Is Now Generating Realistic Faces of Fake People\n  | last         = Rose\n  | first        = Janus\n  | url          = https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people\n  | date         = 24 June 2022\n  | work         = Vice\n  | accessdate   = 2 August 2022\n  | archive-date = 30 July 2022\n  | archive-url  = https://web.archive.org/web/20220730133250/https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people\n  | url-status   = live\n  }}</ref>\n<ref name=\"docs\">{{cite web\n  | title        = DALL\u00b7E 2 Preview - Risks and Limitations\n  | author       = OpenAI\n  | website      = [[GitHub]]\n  | url          = https://github.com/openai/dalle-2-preview/blob/main/system-card.md\n  | date         = 19 June 2022\n  | accessdate   = 2 August 2022\n  | archive-date = 2 August 2022\n  | archive-url  = https://web.archive.org/web/20220802162403/https://github.com/openai/dalle-2-preview/blob/main/system-card.md\n  | url-status   = live\n  }}</ref>\n<ref name=\"FSKdR\">{{Cite web\n  | last         = David\n  | first        = Emilia\n  | date         = 2023-09-20\n  | title        = OpenAI releases third version of DALL\u00b7E\n  | url          = https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai\n  | access-date  = 2023-09-21\n  | website      = The Verge\n  | language     = en-US\n  | archive-date = 20 September 2023\n  | archive-url  = https://web.archive.org/web/20230920192429/https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai\n  | url-status   = live\n  }}</ref>\n<ref name=\"jh5eF\">{{Cite web\n  | title        = OpenAI Platform\n  | url          = https://platform.openai.com/\n  | access-date  = 2023-11-10\n  | website      = platform.openai.com\n  | language     = en\n  | archive-date = 20 March 2023\n  | archive-url  = https://web.archive.org/web/20230320023933/https://platform.openai.com/\n  | url-status   = live\n  }}</ref>\n<ref name=\"Gt7o0\">{{Cite web\n  | last         = Niles\n  | first        = Raymond\n  | date         = 2023-11-10\n  | orig-date    = Updated this week\n  | title        = DALL-E 3 API\n  | url          = https://help.openai.com/en/articles/8555480-dall-e-3-api\n  | access-date  = 2023-11-10\n  | website      = OpenAI help Center\n  | language     = en\n  | archive-date = 10 November 2023\n  | archive-url  = https://web.archive.org/web/20231110182305/https://help.openai.com/en/articles/8555480-dall-e-3-api\n  | url-status   = live\n  }}</ref>\n<ref name=\"jKa67\">{{Cite web\n  | last         = Mehdi\n  | first        = Yusuf\n  | date         = 2023-09-21\n  | title        = Announcing Microsoft Copilot, your everyday AI companion\n  | url          = https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/\n  | access-date  = 2023-09-21\n  | website      = The Official Microsoft Blog\n  | language     = en-US\n  | archive-date = 21 September 2023\n  | archive-url  = https://web.archive.org/web/20230921150139/https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/\n  | url-status   = live\n  }}</ref>\n<ref name=\"NJ4qD\">{{Cite web\n  | title        = DALL\u00b7E 2\n  | url          = https://openai.com/dall-e-2/\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20220406141035/https://openai.com/dall-e-2/\n  | archive-date = 6 April 2022\n  | access-date  = 2022-07-06\n  | website      = OpenAI\n  | language     = en-US\n  }}</ref>\n<ref name=\"bYeQq\">{{Cite web\n  | title        = DALL\u00b7E Waitlist\n  | url          = https://labs.openai.com/\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20220704184756/https://labs.openai.com/\n  | archive-date = 4 July 2022\n  | access-date  = 2022-07-06\n  | website      = labs.openai.com\n  | language     = en\n  }}</ref>\n<ref name=\"Xa1jy\">{{Cite web\n  | date         = 2022-06-18\n  | title        = From Trump Nevermind babies to deep fakes: DALL\u00b7E and the ethics of AI art\n  | url          = https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20220706125540/https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art\n  | archive-date = 6 July 2022\n  | access-date  = 2022-07-06\n  | website      = the Guardian\n  | language     = en\n  }}</ref>\n<ref name=\"BEbJV\">{{Cite web\n  | date         = 2022-09-28\n  | title        = DALL\u00b7E Now Available Without Waitlist\n  | url          = https://openai.com/blog/dall-e-now-available-without-waitlist/\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20221004093145/https://openai.com/blog/dall-e-now-available-without-waitlist/\n  | archive-date = 4 October 2022\n  | access-date  = 2022-10-05\n  | website      = OpenAI\n  | language     = en\n  }}</ref>\n<ref name=\"jodXw\">{{Cite web\n  | date         = 2022-11-03\n  | title        = DALL\u00b7E API Now Available in Public Beta\n  | url          = https://openai.com/blog/dall-e-api-now-available-in-public-beta\n  | url-status   = live\n  | archive-url  = https://web.archive.org/web/20221119222937/https://openai.com/blog/dall-e-api-now-available-in-public-beta/\n  | archive-date = 19 November 2022\n  | access-date  = 2022-11-19\n  | website      = OpenAI\n  | language     = en\n  }}</ref>\n<ref name=\"hhWp9\">{{Cite news\n  | last         = Wiggers\n  | first        = Kyle\n  | date         = 2022-11-03\n  | title        = Now anyone can build apps that use DALL\u00b7E 2 to generate images\n  | work         = [[TechCrunch]]\n  | url          = https://techcrunch.com/2022/11/03/now-anyone-can-build-apps-that-use-dall-e-2-to-generate-images\n  | url-status   = live\n  | access-date  = 2022-11-19\n  | archive-url  = https://web.archive.org/web/20221119154222/https://techcrunch.com/2022/11/03/now-anyone-can-build-apps-that-use-dall-e-2-to-generate-images/\n  | archive-date = 19 November 2022\n  }}</ref>\n<ref name=\"cKHhM\">{{cite web\n  | url          = https://www.makeuseof.com/gpt-models-explained-and-compared/\n  | title        = GPT-1 to GPT-4: Each of OpenAI's GPT Models Explained and Compared\n  | date         = 11 April 2023\n  | access-date  = 29 April 2023\n  | archive-date = 15 April 2023\n  | archive-url  = https://web.archive.org/web/20230415175013/https://www.makeuseof.com/gpt-models-explained-and-compared/\n  | url-status   = live\n  }}</ref>\n<ref name=\"9FmC7\">{{Cite web\n  | title        = 'DALL\u00b7E' AI generates an image out of anything you describe\n  | url          = https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n  | access-date  = 2022-07-18\n  | website      = Engadget\n  | date         = 6 January 2021\n  | language     = en-US\n  | archive-date = 27 January 2021\n  | archive-url  = https://web.archive.org/web/20210127225652/https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"uko4t\">{{Cite arXiv |last1=Radford |first1=Alec |last2=Kim |first2=Jong Wook |last3=Hallacy |first3=Chris |last4=Ramesh |first4=Aditya |last5=Goh |first5=Gabriel |last6=Agarwal |first6=Sandhini |last7=Sastry |first7=Girish |last8=Askell |first8=Amanda |last9=Mishkin |first9=Pamela |last10=Clark |first10=Jack |last11=Krueger |first11=Gretchen |last12=Sutskever |first12=Ilya |date=2021 |title=Learning Transferable Visual Models From Natural Language Supervision |class=cs.CV |eprint=2103.00020}}</ref>\n<ref name=\"fdfao\">{{Cite web\n  | date         = 2021-01-05\n  | title        = DALL\u00b7E: Creating Images from Text\n  | url          = https://openai.com/blog/dall-e/\n  | access-date  = 2022-08-13\n  | website      = OpenAI\n  | language     = en\n  | archive-date = 27 March 2021\n  | archive-url  = https://web.archive.org/web/20210327133043/https://openai.com/blog/dall-e/\n  | url-status   = live\n  }}</ref>\n<ref name=\"UZbGX\">{{Cite web\n  | last         = Edwards\n  | first        = Benj\n  | date         = 2023-09-20\n  | title        = OpenAI's new AI image generator pushes the limits in detail and prompt fidelity\n  | url          = https://arstechnica.com/information-technology/2023/09/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/\n  | access-date  = 2023-09-21\n  | website      = Ars Technica\n  | language     = en-us\n  | archive-date = 21 September 2023\n  | archive-url  = https://web.archive.org/web/20230921130853/https://arstechnica.com/information-technology/2023/09/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/\n  | url-status   = live\n  }}</ref>\n<ref name=\"UOAWM\">{{Cite web\n  | last         = Coldewey\n  | first        = Devin\n  | date         = 2022-04-06\n  | title        = New OpenAI tool draws anything, bigger and better than ever\n  | url          = https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/\n  | access-date  = 2022-11-26\n  | website      = TechCrunch\n  | language     = en-US\n  | archive-date = 6 May 2023\n  | archive-url  = https://web.archive.org/web/20230506073242/https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/\n  | url-status   = live\n  }}</ref>\n<ref name=\"6BZCE\">{{Cite web\n  | date         = 2022-08-31\n  | title        = DALL\u00b7E: Introducing Outpainting\n  | url          = https://openai.com/blog/dall-e-introducing-outpainting/\n  | access-date  = 2022-11-26\n  | website      = OpenAI\n  | language     = en\n  | archive-date = 26 November 2022\n  | archive-url  = https://web.archive.org/web/20221126173545/https://openai.com/blog/dall-e-introducing-outpainting/\n  | url-status   = live\n  }}</ref>\n<ref name=\"uZZnn\">{{cite arXiv |eprint=2205.11487 |class=cs.CV |first1=Chitwan |last1=Saharia |first2=William |last2=Chan |title=Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding |date=2022-05-23 |last3=Saxena |first3=Saurabh |last4=Li |first4=Lala |last5=Whang |first5=Jay |last6=Denton |first6=Emily |last7=Ghasemipour |first7=Seyed Kamyar Seyed |last8=Ayan |first8=Burcu Karagol |last9=Mahdavi |first9=S. Sara |last10=Lopes |first10=Rapha Gontijo |last11=Salimans |first11=Tim}}</ref>\n<ref name=\"l0sAF\">{{Cite web\n  | last         = Marcus\n  | first        = Gary\n  | date         = 2022-05-28\n  | title        = Horse rides astronaut\n  | url          = https://garymarcus.substack.com/p/horse-rides-astronaut\n  | access-date  = 2022-06-18\n  | website      = The Road to AI We Can Trust\n  | archive-date = 19 June 2022\n  | archive-url  = https://web.archive.org/web/20220619135711/https://garymarcus.substack.com/p/horse-rides-astronaut\n  | url-status   = live\n  }}</ref>\n<ref name=\"QJCbe\">{{Cite web\n  | last         = Strickland\n  | first        = Eliza\n  | date         = 2022-07-14\n  | title        = DALL\u00b7E 2's Failures Are the Most Interesting Thing About It\n  | url          = https://spectrum.ieee.org/openai-dall-e-2\n  | access-date  = 2022-08-16\n  | website      = IEEE Spectrum\n  | language     = en\n  | archive-date = 15 July 2022\n  | archive-url  = https://web.archive.org/web/20220715204154/https://spectrum.ieee.org/openai-dall-e-2\n  | url-status   = live\n  }}</ref>\n<ref name=\"yASFB\">{{Cite web\n  | last         = STRICKLAND\n  | first        = ELIZA\n  | date         = 2022-07-14\n  | title        = DALL-E 2's Failures Are the Most Interesting Thing About It\n  | url          = https://spectrum.ieee.org/openai-dall-e-2\n  | access-date  = 2022-07-15\n  | website      = IEEE Spectrum\n  | language     = en\n  | archive-date = 15 July 2022\n  | archive-url  = https://web.archive.org/web/20220715204154/https://spectrum.ieee.org/openai-dall-e-2\n  | url-status   = live\n  }}</ref>\n<ref name=\"QIhyO\">{{cite web\n  | url          = https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped\n  | author       = James Vincent\n  | date         = September 29, 2022\n  | title        = OpenAI's image generator DALL\u00b7E is available for anyone to use immediately\n  | website      = [[The Verge]]\n  | access-date  = 29 September 2022\n  | archive-date = 29 September 2022\n  | archive-url  = https://web.archive.org/web/20220929061004/https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped\n  | url-status   = live\n  }}</ref>\n<ref name=\"w7Syr\">{{cite magazine\n  | last1        = Lane\n  | first1       = Laura\n  | title        = DALL-E, Make Me Another Picasso, Please\n  | url          = https://www.newyorker.com/magazine/2022/07/11/dall-e-make-me-another-picasso-please\n  | magazine     = The New Yorker\n  | date         = 1 July 2022\n  | accessdate   = 2 August 2022\n  | archive-date = 2 August 2022\n  | archive-url  = https://web.archive.org/web/20220802162403/https://www.newyorker.com/magazine/2022/07/11/dall-e-make-me-another-picasso-please\n  | url-status   = live\n  }}</ref>\n<ref name=\"X3g6f\">{{cite web\n  | title        = OpenAI: Will DALL\u00b7E 2 kill creative careers?\n  | last         = Goldman\n  | first        = Sharon\n  | date         = 26 July 2022\n  | url          = https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/\n  | access-date  = 16 August 2022\n  | archive-date = 15 August 2022\n  | archive-url  = https://web.archive.org/web/20220815014607/https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/\n  | url-status   = live\n  }}</ref>\n<ref name=\"74NV7\">{{cite web\n  | title        = DALL-E 2: A dream tool and an existential threat to visual artists\n  | last         = Blain\n  | first        = Loz\n  | date         = 29 July 2022\n  | url          = https://newatlas.com/computers/dall-e-2-ai-art/\n  | access-date  = 16 August 2022\n  | archive-date = 17 August 2022\n  | archive-url  = https://web.archive.org/web/20220817064216/https://newatlas.com/computers/dall-e-2-ai-art/\n  | url-status   = live\n  }}</ref>\n<ref name=\"39hV1\">{{Cite web\n  | last         = Leswing\n  | first        = Kif\n  | title        = Why Silicon Valley is so excited about awkward drawings done by artificial intelligence\n  | url          = https://www.cnbc.com/2022/10/08/generative-ai-silicon-valleys-next-trillion-dollar-companies.html\n  | access-date  = 2022-12-01\n  | website      = CNBC\n  | date         = 8 October 2022\n  | language     = en\n  | archive-date = 29 July 2023\n  | archive-url  = https://web.archive.org/web/20230729005158/https://www.cnbc.com/2022/10/08/generative-ai-silicon-valleys-next-trillion-dollar-companies.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"BoHAr\">{{Cite web\n  | last         = Etherington\n  | first        = Darrell\n  | date         = 2019-07-22\n  | title        = Microsoft invests $1 billion in OpenAI in new multiyear partnership\n  | url          = https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/\n  | access-date  = 2023-09-21\n  | website      = TechCrunch\n  | language     = en-US\n  | archive-date = 22 July 2019\n  | archive-url  = https://web.archive.org/web/20190722173752/https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/\n  | url-status   = live\n  }}</ref>\n<ref name=\"SeRsU\">{{Cite web\n  | title        = OpenAI's first VC backer weighs in on generative A.I.\n  | url          = https://fortune.com/2023/02/02/openais-first-vc-backer-khosla-ventures-weighs-in-on-the-future-of-generative-a-i/\n  | access-date  = 2023-09-21\n  | website      = Fortune\n  | language     = en\n  | archive-date = 23 October 2023\n  | archive-url  = https://web.archive.org/web/20231023211810/https://fortune.com/2023/02/02/openais-first-vc-backer-khosla-ventures-weighs-in-on-the-future-of-generative-a-i/\n  | url-status   = live\n  }}</ref>\n<ref name=\"DmeBW\">{{Cite news\n  | last1        = Metz\n  | first1       = Cade\n  | last2        = Weise\n  | first2       = Karen\n  | date         = 2023-01-23\n  | title        = Microsoft to Invest $10 Billion in OpenAI, the Creator of ChatGPT\n  | language     = en-US\n  | work         = The New York Times\n  | url          = https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-intelligence.html\n  | access-date  = 2023-09-21\n  | issn         = 0362-4331\n  | archive-date = 21 September 2023\n  | archive-url  = https://web.archive.org/web/20230921171844/https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-intelligence.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"dhGoa\">{{Cite web\n  | date         = 2022-10-27\n  | title        = AI-generated art sparks furious backlash from Japan's anime community\n  | url          = https://restofworld.org/2022/ai-backlash-anime-artists/\n  | access-date  = 2023-01-03\n  | website      = Rest of World\n  | language     = en-US\n  | archive-date = 31 December 2022\n  | archive-url  = https://web.archive.org/web/20221231155114/https://restofworld.org/2022/ai-backlash-anime-artists/\n  | url-status   = live\n  }}</ref>\n<ref name=\"1ejYV\">{{Cite news\n  | last         = Roose\n  | first        = Kevin\n  | date         = 2022-09-02\n  | title        = An A.I.-Generated Picture Won an Art Prize. Artists Aren't Happy.\n  | language     = en-US\n  | work         = The New York Times\n  | url          = https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html\n  | access-date  = 2023-01-03\n  | issn         = 0362-4331\n  | archive-date = 31 May 2023\n  | archive-url  = https://web.archive.org/web/20230531203858/https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html\n  | url-status   = live\n  }}</ref>\n<ref name=\"r9oIC\">{{Cite web\n  | last         = Daws\n  | first        = Ryan\n  | date         = 2022-12-15\n  | title        = ArtStation backlash increases following AI art protest response\n  | url          = https://www.artificialintelligence-news.com/2022/12/15/artstation-backlash-increases-ai-art-protest-response/\n  | access-date  = 2023-01-03\n  | website      = AI News\n  | language     = en-GB\n  | archive-date = 3 January 2023\n  | archive-url  = https://web.archive.org/web/20230103233938/https://www.artificialintelligence-news.com/2022/12/15/artstation-backlash-increases-ai-art-protest-response/\n  | url-status   = live\n  }}</ref>\n<ref name=\"SWLwd\">{{Citation\n  | title        = jina-ai/dalle-flow\n  | date         = 2022-06-17\n  | url          = https://github.com/jina-ai/dalle-flow\n  | publisher    = Jina AI\n  | access-date  = 2022-06-17\n  | archive-date = 17 June 2022\n  | archive-url  = https://web.archive.org/web/20220617090248/https://github.com/jina-ai/dalle-flow\n  | url-status   = live\n  }}</ref>\n}}\n\n==External links==\n{{Commons category}}\n*[https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf DALL-E 3 System Card]\n*[https://cdn.openai.com/papers/dall-e-3.pdf DALL-E 3] paper by OpenAI\n*[https://openai.com/dall-e-2/ DALL-E 2 website]\n*[https://www.craiyon.com/ Craiyon website]\n\n{{OpenAI navbox}}\n{{Differentiable computing}}\n{{Authority control}}\n\n[[Category:Artificial intelligence art]]\n[[Category:Text-to-image generation]]\n[[Category:Deep learning software applications]]\n[[Category:Unsupervised learning]]\n[[Category:Generative pre-trained transformers]]\n[[Category:OpenAI]]\n[[Category:2021 software]]\n\n<!--\nThis is NOT DALL-E!--><!--\nThis is WIKIPEDIA!!--><!--\nDo NOT put prompts here!--><!--\nYou are editing a Wikipedia article!--><!--\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n\u2588\u2588              \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588     \u2588\u2588   \u2588\u2588\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588          \u2588\u2588\n       \u2588\u2588       \u2588\u2588       \u2588\u2588     \u2588\u2588   \u2588\u2588            \n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        \u2588\u2588        \u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2588\u2588          \u2588\u2588\n-->"}