{"title": "DALL-E", "page_id": 66303034, "revision_id": 1130669294, "revision_timestamp": "2022-12-31T10:17:21Z", "content": "{{Short description|Image-generating deep-learning model}}\n{{Use dmy dates|date=June 2022}}\n{{Infobox software\n| name = DALL-E\n| logo = \n| screenshot = File:DALL-E 2 artificial intelligence digital image generated photo.jpg\n| screenshot size = 250px\n| caption = An image generated with DALL-E 2 based on the text prompt \"Teddy bears working on new AI research underwater with 1990s technology\"\n| author = [[OpenAI]]\n| developer = \n| released = January 5, 2021\n| latest release version = \n| latest release date = \n| repo = \n| programming language = \n| operating system = \n| genre = [[Transformer (machine learning model)|Transformer]] [[language model]]\n| license = \n| website = {{url|https://openai.com/blog/dall-e/}}\n}}\n{{Artificial intelligence}}\n[[File:DALL-E sample.png|thumb|Images produced with DALL-E 1 when given the text prompt \"a professional high quality illustration of a giraffe dragon chimera. a giraffe imitating a dragon. a giraffe made of dragon.\" (2021)]]\n'''DALL-E''' (stylized as '''DALL\u00b7E''') and '''DALL-E 2''' are [[deep learning]] models developed by [[OpenAI]] to generate digital images from [[natural language]] descriptions, called \"prompts\". DALL-E was revealed by OpenAI in a blog post in January 2021, and uses a version of [[GPT-3]]<ref name=\"vb\" /> modified to generate images. In April 2022, OpenAI announced DALL-E 2, a successor designed to generate more realistic images at higher resolutions that \"can combine concepts, attributes, and styles\".<ref>{{Cite web |title=DALL\u00b7E 2 |url=https://openai.com/dall-e-2/ |access-date=2022-07-06 |website=OpenAI |language=en}}</ref>\n\nOpenAI has not released [[source code]] for either model. On 20 July 2022, DALL-E 2 entered into a beta phase with invitations sent to 1 million waitlisted individuals;<ref>{{Cite web |date=2022-07-20 |title=DALL\u00b7E Now Available in Beta |url=https://openai.com/blog/dall-e-now-available-in-beta/ |access-date=2022-07-20 |website=OpenAI |language=en}}</ref> users can generate a certain number of images for free every month and may purchase more.<ref>{{Cite news |last=Allyn |first=Bobby |date=2022-07-20 |title=Surreal or too real? Breathtaking AI tool DALL-E takes its images to a bigger stage |language=en |work=NPR |url=https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test |access-date=2022-07-20}}</ref> Access had previously been restricted to pre-selected users for a research preview due to concerns about [[Ethics of artificial intelligence|ethics]] and safety.<ref>{{Cite web |title=DALL\u00b7E Waitlist |url=https://labs.openai.com/ |access-date=2022-07-06 |website=labs.openai.com |language=en}}</ref><ref>{{Cite web |date=2022-06-18 |title=From Trump Nevermind babies to deep fakes: DALL-E and the ethics of AI art |url=https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art |access-date=2022-07-06 |website=the Guardian |language=en}}</ref> On 28 September 2022, DALL-E 2 was opened to anyone and the waitlist requirement was removed.<ref>{{Cite web |date=2022-09-28 |title=DALL\u00b7E Now Available Without Waitlist |url=https://openai.com/blog/dall-e-now-available-without-waitlist/ |access-date=2022-10-05 |website=OpenAI |language=en}}</ref>\n\nIn early November 2022, OpenAI released DALL-E 2 as an [[API]], allowing developers to integrate the model into their own applications. [[Microsoft]] unveiled their implementation of DALL-E 2 in their Designer app and Image Creator tool included in [[Microsoft Bing|Bing]] and [[Microsoft Edge]]. CALA and Mixtiles are among other early adopters of the DALL-E 2 API.<ref>{{Cite web |date=2022-11-03 |title=DALL\u00b7E API Now Available in Public Beta |url=https://openai.com/blog/dall-e-api-now-available-in-public-beta |access-date=2022-11-19 |website=OpenAI |language=en}}</ref> The API operates on a cost per image basis, with prices varying depending on image resolution. Volume discounts are available to companies working with OpenAI\u2019s enterprise team.<ref>{{Cite news |last=Wiggers |first=Kyle |date=2022-11-03 |title=Now anyone can build apps that use DALL-E 2 to generate images |work=[[TechCrunch]] |url=https://techcrunch.com/2022/11/03/now-anyone-can-build-apps-that-use-dall-e-2-to-generate-images |access-date=2022-11-19}}</ref>\n\nThe software's name is a [[portmanteau]] of the names of animated robot [[Pixar]] character [[WALL\u00b7E (character)|WALL-E]] and the Spanish surrealist artist [[Salvador Dal\u00ed]].<ref name=\"tc\" /><ref name=\"vb\" />\n\n== Technology ==\nThe [[Generative Pre-trained Transformer]] (GPT) model was initially developed by OpenAI in 2018,<ref name=\"gpt1paper\" /> using a [[Transformer (machine learning model)|Transformer]] architecture. The first iteration, GPT, was scaled up to produce [[GPT-2]] in 2019;<ref name=\"gpt2paper\" /> in 2020 it was scaled up again to produce [[GPT-3]], with 175 billion parameters.<ref name=\"gpt3paper\" /><ref name=\"vb\" /><ref name=\"dallepaper\" /> DALL-E's model is a multimodal implementation of GPT-3<ref name=\"impact\" /> with 12 billion parameters<ref name=\"vb\" /> which \"swaps text for pixels\", trained on text-image pairs from the Internet.<ref name=\"mittr\" /> DALL-E 2 uses 3.5 billion parameters, a smaller number than its predecessor.<ref name=\":2\">{{Cite journal |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |date=2022-04-12 |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |url=http://arxiv.org/abs/2204.06125 |arxiv=2204.06125 }}</ref>\n\nDALL-E was developed and announced to the public in conjunction with CLIP (Contrastive Language-Image Pre-training).<ref name=\"mittr\" /> CLIP is a separate model based on [[zero-shot learning]] that was trained on 400 million pairs of images with text captions [[Web scraping|scraped]] from the Internet.<ref name=\"vb\" /><ref name=\"mittr\" /><ref>{{Cite web |title='DALL-E' AI generates an image out of anything you describe |url=https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html |access-date=2022-07-18 |website=Engadget |language=en-US}}</ref> Its role is to \"understand and rank\" DALL-E's output by predicting which caption from a list of 32,768 captions randomly selected from the dataset (of which one was the correct answer) is most appropriate for an image. This model is used to filter a larger initial list of images generated by DALL-E to select the most appropriate outputs.<ref name=\"tc\" /><ref name=\"mittr\" />\n\nDALL-E 2 uses a [[diffusion model]] conditioned on CLIP image embeddings, which, during inference, are generated from CLIP text embeddings by a prior model.<ref name=\":2\" />\n\n== Capabilities ==\nDALL-E can generate imagery in multiple styles, including [[photorealistic]] imagery, [[paintings]], and [[emoji]].<ref name=\"vb\" /> It can \"manipulate and rearrange\" objects in its images,<ref name=\"vb\" /> and can correctly place design elements in novel compositions without explicit instruction. Thom Dunn writing for ''[[Boing Boing|BoingBoing]]'' remarked that \"For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL-E often draws the handkerchief, hands, and feet in plausible locations.\"<ref name=\"boing\" /> DALL-E showed the ability to \"fill in the blanks\" to infer appropriate details without specific prompts such as adding Christmas imagery to prompts commonly associated with the celebration,<ref name=\"extreme\" /> and appropriately-placed shadows to images that did not mention them.<ref name=\"engadget\" /> Furthermore, DALL-E exhibits broad understanding of visual and design trends.{{Citation needed|date=July 2022}}\n\nDALL-E is able to produce images for a wide variety of arbitrary descriptions from various viewpoints<ref name=\":0\">{{cite arXiv |eprint=2204.13807 |class=cs.CV |first1=Gary |last1=Marcus |first2=Ernest |last2=Davis |title=A very preliminary analysis of DALL-E 2 |date=2022-05-02 |last3=Aaronson |first3=Scott}}</ref> with only rare failures.<ref name=\"tc\" /> Mark Riedl, an associate professor at the [[Georgia Tech]] School of Interactive Computing, found that DALL-E could blend concepts (described as a key element of human [[creativity]]).<ref name=\"cnbc\" /><ref name=\"bbc\" />\n\nIts visual reasoning ability is sufficient to solve [[Raven's Progressive Matrices|Raven's Matrices]] (visual tests often administered to humans to measure intelligence).<ref name=\"dale\" /><ref>{{Cite web |date=2021-01-05 |title=DALL\u00b7E: Creating Images from Text |url=https://openai.com/blog/dall-e/ |access-date=2022-08-13 |website=OpenAI |language=en}}</ref>\n\n{{multiple image\n| align             = left\n| width             = 130\n| image1            = DALL-E 2 variation 1.png\n| image2            = DALL-E 2 variation 2.png\n| footer            = Two \"variations\" of ''[[Girl With a Pearl Earring]]'' generated with DALL-E 2\n}}\nGiven an existing image, DALL-E 2 can produce \"variations\" of the image as unique outputs based on the original, as well as edit the image to modify or expand upon it. DALL-E 2's \"inpainting\" and \"outpainting\" use context from an image to fill in missing areas using a [[Artistic medium|medium]] consistent with the original, following a given prompt. For example, this can be used to insert a new subject into an image, or expand an image beyond its original borders.<ref>{{Cite web |last=Coldewey |first=Devin |date=2022-04-06 |title=New OpenAI tool draws anything, bigger and better than ever |url=https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/ |access-date=2022-11-26 |website=TechCrunch |language=en-US}}</ref> According to OpenAI, \"Outpainting takes into account the image\u2019s existing visual elements \u2014 including shadows, reflections, and textures \u2014 to maintain the context of the original image.\"<ref>{{Cite web |date=2022-08-31 |title=DALL\u00b7E: Introducing Outpainting |url=https://openai.com/blog/dall-e-introducing-outpainting/ |access-date=2022-11-26 |website=OpenAI |language=en}}</ref>\n\n=== Ethical concerns ===\nDALL-E 2's reliance on public datasets influences its results and lead to [[algorithmic bias]] in some cases such as generating higher numbers of men than women for requests that do not mention gender.<ref>{{Cite web |last=STRICKLAND |first=ELIZA |date=2022-07-14 |title=DALL-E 2's Failures Are the Most Interesting Thing About It |url=https://spectrum.ieee.org/openai-dall-e-2 |access-date=2022-07-15 |website=IEEE Spectrum |language=en}}</ref> DALL-E 2's training data was filtered to remove violent and sexual imagery, but this was found to increase bias in some cases such as reducing the frequency of women being generated.<ref name=\":1\" /> OpenAI hypothesize that this may be because women were more likely to be sexualized in training data which caused the filter to influence results.<ref name=\":1\">{{Cite web |date=2022-06-28 |title=DALL\u00b7E 2 Pre-Training Mitigations |url=https://openai.com/blog/dall-e-2-pre-training-mitigations/ |access-date=2022-07-18 |website=OpenAI |language=en}}</ref> In September 2022, OpenAI confirmed to ''[[The Verge]]'' that DALL-E invisibly inserts phrases into user prompts in order to address bias in results; for instance, \"black man\" and \"Asian woman\" are inserted into prompts that do not specify gender or race.<ref>{{cite web|url=https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped|author=James Vincent|date=September 29, 2022|title=OpenAI's image generator DALL-E is available for anyone to use immediately|website=[[The Verge]]}}</ref>\n\nA concern about DALL-E 2 and similar image generation models is that they could be used to propagate [[deepfake]]s and other forms of misinformation.<ref name=\"Taylor\">{{cite news|last=Taylor|first=Josh|title=From Trump Nevermind babies to deep fakes: DALL-E and the ethics of AI art |url=https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art |website=The Guardian|date=18 June 2022|accessdate=2 August 2022}}</ref><ref name=\"wired2\">{{cite magazine |last1=Knight |first1=Will |title=When AI Makes Art, Humans Supply the Creative Spark |url=https://www.wired.com/story/when-ai-makes-art/ |magazine=Wired|date=13 July 2022|accessdate=2 August 2022}}</ref> As an attempt to mitigate this, the software rejects prompts involving public figures and uploads containing human faces.<ref name=\"vice\">{{cite news |title=DALL-E Is Now Generating Realistic Faces of Fake People |last=Rose|first=Janus|url=https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people |date=24 June 2022|work=Vice|accessdate=2 August 2022}}</ref> Prompts containing potentially objectionable content are blocked, and uploaded images are analyzed to detect offensive material.<ref name=\"docs\" /> A disadvantage of prompt-based filtering is that it is easy to bypass using alternative phrases that result in a similar output. For example, the word \"blood\" is filtered, but \"ketchup\" and \"red liquid\" are not.<ref>{{cite magazine |last1=Lane|first1=Laura|title=DALL-E, Make Me Another Picasso, Please |url=https://www.newyorker.com/magazine/2022/07/11/dall-e-make-me-another-picasso-please |magazine=The New Yorker |date=1 July 2022|accessdate=2 August 2022}}</ref><ref name=\"docs\">{{cite web|title=DALL\u00b7E 2 Preview - Risks and Limitations|author=OpenAI|website=[[GitHub]] |url=https://github.com/openai/dalle-2-preview/blob/main/system-card.md|date=19 June 2022|accessdate=2 August 2022}}</ref>\n\nAnother concern about DALL-E 2 and similar models is that they could cause [[technological unemployment]] for artists, photographers, and graphic designers due to their accuracy and popularity. <ref>{{cite web|title=OpenAI: Will DALLE-2 kill creative careers?|last=Goldman|first=Sharon|date=26 July 2022 |url=https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/}}</ref><ref>{{cite web|title=DALL-E 2: A dream tool and an existential threat to visual artists|last=Blain|first=Loz|date=29 July 2022 |url=https://newatlas.com/computers/dall-e-2-ai-art/}}</ref>\n\n=== Technical limitations ===\nDALL-E 2's language understanding has limits. It is sometimes unable to distinguish \"A yellow book and a red vase\" from \"A red book and a yellow vase\" or \"A panda making latte art\" from \"Latte art of a panda\".<ref>{{cite arXiv |eprint=2205.11487 |class=cs.CV |first1=Chitwan |last1=Saharia |first2=William |last2=Chan |title=Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding |date=2022-05-23 |last3=Saxena |first3=Saurabh |last4=Li |first4=Lala |last5=Whang |first5=Jay |last6=Denton |first6=Emily |last7=Ghasemipour |first7=Seyed Kamyar Seyed |last8=Ayan |first8=Burcu Karagol |last9=Mahdavi |first9=S. Sara |last10=Lopes |first10=Rapha Gontijo |last11=Salimans |first11=Tim}}</ref> It generates images of \"an astronaut riding a horse\" when presented with the prompt \"a horse riding an astronaut\".<ref>{{Cite web |last=Marcus |first=Gary |date=2022-05-28 |title=Horse rides astronaut |url=https://garymarcus.substack.com/p/horse-rides-astronaut |access-date=2022-06-18 |website=The Road to AI We Can Trust}}</ref> It also fails to generate the correct images in a variety of circumstances. Requesting more than 3 objects, negation, numbers, and [[Conjunction (grammar)|connected sentences]] may result in mistakes and object features may appear on the wrong object.<ref name=\":0\" /> Additional limitations include handling text - which, even with legible lettering, almost invariably results in dream-like gibberish - and its limited capacity to address scientific information, such as astronomy or medical imagery.<ref>{{Cite web |last=Strickland |first=Eliza |date=2022-07-14 |title=DALL-E 2's Failures Are the Most Interesting Thing About It |url=https://spectrum.ieee.org/openai-dall-e-2 |access-date=2022-08-16 |website=IEEE Spectrum |language=en}}</ref>\n\n== Reception ==\n[[File:DALL-E radish.jpg|thumb|Images generated by DALL-E upon the prompt: \"an illustration of a baby daikon radish in a tutu walking a dog\"]]\nMost coverage of DALL-E focuses on a small subset of \"surreal\"<ref name=\"mittr\" /> or \"quirky\"<ref name=\"cnbc\" /> outputs. DALL-E's output for \"an illustration of a baby daikon radish in a tutu walking a dog\" was mentioned in pieces from ''Input'',<ref name=\"input\" /> [[NBC]],<ref name=\"nbc\" /> ''[[Nature (journal)|Nature]]'',<ref name=\"nature\" /> and other publications.<ref name=\"vb\" /><ref name=\"wired\" /><ref name=\"cnn\" /> Its output for \"an armchair in the shape of an avocado\" was also widely covered.<ref name=\"mittr\" /><ref name=\"bbc\" />\n\n''[[ExtremeTech]]'' stated \"you can ask DALL-E for a picture of a phone or vacuum cleaner from a specified period of time, and it understands how those objects have changed\".<ref name=\"extreme\" /> ''[[Engadget]]'' also noted its unusual capacity for \"understanding how telephones and other objects change over time\".<ref name=\"engadget\" />\n\nAccording to ''[[MIT Technology Review]]'', one of OpenAI's objectives was to \"give language models a better grasp of the everyday concepts that humans use to make sense of things\".<ref name=\"mittr\" />\n\nWall Street investors have had a positive reception of DALL-E 2, with some firms thinking it could represent a turning point for a future multi-trillion dollar industry. OpenAI has already received over 1 billion dollars in funding from [[Microsoft]] and Khosla Ventures.<ref>{{Cite web |last=Leswing |first=Kif |title=Why Silicon Valley is so excited about awkward drawings done by artificial intelligence |url=https://www.cnbc.com/2022/10/08/generative-ai-silicon-valleys-next-trillion-dollar-companies.html |access-date=2022-12-01 |website=CNBC |language=en}}</ref>\n\n== Open-source implementations ==\nThere have been several attempts to create [[Open source|open-source]] implementations of DALL-E.<ref name=\"VentureBeat3\" /><ref>{{Citation |title=jina-ai/dalle-flow |date=2022-06-17 |url=https://github.com/jina-ai/dalle-flow |publisher=Jina AI |access-date=2022-06-17}}</ref> Released in 2022 on [[Hugging Face]]'s Spaces platform, '''Craiyon''' (formerly DALL-E Mini until a name change was requested by OpenAI in June 2022) is an AI model based on the original DALL-E that was trained on unfiltered data from the Internet. It attracted substantial media attention in mid-2022 after its release due to its capacity for producing humorous imagery.<ref name=\"CNETmini\" /><ref name=\"DailyDotmini\" /><ref name=\"Polygonmini\" />\n\n==See also==\n* [[15.ai]]\n* [[Artificial intelligence art]]\n* [[Crungus]]\n* [[Imagen (Google Brain)]]\n* [[Midjourney]]\n* [[Stable Diffusion]]\n* [[Prompt engineering]]\n\n==References==\n{{reflist|\n<ref name=\"replicators\">{{Cite journal\n |doi          = 10.1007/s10710-021-09398-5\n |title        = Tim Taylor and Alan Dorin: Rise of the self-replicators\u2014early visions of machines, AI and robots that can reproduce and evolve\n |year         = 2021\n |last1        = Nichele\n |first1       = Stefano\n |journal      = Genetic Programming and Evolvable Machines\n |volume       = 22\n |pages        = 141\u2013145\n |s2cid        = 231930573\n|doi-access= free\n }}</ref>\n<ref name=\"impact\">{{Cite arXiv\n |eprint       = 2102.02503\n |last1        = Tamkin\n |first1       = Alex\n |last2        = Brundage\n |first2       = Miles\n |last3        = Clark\n |first3       = Jack\n |last4        = Ganguli\n |first4       = Deep\n |title        = Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models\n |year         = 2021\n |class        = cs.CL\n}}</ref>\n<ref name=\"gpt1paper\">{{Cite web\n |url          = https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n |title        = Improving Language Understanding by Generative Pre-Training\n |last1        = Radford\n |first1       = Alec\n |last2        = Narasimhan\n |first2       = Karthik\n |last3        = Salimans\n |first3       = Tim\n |last4        = Sutskever\n |first4       = Ilya\n |pages        = 12\n |publisher    = [[OpenAI]]\n |date         = 11 June 2018\n |access-date  = 23 January 2021\n |archive-date = 26 January 2021\n |archive-url  = https://web.archive.org/web/20210126024542/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n |url-status   = live\n}}</ref>\n<ref name=\"gpt2paper\">{{cite journal\n |url          = https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n |title        = Language models are unsupervised multitask learners\n |last1        = Radford\n |first1       = Alec\n |last2        = Wu\n |first2       = Jeffrey\n |last3        = Child\n |first3       = Rewon\n |last4        = Luan\n |first4       = David\n |last5        = Amodei\n |first5       = Dario\n |last6        = Sutskever\n |first6       = Ilua\n |volume       = 1\n |issue        = 8\n |date         = 14 February 2019\n |access-date  = 19 December 2020\n |quote        = \n |journal      = \n |archive-date = 6 February 2021\n |archive-url  = https://web.archive.org/web/20210206183945/https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n |url-status   = live\n}}</ref>\n<ref name=\"gpt3paper\">{{cite arXiv\n| last1       = Brown\n| first1      = Tom B.\n| last2       = Mann\n| first2      = Benjamin\n| last3       = Ryder\n| first3      = Nick\n| last4       = Subbiah\n| first4      = Melanie\n| last5       = Kaplan\n| first5      = Jared\n| last6       = Dhariwal\n| first6      = Prafulla\n| last7       = Neelakantan\n| first7      = Arvind\n| last8       = Shyam\n| first8      = Pranav\n| last9       = Sastry\n| first9      = Girish\n| last10      = Askell\n| first10     = Amanda\n| last11      = Agarwal\n| first11     = Sandhini\n| last12      = Herbert-Voss\n| first12     = Ariel\n| last13      = Krueger\n| first13     = Gretchen\n| last14      = Henighan\n| first14     = Tom\n| last15      = Child\n| first15     = Rewon\n| last16      = Ramesh\n| first16     = Aditya\n| last17      = Ziegler\n| first17     = Daniel M.\n| last18      = Wu\n| first18     = Jeffrey\n| last19      = Winter\n| first19     = Clemens\n| last20      = Hesse\n| first20     = Christopher\n| last21      = Chen\n| first21     = Mark\n| last22      = Sigler\n| first22     = Eric\n| last23      = Litwin\n| first23     = Mateusz\n| last24      = Gray\n| first24     = Scott\n| last25      = Chess\n| first25     = Benjamin\n| last26      = Clark\n| first26     = Jack\n| last27      = Berner\n| first27     = Christopher\n| last28      = McCandlish\n| first28     = Sam\n| last29      = Radford\n| first29     = Alec\n| last30      = Sutskever\n| first30     = Ilya\n| last31      = Amodei\n| first31     = Dario\n| title       = Language Models are Few-Shot Learners\n| eprint      = 2005.14165 \n| date        = July 22, 2020\n| class       = cs.CL\n}}</ref>\n<ref name=\"dallepaper\">{{cite arXiv\n |last1       = Ramesh\n |first1      = Aditya\n |last2       = Pavlov\n |first2      = Mikhail \n |last3       = Goh\n |first3      = Gabriel \n |last4       = Gray\n |first4      = Scott\n |last5       = Voss\n |first5      = Chelsea \n |last6       = Radford\n |first6      = Alec\n |last7       = Chen\n |first7      = Mark\n |last8       = Sutskever\n |first8      = Ilya\n |eprint      = 2102.12092\n |title       = Zero-Shot Text-to-Image Generation\n |class       = cs.LG\n |date        = 24 February 2021\n}}</ref>\n\n<ref name=\"tc\">{{cite web\n |url       = https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n |title       = OpenAI's DALL-E creates plausible images of literally anything you ask it to\n |last       = Coldewey\n |first       = Devin\n |website       = \n |publisher       = \n |date       = 5 January 2021\n |access-date       = 5 January 2021\n |quote       = \n |archive-date       = 6 January 2021\n |archive-url       = https://web.archive.org/web/20210106075542/https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n |url-status       = live\n }}</ref>\n<ref name=\"vb\">{{cite web\n |url          = https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n |title        = OpenAI debuts DALL-E for generating images from text\n |last         = Johnson\n |first        = Khari\n |website      = \n |publisher    = VentureBeat\n |date         = 5 January 2021\n |access-date  = 5 January 2021\n |quote        = \n |archive-date = 5 January 2021\n |archive-url  = https://web.archive.org/web/20210105221534/https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n |url-status   = live\n}}</ref>\n<ref name=\"mittr\">{{cite web\n |url         = https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/\n |title       = This avocado armchair could be the future of AI\n |last        = Heaven\n |first       = Will Douglas\n |website     = \n |publisher   = MIT Technology Review\n |date        = 5 January 2021\n |access-date = 5 January 2021\n |quote       = \n}}</ref>\n<ref name=\"boing\">{{cite web\n |url          = https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n |title        = This AI neural network transforms text captions into art, like a jellyfish Pikachu\n |last         = Dunn\n |first        = Thom\n |website      = \n |publisher    = [[BoingBoing]]\n |date         = 10 February 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 22 February 2021\n |archive-url  = https://web.archive.org/web/20210222001459/https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n |url-status   = live\n}}</ref>\n<ref name=\"nature\">{{cite web\n |url          = https://www.nature.com/immersive/d41586-021-00095-y/index.html\n |title        = Tardigrade circus and a tree of life \u2014 January's best science images\n |last         = Stove\n |first        = Emma\n |website      = \n |publisher    = [[Nature (journal)|Nature]]\n |date         = 5 February 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 8 March 2021\n |archive-url  = https://web.archive.org/web/20210308032636/https://www.nature.com/immersive/d41586-021-00095-y/index.html\n |url-status   = live\n}}</ref>\n<ref name=\"tnw\">{{cite web\n |url          = https://thenextweb.com/neural/2021/01/06/say-hello-to-openais-dall-e-a-gpt-3-powered-bot-that-creates-weird-images-from-text/\n |title        = Say hello to OpenAI's DALL-E, a GPT-3-powered bot that creates weird images from text\n |last         = Macaulay\n |first        = Thomas\n |website      = \n |publisher    = [[TheNextWeb]]\n |date         = 6 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 28 January 2021\n |archive-url  = https://web.archive.org/web/20210128034743/https://thenextweb.com/neural/2021/01/06/say-hello-to-openais-dall-e-a-gpt-3-powered-bot-that-creates-weird-images-from-text/\n |url-status   = live\n}}</ref>\n<ref name=\"extreme\">{{cite web\n |url          = https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n |title        = OpenAI's 'DALL-E' Generates Images From Text Descriptions\n |last         = Whitwam\n |first        = Ryan\n |website      = \n |publisher    = ExtremeTech\n |date         = 6 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 28 January 2021\n |archive-url  = https://web.archive.org/web/20210128064428/https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n |url-status   = live\n}}</ref>\n<ref name=\"engadget\">{{cite web\n |url          = https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n |title        = OpenAI's DALL-E app generates images from just a description\n |last         = Dent\n |first        = Steve\n |website      = \n |publisher    = [[Engadget]]\n |date         = 6 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 27 January 2021\n |archive-url  = https://web.archive.org/web/20210127225652/https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n |url-status   = live\n}}</ref>\n<ref name=\"input\">{{cite web\n |url          = https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n |title        = This AI turns text into surreal, suggestion-driven art\n |last         = Kasana\n |first        = Mehreen\n |website      = \n |publisher    = Input\n |date         = 7 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 29 January 2021\n |archive-url  = https://web.archive.org/web/20210129211643/https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n |url-status   = live\n}}</ref>\n<ref name=\"cnbc\">{{cite web\n |url         = https://www.cnbc.com/2021/01/08/openai-shows-off-dall-e-image-generator-after-gpt-3.html\n |title       = Why everyone is talking about an image generator released by an Elon Musk-backed A.I. lab \n |last        = Shead\n |first       = Sam\n |website     = \n |publisher   = [[CNBC]]\n |date        = 8 January 2021\n |access-date = 2 March 2021\n |quote       = \n}}</ref>\n<ref name=\"zme\">{{cite web\n |url          = https://www.zmescience.com/research/technology/this-ai-module-can-create-stunning-images-out-of-any-text-input/\n |title        = This AI module can create stunning images out of any text input\n |last         = Andrei\n |first        = Mihai\n |website      = \n |publisher    = ZME Science\n |date         = 8 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 29 January 2021\n |archive-url  = https://web.archive.org/web/20210129033439/https://www.zmescience.com/research/technology/this-ai-module-can-create-stunning-images-out-of-any-text-input/\n |url-status   = live\n}}</ref>\n<ref name=\"dale\">{{cite web\n |url          = https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n |title        = Here's how OpenAI's magical DALL-E image generator works\n |last         = Markowitz\n |first        = Dale\n |website      = \n |publisher    = [[TheNextWeb]]\n |date         = 10 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 23 February 2021\n |archive-url  = https://web.archive.org/web/20210223162340/https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n |url-status   = live\n}}</ref>\n<ref name=\"vb2oped\">{{cite web\n |url          = https://venturebeat.com/2021/01/16/openais-text-to-image-engine-dall-e-is-a-powerful-visual-idea-generator/\n |title        = OpenAI's text-to-image engine, DALL-E, is a powerful visual idea generator\n |last         = Grossman\n |first        = Gary\n |website      = \n |publisher    = [[VentureBeat]]\n |date         = 16 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 26 February 2021\n |archive-url  = https://web.archive.org/web/20210226200420/https://venturebeat.com/2021/01/16/openais-text-to-image-engine-dall-e-is-a-powerful-visual-idea-generator/\n |url-status   = live\n}}</ref>\n<ref name=\"forbesoped\">{{cite web\n |url          = https://www.forbes.com/sites/robtoews/2021/01/18/ai-and-creativity-why-openais-latest-model-is-a-big-deal/\n |title        = AI And Creativity: Why OpenAI's Latest Model Matters\n |last         = Toews\n |first        = Rob\n |work    = [[Forbes]]\n |date         = 18 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 12 February 2021\n |archive-url  = https://web.archive.org/web/20210212183950/https://www.forbes.com/sites/robtoews/2021/01/18/ai-and-creativity-why-openais-latest-model-is-a-big-deal/\n |url-status   = live\n}}</ref>\n<ref name=\"nbc\">{{cite web\n |url          = https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n |title        = Here's DALL-E: An algorithm learned to draw anything you tell it\n |last         = Ehrenkranz\n |first        = Melanie\n |website      = \n |publisher    = [[NBC News]]\n |date         = 27 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 20 February 2021\n |archive-url  = https://web.archive.org/web/20210220164655/https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n |url-status   = live\n}}</ref>\n<ref name=\"axios\">{{cite web\n |url         = https://www.axios.com/openai-artificial-intelligence-model-images-dall-e-5c977633-81cd-450c-8ce5-a30e5f0e90e7.html\n |title       = A new AI model draws images from text\n |last        = Walsh\n |first       = Bryan\n |website     = \n |publisher   = [[Axios (website)|Axios]]\n |date        = 5 January 2021\n |access-date = 2 March 2021\n |quote       = \n}}</ref>\n<ref name=\"synced\">{{cite web\n |url          = https://syncedreview.com/2021/01/05/this-time-openais-gpt-3-generates-images-from-text/\n |title        = For Its Latest Trick, OpenAI's GPT-3 Generates Images From Text Captions\n |website      = \n |publisher    = Synced\n |date         = 5 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 6 January 2021\n |archive-url  = https://web.archive.org/web/20210106185429/https://syncedreview.com/2021/01/05/this-time-openais-gpt-3-generates-images-from-text/\n |url-status   = live\n}}</ref>\n<ref name=\"bbc\">{{cite web\n |url          = https://www.bbc.com/news/technology-55559463\n |title        = AI draws dog-walking baby radish in a tutu\n |last         = Wakefield\n |first        = Jane\n |website      = \n |publisher    = [[British Broadcasting Corporation]]\n |date         = 6 January 2021\n |access-date  = 3 March 2021\n |quote        = \n |archive-date = 2 March 2021\n |archive-url  = https://web.archive.org/web/20210302170623/https://www.bbc.com/news/technology-55559463\n |url-status   = live\n}}</ref>\n<ref name=\"cnn\">{{cite web\n |url         = https://www.cnn.com/2021/01/08/tech/artificial-intelligence-openai-images-from-text/index.html\n |title       = A radish in a tutu walking a dog? This AI can draw it really well\n |last        = Metz\n |first       = Rachel\n |website     = \n |publisher   = CNN\n |date        = 2 February 2021\n |access-date = 2 March 2021\n |quote       = \n}}</ref>\n<ref name=\"wired\">{{cite magazine\n |url          = https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n |title        = This AI Could Go From 'Art' to Steering a Self-Driving Car\n |last         = Knight\n |first        = Will\n |magazine      = Wired\n |date         = 26 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 21 February 2021\n |archive-url  = https://web.archive.org/web/20210221010223/https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n |url-status   = live\n}}</ref>\n<ref name=\"CNETmini\">{{cite web\n |url          = https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n |title        = Everything to Know About Dall-E Mini, the Mind-Bending AI Art Creator\n |last         = Carson\n |first        = Erin\n |website      = [[CNET]]\n |publisher    = \n |date         = 14 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 15 June 2022\n |archive-url  = https://web.archive.org/web/20220615085705/https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n |url-status   = live\n}}</ref>\n<ref name=\"DailyDotmini\">{{cite web\n |url          = https://www.dailydot.com/unclick/dall-e-mini-memes/\n |title        = AI program DALL-E mini prompts some truly cursed images\n |last         = Schroeder\n |first        = Audra\n |website      = [[Daily Dot]]\n |publisher    = \n |date         = 9 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 10 June 2022\n |archive-url  = https://web.archive.org/web/20220610212300/https://www.dailydot.com/unclick/dall-e-mini-memes/\n |url-status   = live\n}}</ref>\n<ref name=\"Polygonmini\">{{cite web\n |url          = https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n |title        = People are using DALL-E mini to make meme abominations \u2014 like pug Pikachu\n |last         = Diaz\n |first        = Ana\n |website      = [[Polygon (website)|Polygon]]\n |publisher    = \n |date         = 15 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 15 June 2022\n |archive-url  = https://web.archive.org/web/20220615151753/https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n |url-status   = live\n}}</ref>\n<ref name=\"VentureBeat3\">{{cite web\n |url = https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n |title = How DALL-E 2 could solve major computer vision challenges\n |last = Sahar Mor\n |first = Stripe\n |website = [[VentureBeat]]\n |publisher = \n |date = 16 April 2022\n |access-date = 15 June 2022\n |quote = \n |archive-date = 24 May 2022\n |archive-url = https://web.archive.org/web/20220524133956/https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n |url-status = live\n}}</ref>\n<!-- Unused\n<ref name=\"newscientist\">{{cite web\n |url          = https://www.newscientist.com/article/2264022-ai-illustrator-draws-imaginative-pictures-to-go-with-text-captions/\n |title        = AI illustrator draws imaginative pictures to go with text captions\n |last         = Stokel-Walker\n |first        = Chris\n |website      = \n |publisher    = [[New Scientist]]\n |date         = 5 January 2021\n |access-date  = 4 March 2021\n |quote        = \n |archive-date = 28 January 2021\n |archive-url  = https://web.archive.org/web/20210128005947/https://www.newscientist.com/article/2264022-ai-illustrator-draws-imaginative-pictures-to-go-with-text-captions/\n |url-status   = live\n}}</ref>\n-->}}\n\n==External links==\n{{Commons category}}\n*[https://openai.com/dall-e-2/ DALL E 2 website]\n\n{{Differentiable computing}}\n\n[[Category:Text-to-image generation]]\n[[Category:Deep learning software applications]]\n[[Category:Unsupervised learning]]\n[[Category:OpenAI]]"}