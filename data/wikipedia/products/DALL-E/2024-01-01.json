{"title": "DALL-E", "page_id": 66303034, "revision_id": 1192428517, "revision_timestamp": "2023-12-29T08:28:11Z", "content": "{{Short description|Image-generating deep-learning model}}\n{{Use dmy dates|date=June 2022}}\n{{Infobox software\n| name = DALL\u00b7E\n| logo = DALL-E 2 Signature.svg\n| logo caption = Watermark present on DALL\u00b7E images generated on OpenAI's {{url|https://labs.openai.com}}\n| screenshot = DALL\u00b7E 2023-10.png\n| caption = An image generated by DALL-E 3 with GPT-4 based on the text prompt \"A modern architectural building with large glass windows, situated on a cliff overlooking a serene ocean at sunset.\"\n| author = \n| developer = [[OpenAI]]\n| released = {{start date and age|2021|1|5}}\n| latest release version = DALL\u00b7E 3\n| latest release date = {{start date and age|2023|8|10}}\n| repo = \n| programming language = \n| operating system = \n| genre = [[Text-to-image model]]\n| license = \n| website = {{url|https://labs.openai.com/}}\n}}\n{{Artificial intelligence}}\n\n'''DALL\u00b7E''', '''DALL\u00b7E 2''', and '''DALL\u00b7E 3''' are [[text-to-image model]]s developed by [[OpenAI]] using [[deep learning]] methodologies to generate [[digital image]]s from [[natural language]] descriptions, called \"[[Prompt (natural language)|prompts]]\". \n\nDALL\u00b7E 3 was released natively into [[ChatGPT]] for ChatGPT Plus and ChatGPT Enterprise customers in October 2023,<ref>{{Cite web |last=David |first=Emilia |date=2023-09-20 |title=OpenAI releases third version of DALL\u00b7E |url=https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai |access-date=2023-09-21 |website=The Verge |language=en-US |archive-date=20 September 2023 |archive-url=https://web.archive.org/web/20230920192429/https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai |url-status=live }}</ref> with availability via OpenAI's API<ref>{{Cite web |title=OpenAI Platform |url=https://platform.openai.com/ |access-date=2023-11-10 |website=platform.openai.com |language=en |archive-date=20 March 2023 |archive-url=https://web.archive.org/web/20230320023933/https://platform.openai.com/ |url-status=live }}</ref> and \"Labs\" platform provided in early November.<ref>{{Cite web |last=Niles |first=Raymond |date=2023-11-10 |orig-date=Updated this week |title=DALL-E 3 API |url=https://help.openai.com/en/articles/8555480-dall-e-3-api |access-date=2023-11-10 |website=OpenAI help Center |language=en |archive-date=10 November 2023 |archive-url=https://web.archive.org/web/20231110182305/https://help.openai.com/en/articles/8555480-dall-e-3-api |url-status=live }}</ref> Microsoft implemented the model in Bing's Image Creator tool and plans to implement it into their Designer app.<ref>{{Cite web |last=Mehdi |first=Yusuf |date=2023-09-21 |title=Announcing Microsoft Copilot, your everyday AI companion |url=https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/ |access-date=2023-09-21 |website=The Official Microsoft Blog |language=en-US |archive-date=21 September 2023 |archive-url=https://web.archive.org/web/20230921150139/https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/ |url-status=live }}</ref>\n\n== History and background ==\nDALL\u00b7E was revealed by OpenAI in a blog post in 5 January 2021, and uses a version of [[GPT-3]]<ref name=\"vb\" /> modified to generate images. \n\nIn 6 April 2022, OpenAI announced DALL\u00b7E 2, a successor designed to generate more realistic images at higher resolutions that \"can combine concepts, attributes, and styles\".<ref>{{Cite web |title=DALL\u00b7E 2 |url=https://openai.com/dall-e-2/ |url-status=live |archive-url=https://web.archive.org/web/20220406141035/https://openai.com/dall-e-2/ |archive-date=6 April 2022 |access-date=2022-07-06 |website=OpenAI |language=en-US}}</ref> On 20 July 2022, DALL\u00b7E 2 entered into a beta phase with invitations sent to 1 million waitlisted individuals;<ref name=\":3\">{{Cite web |date=2022-07-20 |title=DALL\u00b7E Now Available in Beta |url=https://openai.com/blog/dall-e-now-available-in-beta/ |url-status=live |archive-url=https://web.archive.org/web/20220720162939/https://openai.com/blog/dall-e-now-available-in-beta/ |archive-date=20 July 2022 |access-date=2022-07-20 |website=OpenAI |language=en}}</ref> users could generate a certain number of images for free every month and may purchase more.<ref name=\":4\">{{Cite news |last=Allyn |first=Bobby |date=2022-07-20 |title=Surreal or too real? Breathtaking AI tool DALL\u00b7E takes its images to a bigger stage |language=en |work=NPR |url=https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test |url-status=live |access-date=2022-07-20 |archive-url=https://web.archive.org/web/20220720170255/https://www.npr.org/2022/07/20/1112331013/dall-e-ai-art-beta-test |archive-date=20 July 2022}}</ref> Access had previously been restricted to pre-selected users for a research preview due to concerns about [[Ethics of artificial intelligence|ethics]] and safety.<ref>{{Cite web |title=DALL\u00b7E Waitlist |url=https://labs.openai.com/ |url-status=live |archive-url=https://web.archive.org/web/20220704184756/https://labs.openai.com/ |archive-date=4 July 2022 |access-date=2022-07-06 |website=labs.openai.com |language=en}}</ref><ref>{{Cite web |date=2022-06-18 |title=From Trump Nevermind babies to deep fakes: DALL\u00b7E and the ethics of AI art |url=https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art |url-status=live |archive-url=https://web.archive.org/web/20220706125540/https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art |archive-date=6 July 2022 |access-date=2022-07-06 |website=the Guardian |language=en}}</ref> On 28 September 2022, DALL\u00b7E 2 was opened to everyone and the waitlist requirement was removed.<ref>{{Cite web |date=2022-09-28 |title=DALL\u00b7E Now Available Without Waitlist |url=https://openai.com/blog/dall-e-now-available-without-waitlist/ |url-status=live |archive-url=https://web.archive.org/web/20221004093145/https://openai.com/blog/dall-e-now-available-without-waitlist/ |archive-date=4 October 2022 |access-date=2022-10-05 |website=OpenAI |language=en}}</ref> In September 2023, OpenAI announced their latest image model, DALL\u00b7E 3, capable of understanding \"significantly more nuance and detail\" than previous iterations.<ref name=\":5\">{{Cite web |title=DALL\u00b7E 3 |url=https://openai.com/dall-e-3/ |url-status=live |archive-url=https://web.archive.org/web/20230920225833/https://openai.com/dall-e-3 |archive-date=20 September 2023 |access-date=2023-09-21 |website=OpenAI |language=en-US}}</ref> In early November 2022, OpenAI released DALL\u00b7E 2 as an [[API]], allowing developers to integrate the model into their own applications. [[Microsoft]] unveiled their implementation of DALL\u00b7E 2 in their Designer app and Image Creator tool included in [[Microsoft Bing|Bing]] and [[Microsoft Edge]].<ref>{{Cite web |date=2022-11-03 |title=DALL\u00b7E API Now Available in Public Beta |url=https://openai.com/blog/dall-e-api-now-available-in-public-beta |url-status=live |archive-url=https://web.archive.org/web/20221119222937/https://openai.com/blog/dall-e-api-now-available-in-public-beta/ |archive-date=19 November 2022 |access-date=2022-11-19 |website=OpenAI |language=en}}</ref> The API operates on a cost per image basis, with prices varying depending on image resolution. Volume discounts are available to companies working with OpenAI\u2019s enterprise team.<ref>{{Cite news |last=Wiggers |first=Kyle |date=2022-11-03 |title=Now anyone can build apps that use DALL\u00b7E 2 to generate images |work=[[TechCrunch]] |url=https://techcrunch.com/2022/11/03/now-anyone-can-build-apps-that-use-dall-e-2-to-generate-images |url-status=live |access-date=2022-11-19 |archive-url=https://web.archive.org/web/20221119154222/https://techcrunch.com/2022/11/03/now-anyone-can-build-apps-that-use-dall-e-2-to-generate-images/ |archive-date=19 November 2022}}</ref>\n\nThe software's name is a [[portmanteau]] of the names of animated robot [[Pixar]] character [[WALL-E (character)|WALL-E]] and the Spanish surrealist artist [[Salvador Dal\u00ed]].<ref name=\"tc\" /><ref name=\"vb\" />\n\n== Technology ==\nThe first [[generative pre-trained transformer]] (GPT) model was initially developed by OpenAI in 2018,<ref name=\"gpt1paper\" /> using a [[Transformer (machine learning model)|Transformer]] architecture. The first iteration, GPT-1,<ref>{{cite web | url=https://www.makeuseof.com/gpt-models-explained-and-compared/ | title=GPT-1 to GPT-4: Each of OpenAI's GPT Models Explained and Compared | date=11 April 2023 | access-date=29 April 2023 | archive-date=15 April 2023 | archive-url=https://web.archive.org/web/20230415175013/https://www.makeuseof.com/gpt-models-explained-and-compared/ | url-status=live }}</ref> was scaled up to produce [[GPT-2]] in 2019;<ref name=\"gpt2paper\" /> in 2020 it was scaled up again to produce [[GPT-3]], with 175 billion parameters.<ref name=\"gpt3paper\" /><ref name=\"vb\" /><ref name=\"dallepaper\" /> \n\nDALL\u00b7E's model is a [[multimodal interaction|multimodal]] implementation of GPT-3<ref name=\"impact\" /> with 12 billion parameters<ref name=\"vb\" /> which \"swaps text for pixels\", trained on text-image pairs from the Internet.<ref name=\"mittr\" /> In detail, the input to the Transformer model is a sequence of tokenized image caption followed by tokenized image patches. The image caption is in English, tokenized by [[byte pair encoding]] (vocabulary size 16384), and can be up to 256 tokens long. Each image is a 256x256 RGB image, divided into 32x32 patches of 4x4 each. Each patch is then converted by a discrete [[variational autoencoder]] to a token (vocabulary size 8192).\n\nDALL\u00b7E was developed and announced to the public in conjunction with CLIP (Contrastive Language-Image Pre-training).<ref name=\"mittr\" /> CLIP is a separate model based on [[zero-shot learning]] that was trained on 400 million pairs of images with text captions [[Web scraping|scraped]] from the Internet.<ref name=\"vb\" /><ref name=\"mittr\" /><ref>{{Cite web |title='DALL\u00b7E' AI generates an image out of anything you describe |url=https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html |access-date=2022-07-18 |website=Engadget |date=6 January 2021 |language=en-US |archive-date=27 January 2021 |archive-url=https://web.archive.org/web/20210127225652/https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html |url-status=live }}</ref> Its role is to \"understand and rank\" DALL\u00b7E's output by predicting which caption from a list of 32,768 captions randomly selected from the dataset (of which one was the correct answer) is most appropriate for an image. This model is used to filter a larger initial list of images generated by DALL\u00b7E to select the most appropriate outputs.<ref name=\"tc\" /><ref name=\"mittr\" />\n\nDALL\u00b7E 2 uses 3.5 billion parameters, a smaller number than its predecessor.<ref name=\":2\">{{Cite arXiv |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |date=2022-04-12 |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |class=cs.CV |eprint=2204.06125}}</ref> DALL\u00b7E 2 uses a [[diffusion model]] conditioned on CLIP image embeddings, which, during inference, are generated from CLIP text embeddings by a prior model.<ref name=\":2\" />\n\n=== Contrastive Language-Image Pre-training (CLIP) ===\nContrastive Language-Image Pre-training<ref>{{Cite arXiv |last1=Radford |first1=Alec |last2=Kim |first2=Jong Wook |last3=Hallacy |first3=Chris |last4=Ramesh |first4=Aditya |last5=Goh |first5=Gabriel |last6=Agarwal |first6=Sandhini |last7=Sastry |first7=Girish |last8=Askell |first8=Amanda |last9=Mishkin |first9=Pamela |last10=Clark |first10=Jack |last11=Krueger |first11=Gretchen |last12=Sutskever |first12=Ilya |date=2021 |title=Learning Transferable Visual Models From Natural Language Supervision |class=cs.CV |eprint=2103.00020}}</ref> is a technique for training a pair of models. One model takes in a piece of text and outputs a single vector. Another takes in an image and outputs a single vector. \n\nTo train such a pair of models, one would start by preparing a large dataset of image-caption pairs, then sample batches of size <math>N</math>. Let the outputs from the text and image models be respectively <math>v_1, ..., v_N, w_1, ..., w_N </math>. The loss incurred on this batch is<math display=\"block\">-\\sum_{i} \\ln\\frac{e^{v_i \\cdot w_i}}{\\sum_j e^{v_i \\cdot w_j}} -\\sum_{j} \\ln\\frac{e^{v_j \\cdot w_j}}{\\sum_i e^{v_i \\cdot w_j}} </math>In words, it is the total sum of cross-entropy loss across every column and every row of the matrix <math>[v_i \\cdot w_j]_{i, j}</math>.\n\nThe models released were trained on a dataset \"WebImageText\", containing 400 million pairs of image-captions. The total number of words is similar to WebText, which contains about 40 GB of text.\n\n== Capabilities ==\nDALL\u00b7E can generate imagery in multiple styles, including [[photorealistic]] imagery, [[paintings]], and [[emoji]].<ref name=\"vb\" /> It can \"manipulate and rearrange\" objects in its images,<ref name=\"vb\" /> and can correctly place design elements in novel compositions without explicit instruction. Thom Dunn writing for ''[[Boing Boing|BoingBoing]]'' remarked that \"For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL\u00b7E often draws the handkerchief, hands, and feet in plausible locations.\"<ref name=\"boing\" /> DALL\u00b7E showed the ability to \"fill in the blanks\" to infer appropriate details without specific prompts, such as adding Christmas imagery to prompts commonly associated with the celebration,<ref name=\"extreme\" /> and appropriately placed shadows to images that did not mention them.<ref name=\"engadget\" /> Furthermore, DALL\u00b7E exhibits a broad understanding of visual and design trends.{{Citation needed|date=July 2022}}\n\nDALL\u00b7E can produce images for a wide variety of arbitrary descriptions from various viewpoints<ref name=\":0\">{{cite arXiv |eprint=2204.13807 |class=cs.CV |first1=Gary |last1=Marcus |first2=Ernest |last2=Davis |title=A very preliminary analysis of DALL-E 2 |date=2022-05-02 |last3=Aaronson |first3=Scott}}</ref> with only rare failures.<ref name=\"tc\" /> Mark Riedl, an associate professor at the [[Georgia Tech]] School of Interactive Computing, found that DALL-E could blend concepts (described as a key element of human [[creativity]]).<ref name=\"cnbc\" /><ref name=\"bbc\" />\n\nIts visual reasoning ability is sufficient to solve [[Raven's Progressive Matrices|Raven's Matrices]] (visual tests often administered to humans to measure intelligence).<ref name=\"dale\" /><ref>{{Cite web |date=2021-01-05 |title=DALL\u00b7E: Creating Images from Text |url=https://openai.com/blog/dall-e/ |access-date=2022-08-13 |website=OpenAI |language=en |archive-date=27 March 2021 |archive-url=https://web.archive.org/web/20210327133043/https://openai.com/blog/dall-e/ |url-status=live }}</ref>\n\n[[File:DALL-E 3 AI image of an avocado speaking.png|thumb|An image of accurate text generated by DALL-E 3 based on the text prompt \"An illustration of an avocado sitting in a therapist's chair, saying 'I just feel so empty inside' with a pit-sized hole in its center. The therapist, a spoon, scribbles notes.\"]]\nDALL\u00b7E 3 follows complex prompts with more accuracy and detail than its predecessors, and is able to generate more coherent and accurate text.<ref>{{Cite web |last=Edwards |first=Benj |date=2023-09-20 |title=OpenAI's new AI image generator pushes the limits in detail and prompt fidelity |url=https://arstechnica.com/information-technology/2023/09/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/ |access-date=2023-09-21 |website=Ars Technica |language=en-us |archive-date=21 September 2023 |archive-url=https://web.archive.org/web/20230921130853/https://arstechnica.com/information-technology/2023/09/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/ |url-status=live }}</ref><ref name=\":5\" /> DALL-E 3 integrates into ChatGPT. <ref name=\":5\" />\n\n=== Image modification ===\n{{multiple image\n| align             = \n| width             = 130\n| image1            = DALL-E 2 variation 1.png\n| image2            = DALL-E 2 variation 2.png\n| footer            = Two \"variations\" of ''[[Girl With a Pearl Earring]]'' generated with DALL\u00b7E 2\n}}\nGiven an existing image, DALL\u00b7E 2 can produce \"variations\" of the image as individual outputs based on the original, as well as edit the image to modify or expand upon it. DALL\u00b7E 2's \"inpainting\" and \"outpainting\" use context from an image to fill in missing areas using a [[Artistic medium|medium]] consistent with the original, following a given prompt.\n\nFor example, this can be used to insert a new subject into an image, or expand an image beyond its original borders.<ref>{{Cite web |last=Coldewey |first=Devin |date=2022-04-06 |title=New OpenAI tool draws anything, bigger and better than ever |url=https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/ |access-date=2022-11-26 |website=TechCrunch |language=en-US |archive-date=6 May 2023 |archive-url=https://web.archive.org/web/20230506073242/https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/ |url-status=live }}</ref> According to OpenAI, \"Outpainting takes into account the image\u2019s existing visual elements \u2014 including shadows, reflections, and textures \u2014 to maintain the context of the original image.\"<ref>{{Cite web |date=2022-08-31 |title=DALL\u00b7E: Introducing Outpainting |url=https://openai.com/blog/dall-e-introducing-outpainting/ |access-date=2022-11-26 |website=OpenAI |language=en |archive-date=26 November 2022 |archive-url=https://web.archive.org/web/20221126173545/https://openai.com/blog/dall-e-introducing-outpainting/ |url-status=live }}</ref>\n\n=== Technical limitations ===\nDALL\u00b7E 2's language understanding has limits. It is sometimes unable to distinguish \"A yellow book and a red vase\" from \"A red book and a yellow vase\" or \"A panda making latte art\" from \"Latte art of a panda\".<ref>{{cite arXiv |eprint=2205.11487 |class=cs.CV |first1=Chitwan |last1=Saharia |first2=William |last2=Chan |title=Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding |date=2022-05-23 |last3=Saxena |first3=Saurabh |last4=Li |first4=Lala |last5=Whang |first5=Jay |last6=Denton |first6=Emily |last7=Ghasemipour |first7=Seyed Kamyar Seyed |last8=Ayan |first8=Burcu Karagol |last9=Mahdavi |first9=S. Sara |last10=Lopes |first10=Rapha Gontijo |last11=Salimans |first11=Tim}}</ref> It generates images of \"an astronaut riding a horse\" when presented with the prompt \"a horse riding an astronaut\".<ref>{{Cite web |last=Marcus |first=Gary |date=2022-05-28 |title=Horse rides astronaut |url=https://garymarcus.substack.com/p/horse-rides-astronaut |access-date=2022-06-18 |website=The Road to AI We Can Trust |archive-date=19 June 2022 |archive-url=https://web.archive.org/web/20220619135711/https://garymarcus.substack.com/p/horse-rides-astronaut |url-status=live }}</ref> It also fails to generate the correct images in a variety of circumstances. Requesting more than three objects, negation, numbers, and [[Conjunction (grammar)|connected sentences]] may result in mistakes, and object features may appear on the wrong object.<ref name=\":0\" /> Additional limitations include handling text - which, even with legible lettering, almost invariably results in dream-like gibberish - and its limited capacity to address scientific information, such as astronomy or medical imagery.<ref>{{Cite web |last=Strickland |first=Eliza |date=2022-07-14 |title=DALL\u00b7E 2's Failures Are the Most Interesting Thing About It |url=https://spectrum.ieee.org/openai-dall-e-2 |access-date=2022-08-16 |website=IEEE Spectrum |language=en |archive-date=15 July 2022 |archive-url=https://web.archive.org/web/20220715204154/https://spectrum.ieee.org/openai-dall-e-2 |url-status=live }}</ref>\n\n== Ethical concerns ==\nDALL\u00b7E 2's reliance on public datasets influences its results and leads to [[algorithmic bias]] in some cases, such as generating higher numbers of men than women for requests that do not mention gender.<ref>{{Cite web |last=STRICKLAND |first=ELIZA |date=2022-07-14 |title=DALL-E 2's Failures Are the Most Interesting Thing About It |url=https://spectrum.ieee.org/openai-dall-e-2 |access-date=2022-07-15 |website=IEEE Spectrum |language=en |archive-date=15 July 2022 |archive-url=https://web.archive.org/web/20220715204154/https://spectrum.ieee.org/openai-dall-e-2 |url-status=live }}</ref> DALL-E 2's training data was filtered to remove violent and sexual imagery, but this was found to increase bias in some cases such as reducing the frequency of women being generated.<ref name=\":1\" /> OpenAI hypothesize that this may be because women were more likely to be sexualized in training data which caused the filter to influence results.<ref name=\":1\">{{Cite web |date=2022-06-28 |title=DALL\u00b7E 2 Pre-Training Mitigations |url=https://openai.com/blog/dall-e-2-pre-training-mitigations/ |access-date=2022-07-18 |website=OpenAI |language=en |archive-date=19 July 2022 |archive-url=https://web.archive.org/web/20220719044249/https://openai.com/blog/dall-e-2-pre-training-mitigations/ |url-status=live }}</ref> In September 2022, OpenAI confirmed to ''[[The Verge]]'' that DALL-E invisibly inserts phrases into user prompts to address bias in results; for instance, \"black man\" and \"Asian woman\" are inserted into prompts that do not specify gender or race.<ref>{{cite web|url=https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped|author=James Vincent|date=September 29, 2022|title=OpenAI's image generator DALL\u00b7E is available for anyone to use immediately|website=[[The Verge]]|access-date=29 September 2022|archive-date=29 September 2022|archive-url=https://web.archive.org/web/20220929061004/https://www.theverge.com/2022/9/28/23376328/ai-art-image-generator-dall-e-access-waitlist-scrapped|url-status=live}}</ref>\n\nA concern about DALL\u00b7E 2 and similar image generation models is that they could be used to propagate [[deepfake]]s and other forms of misinformation.<ref name=\"Taylor\">{{cite news|last=Taylor|first=Josh|title=From Trump Nevermind babies to deep fakes: DALL-E and the ethics of AI art|url=https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art|website=The Guardian|date=18 June 2022|accessdate=2 August 2022|archive-date=6 July 2022|archive-url=https://web.archive.org/web/20220706125540/https://www.theguardian.com/technology/2022/jun/19/from-trump-nevermind-babies-to-deep-fakes-dall-e-and-the-ethics-of-ai-art|url-status=live}}</ref><ref name=\"wired2\">{{cite magazine |last1=Knight |first1=Will |title=When AI Makes Art, Humans Supply the Creative Spark |url=https://www.wired.com/story/when-ai-makes-art/ |magazine=Wired |date=13 July 2022 |accessdate=2 August 2022 |archive-date=2 August 2022 |archive-url=https://web.archive.org/web/20220802162402/https://www.wired.com/story/when-ai-makes-art/ |url-status=live }}</ref> As an attempt to mitigate this, the software rejects prompts involving public figures and uploads containing human faces.<ref name=\"vice\">{{cite news|title=DALL-E Is Now Generating Realistic Faces of Fake People|last=Rose|first=Janus|url=https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people|date=24 June 2022|work=Vice|accessdate=2 August 2022|archive-date=30 July 2022|archive-url=https://web.archive.org/web/20220730133250/https://www.vice.com/en/article/g5vbx9/dall-e-is-now-generating-realistic-faces-of-fake-people|url-status=live}}</ref> Prompts containing potentially objectionable content are blocked, and uploaded images are analyzed to detect offensive material.<ref name=\"docs\" /> A disadvantage of prompt-based filtering is that it is easy to bypass using alternative phrases that result in a similar output. For example, the word \"blood\" is filtered, but \"ketchup\" and \"red liquid\" are not.<ref>{{cite magazine|last1=Lane|first1=Laura|title=DALL-E, Make Me Another Picasso, Please|url=https://www.newyorker.com/magazine/2022/07/11/dall-e-make-me-another-picasso-please|magazine=The New Yorker|date=1 July 2022|accessdate=2 August 2022|archive-date=2 August 2022|archive-url=https://web.archive.org/web/20220802162403/https://www.newyorker.com/magazine/2022/07/11/dall-e-make-me-another-picasso-please|url-status=live}}</ref><ref name=\"docs\">{{cite web|title=DALL\u00b7E 2 Preview - Risks and Limitations|author=OpenAI|website=[[GitHub]]|url=https://github.com/openai/dalle-2-preview/blob/main/system-card.md|date=19 June 2022|accessdate=2 August 2022|archive-date=2 August 2022|archive-url=https://web.archive.org/web/20220802162403/https://github.com/openai/dalle-2-preview/blob/main/system-card.md|url-status=live}}</ref>\n\nAnother concern about DALL\u00b7E 2 and similar models is that they could cause [[technological unemployment]] for artists, photographers, and graphic designers due to their accuracy and popularity.<ref>{{cite web|title=OpenAI: Will DALL\u00b7E 2 kill creative careers?|last=Goldman|first=Sharon|date=26 July 2022|url=https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/|access-date=16 August 2022|archive-date=15 August 2022|archive-url=https://web.archive.org/web/20220815014607/https://venturebeat.com/business/openai-will-dall-e-2-kill-creative-careers/|url-status=live}}</ref><ref>{{cite web|title=DALL-E 2: A dream tool and an existential threat to visual artists|last=Blain|first=Loz|date=29 July 2022|url=https://newatlas.com/computers/dall-e-2-ai-art/|access-date=16 August 2022|archive-date=17 August 2022|archive-url=https://web.archive.org/web/20220817064216/https://newatlas.com/computers/dall-e-2-ai-art/|url-status=live}}</ref> DALL\u00b7E 3 is designed to block users from generating art in the style of currently-living artists.<ref name=\":5\" />\n\n== Reception ==\n[[File:DALL-E radish.jpg|thumb|upright=1.35 |Images generated by DALL-E upon the prompt: \"an illustration of a baby daikon radish in a tutu walking a dog\"]]\nMost coverage of DALL\u00b7E focuses on a small subset of \"surreal\"<ref name=\"mittr\" /> or \"quirky\"<ref name=\"cnbc\" /> outputs. DALL-E's output for \"an illustration of a baby daikon radish in a tutu walking a dog\" was mentioned in pieces from ''Input'',<ref name=\"input\" /> [[NBC]],<ref name=\"nbc\" /> ''[[Nature (journal)|Nature]]'',<ref name=\"nature\" /> and other publications.<ref name=\"vb\" /><ref name=\"wired\" /><ref name=\"cnn\" /> Its output for \"an armchair in the shape of an avocado\" was also widely covered.<ref name=\"mittr\" /><ref name=\"bbc\" />\n\n''[[ExtremeTech]]'' stated \"you can ask DALL\u00b7E for a picture of a phone or vacuum cleaner from a specified period of time, and it understands how those objects have changed\".<ref name=\"extreme\" /> ''[[Engadget]]'' also noted its unusual capacity for \"understanding how telephones and other objects change over time\".<ref name=\"engadget\" />\n\nAccording to ''[[MIT Technology Review]]'', one of OpenAI's objectives was to \"give language models a better grasp of the everyday concepts that humans use to make sense of things\".<ref name=\"mittr\" />\n\nWall Street investors have had a positive reception of DALL\u00b7E 2, with some firms thinking it could represent a turning point for a future multi-trillion dollar industry. By mid-2019, OpenAI had already received over $1 billion in funding from [[Microsoft]] and Khosla Ventures,<ref>{{Cite web |last=Leswing |first=Kif |title=Why Silicon Valley is so excited about awkward drawings done by artificial intelligence |url=https://www.cnbc.com/2022/10/08/generative-ai-silicon-valleys-next-trillion-dollar-companies.html |access-date=2022-12-01 |website=CNBC |date=8 October 2022 |language=en |archive-date=29 July 2023 |archive-url=https://web.archive.org/web/20230729005158/https://www.cnbc.com/2022/10/08/generative-ai-silicon-valleys-next-trillion-dollar-companies.html |url-status=live }}</ref><ref>{{Cite web |last=Etherington |first=Darrell |date=2019-07-22 |title=Microsoft invests $1 billion in OpenAI in new multiyear partnership |url=https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/ |access-date=2023-09-21 |website=TechCrunch |language=en-US |archive-date=22 July 2019 |archive-url=https://web.archive.org/web/20190722173752/https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/ |url-status=live }}</ref><ref>{{Cite web |title=OpenAI's first VC backer weighs in on generative A.I. |url=https://fortune.com/2023/02/02/openais-first-vc-backer-khosla-ventures-weighs-in-on-the-future-of-generative-a-i/ |access-date=2023-09-21 |website=Fortune |language=en |archive-date=23 October 2023 |archive-url=https://web.archive.org/web/20231023211810/https://fortune.com/2023/02/02/openais-first-vc-backer-khosla-ventures-weighs-in-on-the-future-of-generative-a-i/ |url-status=live }}</ref> and in January 2023, following the launch of DALL\u00b7E 2 and ChatGPT, received an additional $10 billion in funding from Microsoft.<ref>{{Cite news |last1=Metz |first1=Cade |last2=Weise |first2=Karen |date=2023-01-23 |title=Microsoft to Invest $10 Billion in OpenAI, the Creator of ChatGPT |language=en-US |work=The New York Times |url=https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-intelligence.html |access-date=2023-09-21 |issn=0362-4331 |archive-date=21 September 2023 |archive-url=https://web.archive.org/web/20230921171844/https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-intelligence.html |url-status=live }}</ref>\n\nJapan's [[anime]] community has had a negative reaction to DALL\u00b7E 2 and similar models.<ref>{{Cite web |date=2022-10-27 |title=AI-generated art sparks furious backlash from Japan's anime community |url=https://restofworld.org/2022/ai-backlash-anime-artists/ |access-date=2023-01-03 |website=Rest of World |language=en-US |archive-date=31 December 2022 |archive-url=https://web.archive.org/web/20221231155114/https://restofworld.org/2022/ai-backlash-anime-artists/ |url-status=live }}</ref><ref>{{Cite news |last=Roose |first=Kevin |date=2022-09-02 |title=An A.I.-Generated Picture Won an Art Prize. Artists Aren't Happy. |language=en-US |work=The New York Times |url=https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html |access-date=2023-01-03 |issn=0362-4331 |archive-date=31 May 2023 |archive-url=https://web.archive.org/web/20230531203858/https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html |url-status=live }}</ref><ref>{{Cite web |last=Daws |first=Ryan |date=2022-12-15 |title=ArtStation backlash increases following AI art protest response |url=https://www.artificialintelligence-news.com/2022/12/15/artstation-backlash-increases-ai-art-protest-response/ |access-date=2023-01-03 |website=AI News |language=en-GB |archive-date=3 January 2023 |archive-url=https://web.archive.org/web/20230103233938/https://www.artificialintelligence-news.com/2022/12/15/artstation-backlash-increases-ai-art-protest-response/ |url-status=live }}</ref> Two arguments are typically presented by artists against the software. The first is that AI art is not art because it is not created by a human with intent. \"The juxtaposition of AI-generated images with their own work is degrading and undermines the time and skill that goes into their art. AI-driven image generation tools have been heavily criticized by artists because they are trained on human-made art scraped from the web.\"<ref name=\":3\" /> The second is the trouble with [[copyright law]], and data text-to-image models are trained on. OpenAI has not released information about what dataset(s) were used to train DALL\u00b7E 2, inciting concern from some that the work of artists has been used for training without permission. Copyright laws surrounding these topics are inconclusive at the moment.<ref name=\":4\" />\n\nAfter integrating DALL\u00b7E 3 into Bing Chat and ChatGPT, Microsoft and OpenAI faced criticism for excessive content filtering, with critics saying DALL\u00b7E had been \"lobotomized\".<ref name=\"WindowsCentral\" /> The flagging of images generated by prompts such as 'man breaks server rack with sledgehammer' was cited as evidence. Over the first days of its launch, filtering was reportedly increased to the point where images generated by some of Bing's own suggested prompts were being blocked.<ref name=\"WindowsCentral\" /><ref name=\"TechRadar\" /> ''[[TechRadar]]'' argued that leaning too heavily on the side of caution could limit DALL\u00b7E's value as a creative tool.<ref name=\"TechRadar\" />\n\n== Open-source implementations ==\nSince OpenAI has not released [[source code]] for any of the three models, there have been several attempts to create [[Open source|open-source]] implementations of DALL\u00b7E.<ref name=\"VentureBeat3\" /><ref>{{Citation |title=jina-ai/dalle-flow |date=2022-06-17 |url=https://github.com/jina-ai/dalle-flow |publisher=Jina AI |access-date=2022-06-17 |archive-date=17 June 2022 |archive-url=https://web.archive.org/web/20220617090248/https://github.com/jina-ai/dalle-flow |url-status=live }}</ref> Released in 2022 on [[Hugging Face]]'s Spaces platform, Craiyon (formerly DALL\u00b7E Mini until a name change was requested by OpenAI in June 2022) is an AI model based on the original DALL\u00b7E that was trained on unfiltered data from the Internet. It attracted substantial media attention in mid-2022, after its release due to its capacity for producing humorous imagery.<ref name=\"CNETmini\" /><ref name=\"DailyDotmini\" /><ref name=\"Polygonmini\" />\n\n==See also==\n* [[Artificial intelligence art]]\n* [[DeepDream]]\n* [[Imagen (Google Brain)]]\n* [[Midjourney]]\n* [[Stable Diffusion]]\n* [[Prompt engineering]]\n\n==References==\n{{reflist|refs=\n\n<ref name=\"impact\">{{Cite arXiv\n |eprint       = 2102.02503\n |last1        = Tamkin\n |first1       = Alex\n |last2        = Brundage\n |first2       = Miles\n |last3        = Clark\n |first3       = Jack\n |last4        = Ganguli\n |first4       = Deep\n |title        = Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models\n |year         = 2021\n |class        = cs.CL\n}}</ref>\n<ref name=\"gpt1paper\">{{Cite web\n |url          = https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n |title        = Improving Language Understanding by Generative Pre-Training\n |last1        = Radford\n |first1       = Alec\n |last2        = Narasimhan\n |first2       = Karthik\n |last3        = Salimans\n |first3       = Tim\n |last4        = Sutskever\n |first4       = Ilya\n |pages        = 12\n |publisher    = [[OpenAI]]\n |date         = 11 June 2018\n |access-date  = 23 January 2021\n |archive-date = 26 January 2021\n |archive-url  = https://web.archive.org/web/20210126024542/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n |url-status   = live\n}}</ref>\n<ref name=\"gpt2paper\">{{cite journal\n |url          = https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n |title        = Language models are unsupervised multitask learners\n |last1        = Radford\n |first1       = Alec\n |last2        = Wu\n |first2       = Jeffrey\n |last3        = Child\n |first3       = Rewon\n |last4        = Luan\n |first4       = David\n |last5        = Amodei\n |first5       = Dario\n |last6        = Sutskever\n |first6       = Ilua\n |volume       = 1\n |issue        = 8\n |date         = 14 February 2019\n |access-date  = 19 December 2020\n |quote        = \n |website      = cdn.openai.com\n |archive-date = 6 February 2021\n |archive-url  = https://web.archive.org/web/20210206183945/https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n |url-status   = live\n}}</ref>\n<ref name=\"gpt3paper\">{{cite arXiv\n| last1       = Brown\n| first1      = Tom B.\n| last2       = Mann\n| first2      = Benjamin\n| last3       = Ryder\n| first3      = Nick\n| last4       = Subbiah\n| first4      = Melanie\n| last5       = Kaplan\n| first5      = Jared\n| last6       = Dhariwal\n| first6      = Prafulla\n| last7       = Neelakantan\n| first7      = Arvind\n| last8       = Shyam\n| first8      = Pranav\n| last9       = Sastry\n| first9      = Girish\n| last10      = Askell\n| first10     = Amanda\n| last11      = Agarwal\n| first11     = Sandhini\n| last12      = Herbert-Voss\n| first12     = Ariel\n| last13      = Krueger\n| first13     = Gretchen\n| last14      = Henighan\n| first14     = Tom\n| last15      = Child\n| first15     = Rewon\n| last16      = Ramesh\n| first16     = Aditya\n| last17      = Ziegler\n| first17     = Daniel M.\n| last18      = Wu\n| first18     = Jeffrey\n| last19      = Winter\n| first19     = Clemens\n| last20      = Hesse\n| first20     = Christopher\n| last21      = Chen\n| first21     = Mark\n| last22      = Sigler\n| first22     = Eric\n| last23      = Litwin\n| first23     = Mateusz\n| last24      = Gray\n| first24     = Scott\n| last25      = Chess\n| first25     = Benjamin\n| last26      = Clark\n| first26     = Jack\n| last27      = Berner\n| first27     = Christopher\n| last28      = McCandlish\n| first28     = Sam\n| last29      = Radford\n| first29     = Alec\n| last30      = Sutskever\n| first30     = Ilya\n| last31      = Amodei\n| first31     = Dario\n| title       = Language Models are Few-Shot Learners\n| eprint      = 2005.14165 \n| date        = July 22, 2020\n| class       = cs.CL\n}}</ref>\n<ref name=\"dallepaper\">{{cite arXiv\n |last1       = Ramesh\n |first1      = Aditya\n |last2       = Pavlov\n |first2      = Mikhail \n |last3       = Goh\n |first3      = Gabriel \n |last4       = Gray\n |first4      = Scott\n |last5       = Voss\n |first5      = Chelsea \n |last6       = Radford\n |first6      = Alec\n |last7       = Chen\n |first7      = Mark\n |last8       = Sutskever\n |first8      = Ilya\n |eprint      = 2102.12092\n |title       = Zero-Shot Text-to-Image Generation\n |class       = cs.LG\n |date        = 24 February 2021\n}}</ref>\n\n<ref name=\"tc\">{{cite web\n |url       = https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n |title       = OpenAI's DALL-E creates plausible images of literally anything you ask it to\n |last       = Coldewey\n |first       = Devin\n |website       = \n |publisher       = \n |date       = 5 January 2021\n |access-date       = 5 January 2021\n |quote       = \n |archive-date       = 6 January 2021\n |archive-url       = https://web.archive.org/web/20210106075542/https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n |url-status       = live\n }}</ref>\n<ref name=\"vb\">{{cite web\n |url          = https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n |title        = OpenAI debuts DALL-E for generating images from text\n |last         = Johnson\n |first        = Khari\n |website      = \n |publisher    = VentureBeat\n |date         = 5 January 2021\n |access-date  = 5 January 2021\n |quote        = \n |archive-date = 5 January 2021\n |archive-url  = https://web.archive.org/web/20210105221534/https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/\n |url-status   = live\n}}</ref>\n<ref name=\"mittr\">{{cite web\n |url          = https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/\n |title        = This avocado armchair could be the future of AI\n |last         = Heaven\n |first        = Will Douglas\n |website      = \n |publisher    = MIT Technology Review\n |date         = 5 January 2021\n |access-date  = 5 January 2021\n |quote        = \n |archive-date = 5 January 2021\n |archive-url  = https://web.archive.org/web/20210105193658/https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/\n |url-status   = live\n}}</ref>\n<ref name=\"boing\">{{cite web\n |url          = https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n |title        = This AI neural network transforms text captions into art, like a jellyfish Pikachu\n |last         = Dunn\n |first        = Thom\n |website      = \n |publisher    = [[BoingBoing]]\n |date         = 10 February 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 22 February 2021\n |archive-url  = https://web.archive.org/web/20210222001459/https://boingboing.net/2021/02/10/this-ai-neural-network-transforms-text-captions-into-art-like-a-jellyfish-pikachu.html\n |url-status   = live\n}}</ref>\n<ref name=\"nature\">{{cite web\n |url          = https://www.nature.com/immersive/d41586-021-00095-y/index.html\n |title        = Tardigrade circus and a tree of life \u2014 January's best science images\n |last         = Stove\n |first        = Emma\n |website      = \n |publisher    = [[Nature (journal)|Nature]]\n |date         = 5 February 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 8 March 2021\n |archive-url  = https://web.archive.org/web/20210308032636/https://www.nature.com/immersive/d41586-021-00095-y/index.html\n |url-status   = live\n}}</ref>\n\n<ref name=\"extreme\">{{cite news\n |url          = https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n |title        = OpenAI's 'DALL-E' Generates Images From Text Descriptions\n |last         = Whitwam\n |first        = Ryan\n |website      = \n |newspaper    = ExtremeTech\n |date         = 6 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 28 January 2021\n |archive-url  = https://web.archive.org/web/20210128064428/https://www.extremetech.com/extreme/318881-openais-dall-e-generates-images-from-text-descriptions\n |url-status   = live\n}}</ref>\n<ref name=\"engadget\">{{cite web\n |url          = https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n |title        = OpenAI's DALL-E app generates images from just a description\n |last         = Dent\n |first        = Steve\n |website      = \n |publisher    = [[Engadget]]\n |date         = 6 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 27 January 2021\n |archive-url  = https://web.archive.org/web/20210127225652/https://www.engadget.com/dall-e-ai-gpt-make-image-from-any-description-135535140.html\n |url-status   = live\n}}</ref>\n<ref name=\"input\">{{cite web\n |url          = https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n |title        = This AI turns text into surreal, suggestion-driven art\n |last         = Kasana\n |first        = Mehreen\n |website      = \n |publisher    = Input\n |date         = 7 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 29 January 2021\n |archive-url  = https://web.archive.org/web/20210129211643/https://www.inputmag.com/tech/dalle-takes-your-text-turns-it-into-surreal-captivating-art\n |url-status   = live\n}}</ref>\n<ref name=\"cnbc\">{{cite web\n |url          = https://www.cnbc.com/2021/01/08/openai-shows-off-dall-e-image-generator-after-gpt-3.html\n |title        = Why everyone is talking about an image generator released by an Elon Musk-backed A.I. lab\n |last         = Shead\n |first        = Sam\n |website      = \n |publisher    = [[CNBC]]\n |date         = 8 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 16 July 2022\n |archive-url  = https://web.archive.org/web/20220716230354/https://www.cnbc.com/2021/01/08/openai-shows-off-dall-e-image-generator-after-gpt-3.html\n |url-status   = live\n}}</ref>\n\n<ref name=\"dale\">{{cite web\n |url          = https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n |title        = Here's how OpenAI's magical DALL-E image generator works\n |last         = Markowitz\n |first        = Dale\n |website      = \n |publisher    = [[TheNextWeb]]\n |date         = 10 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 23 February 2021\n |archive-url  = https://web.archive.org/web/20210223162340/https://thenextweb.com/neural/2021/01/10/heres-how-openais-magical-dall-e-generates-images-from-text-syndication/\n |url-status   = live\n}}</ref>\n\n<ref name=\"nbc\">{{cite web\n |url          = https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n |title        = Here's DALL-E: An algorithm learned to draw anything you tell it\n |last         = Ehrenkranz\n |first        = Melanie\n |website      = \n |publisher    = [[NBC News]]\n |date         = 27 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 20 February 2021\n |archive-url  = https://web.archive.org/web/20210220164655/https://www.nbcnews.com/tech/innovation/here-s-dall-e-algorithm-learned-draw-anything-you-tell-n1255834\n |url-status   = live\n}}</ref>\n\n<ref name=\"bbc\">{{cite web\n |url          = https://www.bbc.com/news/technology-55559463\n |title        = AI draws dog-walking baby radish in a tutu\n |last         = Wakefield\n |first        = Jane\n |website      = \n |publisher    = [[British Broadcasting Corporation]]\n |date         = 6 January 2021\n |access-date  = 3 March 2021\n |quote        = \n |archive-date = 2 March 2021\n |archive-url  = https://web.archive.org/web/20210302170623/https://www.bbc.com/news/technology-55559463\n |url-status   = live\n}}</ref>\n<ref name=\"cnn\">{{cite web\n |url          = https://www.cnn.com/2021/01/08/tech/artificial-intelligence-openai-images-from-text/index.html\n |title        = A radish in a tutu walking a dog? This AI can draw it really well\n |last         = Metz\n |first        = Rachel\n |website      = \n |publisher    = CNN\n |date         = 2 February 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 16 July 2022\n |archive-url  = https://web.archive.org/web/20220716171727/https://www.cnn.com/2021/01/08/tech/artificial-intelligence-openai-images-from-text/index.html\n |url-status   = live\n}}</ref>\n<ref name=\"wired\">{{cite magazine\n |url          = https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n |title        = This AI Could Go From 'Art' to Steering a Self-Driving Car\n |last         = Knight\n |first        = Will\n |magazine      = Wired\n |date         = 26 January 2021\n |access-date  = 2 March 2021\n |quote        = \n |archive-date = 21 February 2021\n |archive-url  = https://web.archive.org/web/20210221010223/https://www.wired.com/story/ai-go-art-steering-self-driving-car/\n |url-status   = live\n}}</ref>\n<ref name=\"CNETmini\">{{cite web\n |url          = https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n |title        = Everything to Know About Dall-E Mini, the Mind-Bending AI Art Creator\n |last         = Carson\n |first        = Erin\n |website      = [[CNET]]\n |publisher    = \n |date         = 14 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 15 June 2022\n |archive-url  = https://web.archive.org/web/20220615085705/https://www.cnet.com/culture/everything-to-know-about-dall-e-mini-the-mind-bending-ai-art-creator/\n |url-status   = live\n}}</ref>\n<ref name=\"DailyDotmini\">{{cite web\n |url          = https://www.dailydot.com/unclick/dall-e-mini-memes/\n |title        = AI program DALL-E mini prompts some truly cursed images\n |last         = Schroeder\n |first        = Audra\n |website      = [[Daily Dot]]\n |publisher    = \n |date         = 9 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 10 June 2022\n |archive-url  = https://web.archive.org/web/20220610212300/https://www.dailydot.com/unclick/dall-e-mini-memes/\n |url-status   = live\n}}</ref>\n<ref name=\"Polygonmini\">{{cite web\n |url          = https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n |title        = People are using DALL-E mini to make meme abominations \u2014 like pug Pikachu\n |last         = Diaz\n |first        = Ana\n |website      = [[Polygon (website)|Polygon]]\n |publisher    = \n |date         = 15 June 2022\n |access-date  = 15 June 2022\n |quote        = \n |archive-date = 15 June 2022\n |archive-url  = https://web.archive.org/web/20220615151753/https://www.polygon.com/23167596/memes-dall-e-mini-image-generator-ai-explained\n |url-status   = live\n}}</ref>\n<ref name=\"VentureBeat3\">{{cite web\n |url = https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n |title = How DALL-E 2 could solve major computer vision challenges\n |last = Sahar Mor\n |first = Stripe\n |website = [[VentureBeat]]\n |publisher = \n |date = 16 April 2022\n |access-date = 15 June 2022\n |quote = \n |archive-date = 24 May 2022\n |archive-url = https://web.archive.org/web/20220524133956/https://venturebeat.com/2022/04/16/how-dall-e-2-could-solve-major-computer-vision-challenges/\n |url-status = live\n}}</ref>\n<ref name=\"WindowsCentral\">{{cite web\n |url = https://www.windowscentral.com/software-apps/bing/bing-dall-e-3-image-creation-was-great-for-a-few-days-but-now-microsoft-has-predictably-lobotomized-it\n |title = Bing Dall-E 3 image creation was great for a few days, but now Microsoft has predictably lobotomized it\n |last = Corden\n |first = Jez\n |website = Windows Central\n |date = 8 October 2023\n |access-date = 11 October 2023\n |archive-date = 10 October 2023\n |archive-url = https://web.archive.org/web/20231010185641/https://www.windowscentral.com/software-apps/bing/bing-dall-e-3-image-creation-was-great-for-a-few-days-but-now-microsoft-has-predictably-lobotomized-it\n |url-status = live\n }}</ref>\n<ref name=\"TechRadar\">{{cite web\n |url = https://www.techradar.com/computing/artificial-intelligence/microsoft-reins-in-bing-ais-image-creator-and-the-results-dont-make-much-sense\n |title = Microsoft reins in Bing AI's Image Creator \u2013 and the results don't make much sense\n |last = Allan\n |first = Darren\n |website = [[TechRadar]]\n |date = 9 October 2023\n |access-date = 11 October 2023\n |archive-date = 10 October 2023\n |archive-url = https://web.archive.org/web/20231010075911/https://www.techradar.com/computing/artificial-intelligence/microsoft-reins-in-bing-ais-image-creator-and-the-results-dont-make-much-sense\n |url-status = live\n }}</ref>\n<!-- Unused\n<ref name=\"newscientist\">{{cite web\n |url          = https://www.newscientist.com/article/2264022-ai-illustrator-draws-imaginative-pictures-to-go-with-text-captions/\n |title        = AI illustrator draws imaginative pictures to go with text captions\n |last         = Stokel-Walker\n |first        = Chris\n |website      = \n |publisher    = [[New Scientist]]\n |date         = 5 January 2021\n |access-date  = 4 March 2021\n |quote        = \n |archive-date = 28 January 2021\n |archive-url  = https://web.archive.org/web/20210128005947/https://www.newscientist.com/article/2264022-ai-illustrator-draws-imaginative-pictures-to-go-with-text-captions/\n |url-status   = live\n}}</ref>\n-->}}\n\n==External links==\n{{Commons category}}\n*[https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf DALL-E 3 System Card]\n*[https://cdn.openai.com/papers/dall-e-3.pdf DALL-E 3] paper by OpenAI\n*[https://openai.com/dall-e-2/ DALL-E 2 website]\n*[https://www.craiyon.com/ Craiyon website]\n\n{{OpenAI navbox}}\n{{Differentiable computing}}\n{{Authority control}}\n\n[[Category:Artificial intelligence art]]\n[[Category:Text-to-image generation]]\n[[Category:Deep learning software applications]]\n[[Category:Unsupervised learning]]\n[[Category:Generative pre-trained transformers]]\n[[Category:OpenAI]]\n[[Category:2021 software]]"}