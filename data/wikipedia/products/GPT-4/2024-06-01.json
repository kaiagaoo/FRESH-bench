{"title": "GPT-4", "page_id": 72861474, "revision_id": 1225440810, "revision_timestamp": "2024-05-24T13:29:55Z", "content": "{{Short description|2023 text-generating language model}}\n{{Use American English|date=May 2023}}\n{{Use mdy dates|date=May 2023}}\n{{Infobox software\n| title = Generative Pre-trained Transformer 4 (GPT-4)\n| logo =\n| developer = [[OpenAI]]\n| released = {{start date and age|2023|3|14}}\n| genre = {{ indented plainlist |\n*[[Multimodal learning|Multimodal]]\n*[[Large language model]]\n*[[Generative pre-trained transformer]]\n*[[Foundation model]]\n}}\n| replaces = [[GPT-3.5]]\n| replaced_by = \n| license = [[Proprietary software|Proprietary]]\n}}\n{{Machine learning|Artificial neural network}}\n'''Generative Pre-trained Transformer 4''' ('''GPT-4''') is a [[Multimodal learning|multimodal]] [[large language model]] created by [[OpenAI]], and the fourth in its series of [[Generative pre-trained transformer#Foundational models|GPT foundation models]].<ref name=\"ars-technica\">{{Cite web |last=Edwards |first=Benj |date=March 14, 2023 |title=OpenAI's GPT-4 exhibits \"human-level performance\" on professional benchmarks |url=https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/ |url-status=live |archive-url=https://web.archive.org/web/20230314225236/https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/ |archive-date=March 14, 2023 |access-date=March 15, 2023 |website=[[Ars Technica]]}}</ref> It was launched on March 14, 2023,<ref name=\"ars-technica\" /> and made publicly available via the paid [[chatbot]] product [[ChatGPT Plus]], via OpenAI's [[API]], and via the free chatbot [[Microsoft Copilot]].<ref>{{Cite web |last=Wiggers |first=Kyle |date=2023-07-06 |title=OpenAI makes GPT-4 generally available |url=https://techcrunch.com/2023/07/06/openai-makes-gpt-4-generally-available/ |access-date=2023-08-16 |website=TechCrunch |language=en-US |archive-date=August 16, 2023 |archive-url=https://web.archive.org/web/20230816000049/https://techcrunch.com/2023/07/06/openai-makes-gpt-4-generally-available/ |url-status=live }}</ref>  As a [[Transformer (machine learning model)|transformer]]-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next [[Lexical analysis#Tokenization|token]]. After this step, the model was then fine-tuned with [[reinforcement learning]] feedback from [[Reinforcement learning from human feedback|humans]] and AI for [[AI alignment|human alignment]] and policy compliance.<ref name=\"gpt4_tech_report\" />{{Rp|page=2}}\n\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions.<ref name=\"vox\"/> GPT-4, equipped with vision capabilities (GPT-4V),<ref>{{Cite web |title=GPT-4V(ision) system card |url=https://openai.com/research/gpt-4v-system-card |access-date=2024-02-05 |website=OpenAI |language=en-US}}</ref> is capable of taking images as input on ChatGPT.<ref>{{Cite news |last=Roose |first=Kevin |date=2023-09-28 |title=The New ChatGPT Can 'See' and 'Talk.' Here's What It's Like. |work=The New York Times |url=https://www.nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html |access-date=2023-10-30 |archive-date=October 31, 2023 |archive-url=https://web.archive.org/web/20231031055345/https://www.nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html |url-status=live }}</ref> OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.<ref name=\"verge wrong\" />\n\n== Background ==\n{{further|GPT-3#Background|GPT-2#Background}} \nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\"<ref>{{Cite web |last1=Radford |first1=Alec |last2=Narasimhan |first2=Karthik |last3=Salimans |first3=Tim |last4=Sutskever |first4=Ilya |date=June 11, 2018 |title=Improving Language Understanding by Generative Pre-Training |url=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf |access-date=April 3, 2023 |archive-date=January 26, 2021 |archive-url=https://web.archive.org/web/20210126024542/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf |url-status=live }}</ref> It was based on the transformer architecture and trained on a large [[Text corpus|corpus]] of books.<ref>{{Cite web |last=Khandelwal |first=Umesh |date=April 1, 2023 |title=How Large Language GPT models evolved and work |url=https://www.linkedin.com/pulse/how-large-language-gpt-models-evolved-work-umesh-khandelwal |access-date=April 3, 2023 |archive-date=April 4, 2023|archive-url=https://web.archive.org/web/20230404041003/https://www.linkedin.com/pulse/how-large-language-gpt-models-evolved-work-umesh-khandelwal |url-status=live }}</ref> The next year, they introduced [[GPT-2]], a larger model that could generate coherent text.<ref>{{Cite web |date=April 3, 2023 |title=What is GPT-4 and Why Does it Matter? |url=https://www.datacamp.com/blog/what-we-know-gpt4 |access-date=April 3, 2023 |archive-date=April 3, 2023 |archive-url=https://web.archive.org/web/20230403223832/https://www.datacamp.com/blog/what-we-know-gpt4 |url-status=live }}</ref> In 2020, they introduced [[GPT-3]], a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples.<ref>{{Cite arXiv |last=Brown |first=Tom B. |date=July 20, 2020 |title=Language Models are Few-Shot Learners |class=cs.CL |eprint=2005.14165v4 }}</ref> GPT-3 was further improved into [[GPT-3.5]], which was used to create the chatbot product [[ChatGPT]].\n\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by [[George Hotz]].<ref>{{Cite web |last=Schreiner |first=Maximilian |date=2023-07-11 |title=GPT-4 architecture, datasets, costs and more leaked |url=https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/ |access-date=2023-07-12 |website=THE DECODER |language=en-US |archive-date=July 12, 2023 |archive-url=https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/ |url-status=live }}</ref>\n\n== Capabilities ==\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\"<ref>{{Cite web |last=Wiggers |first=Kyle |date=March 14, 2023 |title=OpenAI releases GPT-4, a multimodal AI that it claims is state-of-the-art |url=https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/ |url-status=live |archive-url=https://web.archive.org/web/20230315003723/https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/ |archive-date=March 15, 2023 |access-date=March 15, 2023 |website=[[TechCrunch]]}}</ref> They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively.<ref>{{Cite web |author=OpenAI |title=Models |url=https://platform.openai.com/docs/models |access-date=March 18, 2023 |website=OpenAI API |archive-date=March 17, 2023 |archive-url=https://web.archive.org/web/20230317000210/https://platform.openai.com/docs/models |url-status=live }}</ref> Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to [[Neural scaling law#Broken Neural Scaling Laws (BNSL)|breaks]]<ref>Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). [[arxiv:2210.14891|Broken Neural Scaling Laws]]. International Conference on Learning Representations (ICLR), 2023.</ref> in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input;<ref name=\"guardian creative\">{{cite web |author1=Alex Hern |author2=Johana Bhuiyan |date=March 14, 2023 |title=OpenAI says new model GPT-4 is more creative and less likely to invent facts |url=https://www.theguardian.com/technology/2023/mar/14/chat-gpt-4-new-model |url-status=live |archive-url=https://web.archive.org/web/20230315003816/https://www.theguardian.com/technology/2023/mar/14/chat-gpt-4-new-model |archive-date=March 15, 2023 |access-date=March 15, 2023 |website=[[The Guardian]]}}</ref> this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.<ref name=\"openai_research\">{{Cite web |author=OpenAI |date=March 14, 2023 |title=GPT-4 |url=https://openai.com/research/gpt-4 |access-date=March 20, 2023 |website=OpenAI Research |archive-date=March 14, 2023 |archive-url=https://web.archive.org/web/20230314174531/https://openai.com/research/gpt-4 |url-status=live }}</ref> It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.<ref>{{Cite web |last1=Metz |first1=Cade |last2=Chen |first2=Brian X. |last3=Weise |first3=Karen |date=September 25, 2023 |title=ChatGPT Can Now Respond With Spoken Words |website=[[The New York Times]] |url=https://www.nytimes.com/2023/09/25/technology/chatgpt-talk-digital-assistance.html?searchResultPosition=5}}</ref>\n\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in [[Natural-language user interface|natural language]] given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in [[JSON]]\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.<ref name=\"openai_research\" />\n\nWhen instructed to do so, GPT-4 can interact with external interfaces.<ref>{{Cite web |title=ChatGPT plugins |url=https://openai.com/blog/chatgpt-plugins |access-date=2023-06-01 |website=openai.com |language=en-US |archive-date=March 23, 2023 |archive-url=https://web.archive.org/web/20230323213712/https://openai.com/blog/chatgpt-plugins |url-status=live }}</ref> For example, the model could be instructed to enclose a query within <code>&lt;search&gt;&lt;/search&gt;</code> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using [[API]]s, generating images, and accessing and summarizing webpages.<ref name=\"Bubeck-2023\" />\n\nA 2023 article in ''[[Nature (journal)|Nature]]'' stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from [[MATLAB]] to [[Python (language)|Python]] went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4 produced code vulnerable to SQL injection attacks 5% of the time, an improvement over GitHub Copilot from the year 2021, which produced vulnerabilities 40% of the time.<ref>{{cite journal |last1=Perkel |first1=Jeffrey M. |title=Six tips for better coding with ChatGPT |journal=Nature |date=5 June 2023 |volume=618 |issue=7964 |pages=422\u2013423 |doi=10.1038/d41586-023-01833-0 |pmid=37277596 |bibcode=2023Natur.618..422P |s2cid=259066258 |url=https://www.nature.com/articles/d41586-023-01833-0 |language=en |access-date=June 15, 2023 |archive-date=June 15, 2023 |archive-url=https://web.archive.org/web/20230615234659/https://www.nature.com/articles/d41586-023-01833-0 |url-status=live }}</ref>\n\nIn November 2023, OpenAI announced the GPT-4 Turbo and GPT-4 Turbo with Vision model, which features a 128K context window and significantly cheaper pricing.<ref>{{Cite web |title=New models and developer products announced at DevDay |url=https://openai.com/blog/new-models-and-developer-products-announced-at-devday |access-date=2023-11-14 |website=openai.com |language=en-US |archive-date=November 14, 2023 |archive-url=https://web.archive.org/web/20231114101805/https://openai.com/blog/new-models-and-developer-products-announced-at-devday |url-status=live }}</ref><ref>{{cite web | last=David | first=Emilia | title=OpenAI turbocharges GPT-4 and makes it cheaper | website=The Verge | date=2023-11-06 | url=https://www.theverge.com/2023/11/6/23948426/openai-gpt4-turbo-generative-ai-new-model | access-date=2024-01-23}}</ref>\n\n=== GPT-4o ===\n{{Main|GPT-4o}}\nOn May 13, 2024, OpenAI introduced GPT-4o (\"o\" for \"omni\"), a model that marks a significant advancement by processing and generating outputs across text, audio, and image modalities in real time. GPT-4o exhibits rapid response times comparable to human reaction in conversations, substantially improved performance on non-English languages, and enhanced understanding of vision and audio.<ref>{{Cite web |last=Field |first=Hayden |date=2024-05-13 |title=OpenAI launches new AI model and desktop version of ChatGPT |url=https://www.cnbc.com/2024/05/13/openai-launches-new-ai-model-and-desktop-version-of-chatgpt.html |access-date=2024-05-13 |website=CNBC |language=en}}</ref> \n\nGPT-4o integrates its various inputs and outputs under a unified model, making it faster, more cost-effective, and efficient than its predecessors. GPT-4o achieves state-of-the-art results in multilingual and vision benchmarks, setting new records in audio speech recognition and translation.{{Citation needed|date=May 2024}}<ref name=\":0\">{{Cite web |date=13 May 2024 |title=Hello GPT-4o |url=https://openai.com/index/hello-gpt-4o/ |url-status=live |archive-url=https://web.archive.org/web/20240514024319/https://openai.com/index/hello-gpt-4o/ |archive-date=May 14, 2024 |access-date=May 14, 2024 |website=OpenAI}}</ref>\n\nOpenAI plans to immediately roll out GPT-4o's image and text capabilities to ChatGPT, including its free tier, with voice mode becoming available for ChatGPT Plus users in coming weeks. They plan to make the model's audio and video capabilities available for limited API partners in coming weeks.<ref name=\":0\" />\n\nIn its launch announcement, OpenAI noted GPT-4o's capabilities presented new safety challenges, and noted mitigations and limitations as a result.<ref name=\":0\" />\n\n=== Aptitude on standardized tests===\nGPT-4 demonstrates aptitude on several standardized tests. OpenAI claims that in their own testing the model received a score of 1410 on the [[SAT]] (94th<ref name=\"College Board-2022\">{{Cite web |date=2022 |title=SAT: Understanding Scores |url=https://satsuite.collegeboard.org/media/pdf/understanding-sat-scores.pdf |access-date=March 21, 2023 |website=[[College Board]] |archive-date=March 16, 2023 |archive-url=https://web.archive.org/web/20230316022540/https://satsuite.collegeboard.org/media/pdf/understanding-sat-scores.pdf |url-status=live }}</ref> percentile), 163 on the [[LSAT]] (88th percentile), and 298 on the [[Uniform Bar Exam]] (90th percentile).<ref>{{Cite web |last=Ver Meer |first=Dave |date=2023-05-23 |title=ChatGPT Statistics |url=https://www.namepepper.com/chatgpt-users |access-date=2023-06-01 |website=NamePepper |language=en |archive-date=June 5, 2023 |archive-url=https://web.archive.org/web/20230605230914/https://www.namepepper.com/chatgpt-users |url-status=live }}</ref> In contrast, OpenAI claims that GPT-3.5 received scores for the same exams in the 82nd,<ref name=\"College Board-2022\" /> 40th, and 10th percentiles, respectively.<ref name=\"gpt4_tech_report\">{{Cite arXiv |last=OpenAI |date=2023 |title=GPT-4 Technical Report |class=cs.CL |eprint=2303.08774}}</ref>\nGPT-4 also passed an oncology exam,<ref name=\"Holmes2023\">{{cite journal |last1=Holmes |first1=Jason |last2=Liu |first2=Zhengliang |last3=Zhang |first3=Lian |last4=Ding |first4=Yuzhen |last5=Sio |first5=Terence T. |last6=McGee |first6=Lisa A. |last7=Ashman |first7=Jonathan B. |last8=Li |first8=Xiang |last9=Liu |first9=Tianming |last10=Shen |first10=Jiajian |last11=Liu |first11=Wei |date=2023 |title=Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics |journal=Frontiers in Oncology |volume=13 |doi=10.3389/fonc.2023.1219326 |pmid=37529688 |pmc=10388568 |arxiv=2304.01938 |doi-access=free }}</ref> an engineering exam<ref name=\"Naser2023\">{{cite arXiv |last1=Naser |first1=M.Z. |last2=Ross |first2=Brandon |last3=Ogle |first3=Jennifer |last4=Kodur |first4=Venkatesh |last5=Hawileh |first5=Rami |last6=Abdalla |first6=Jamal |last7=Thai |first7=Huu-Tai |date=2023 |title=Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams? |class=cs.CL |eprint=2303.18149}}</ref> and a plastic surgery exam.<ref name=\"Freedman2023\">{{cite arXiv |last1=Freedman |first1=Jonathan D. |last2=Nappier |first2=Ian A. |date=2023 |title=GPT-4 to GPT-3.5: 'Hold My Scalpel' \u2013 A Look at the Competency of OpenAI's GPT on the Plastic Surgery In-Service Training Exam |class=cs.AI |eprint=2304.01503}}</ref> In the [[Torrance Tests of Creative Thinking]], GPT-4 scored within the top 1% for originality and fluency, while its flexibility scores ranged from the 93rd to the 99th percentile.<ref>{{cite journal|title=The originality of machines: AI takes the Torrance Test|author1=Guzik, Erik E.|author2=Byrge, Christian|author3=Gilde, Christian|doi=10.1016/j.yjoc.2023.100065|journal=Journal of Creativity|volume=33|issue=3|year=2023|s2cid=261087185 |doi-access=free}}</ref>\n\n===Medical applications===\nResearchers from Microsoft tested GPT-4 on medical problems and found \"that GPT-4, without any specialized prompt crafting, exceeds the passing score on [[USMLE]] by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge ([[Med-PaLM]], a prompt-tuned version of Flan-PaLM 540B). Despite GPT-4's strong performance on tests, the report warns of \"significant risks\" of using LLMs in medical applications, as they may provide inaccurate recommendations and [[Hallucination (artificial intelligence)|hallucinate]] major factual errors.<ref>{{Cite arXiv |last1=Nori |first1=Harsha |last2=King |first2=Nicholas |last3=McKinney |first3=Scott Mayer |last4=Carignan |first4=Dean |last5=Horvitz |first5=Eric |date=March 20, 2023 |title=Capabilities of GPT-4 on Medical Challenge Problems |class=cs.CL |eprint=2303.13375 }}</ref><ref>{{cite journal |last1=Azamfirei |first1=R |last2=Kudchadkar |first2=SR |last3=Fackler |first3=J |title=Large language models and the perils of their hallucinations. |journal=Critical Care |date=21 March 2023 |volume=27 |issue=1 |pages=120 |doi=10.1186/s13054-023-04393-x |doi-access=free |pmid=36945051 |pmc=10032023 }}</ref> Researchers from Columbia University and Duke University have also demonstrated that GPT-4 can be utilized for cell type annotation, a standard task in the analysis of single-cell RNA-seq data. <ref>{{cite journal |last1=Hou |first1=W |last2=Ji |first2=Z | title=Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis |journal=Nature Methods |date=25 March 2024 |doi=10.1038/s41592-024-02235-4 |doi-access=free |pmid=38528186 |pmc=10187429 }}</ref>\n\nIn April 2023, Microsoft and [[Epic Systems]] announced that they will provide healthcare providers with GPT-4-powered systems for assisting in responding to questions from patients and analysing medical records.<ref>{{Cite web |last=Edwards |first=Benj |date=2023-04-18 |title=GPT-4 will hunt for trends in medical records thanks to Microsoft and Epic |url=https://arstechnica.com/information-technology/2023/04/gpt-4-will-hunt-for-trends-in-medical-records-thanks-to-microsoft-and-epic/ |access-date=2023-05-03 |website=Ars Technica |language=en-us |archive-date=May 3, 2023 |archive-url=https://web.archive.org/web/20230503172538/https://arstechnica.com/information-technology/2023/04/gpt-4-will-hunt-for-trends-in-medical-records-thanks-to-microsoft-and-epic/ |url-status=live }}</ref><ref>{{Cite journal |last1=Perera Molligoda Arachchige |first1=Arosh S. |last2=Stomeo |first2=Niccol\u00f2 |date=2023-08-18 |title=Controversies surrounding AI-based reporting systems in echocardiography |url=https://pubmed.ncbi.nlm.nih.gov/37594682 |journal=Journal of Echocardiography |volume=21 |issue=4 |pages=184\u2013185 |doi=10.1007/s12574-023-00620-0 |issn=1880-344X |pmid=37594682 |s2cid=260969922 |access-date=November 1, 2023 |archive-date=November 1, 2023 |archive-url=https://web.archive.org/web/20231101120824/https://pubmed.ncbi.nlm.nih.gov/37594682/ |url-status=live }}</ref><ref>{{Cite journal |last=Arachchige |first=Arosh S. Perera Molligoda |date=July 2023 |title=Early applications of ChatGPT in medical practice, education and research |journal=Clinical Medicine |volume=23 |issue=4 |pages=429\u2013430 |doi=10.7861/clinmed.Let.23.4.2 |issn=1473-4893 |pmc=10541035 |pmid=37524422}}</ref><ref>{{Cite journal |last=Perera Molligoda Arachchige |first=Arosh S. |date=July 2023 |title=Large language models (LLM) and ChatGPT: a medical student perspective |url=https://pubmed.ncbi.nlm.nih.gov/37046082 |journal=European Journal of Nuclear Medicine and Molecular Imaging |volume=50 |issue=8 |pages=2248\u20132249 |doi=10.1007/s00259-023-06227-y |issn=1619-7089 |pmid=37046082 |s2cid=258111774 |access-date=November 1, 2023 |archive-date=November 1, 2023 |archive-url=https://web.archive.org/web/20231101120824/https://pubmed.ncbi.nlm.nih.gov/37046082/ |url-status=live }}</ref><ref>{{Cite journal |last1=Perera Molligoda Arachchige |first1=Arosh S. |last2=Stomeo |first2=Niccol\u00f2 |date=October 2023 |title=Exploring the Opportunities and Challenges of ChatGPT in Academic Writing: Reply to Bom et al |journal=Nuclear Medicine and Molecular Imaging |volume=57 |issue=5 |pages=213\u2013214 |doi=10.1007/s13139-023-00816-3 |issn=1869-3474 |pmc=10504185 |pmid=37720884 |pmc-embargo-date=October 1, 2024 }}</ref><ref>{{Cite journal |last=Perera Molligoda Arachchige |first=Arosh S. |date=2023-07-28 |title=New Horizons: The Potential Role of OpenAI's ChatGPT in Clinical Radiology |url=https://pubmed.ncbi.nlm.nih.gov/37517771 |journal=Journal of the American College of Radiology: JACR |volume=20 |issue=10 |pages=S1546\u20131440(23)00536\u20137 |doi=10.1016/j.jacr.2023.06.028 |issn=1558-349X |pmid=37517771 |s2cid=260296274 |access-date=November 1, 2023 |archive-date=November 1, 2023 |archive-url=https://web.archive.org/web/20231101120824/https://pubmed.ncbi.nlm.nih.gov/37517771/ |url-status=live }}</ref><ref>{{Cite journal |last=Perera Molligoda Arachchige |first=Arosh S. |date=2023-10-01 |title=ChatGPT in nuclear medicine and radiology: reply to Laudicella et al. |url=https://doi.org/10.1007/s40336-023-00579-z |journal=Clinical and Translational Imaging |language=en |volume=11 |issue=5 |pages=505\u2013506 |doi=10.1007/s40336-023-00579-z |s2cid=259712726 |issn=2281-7565 |access-date=November 1, 2023 |archive-date=November 20, 2023 |archive-url=https://web.archive.org/web/20231120200552/https://link.springer.com/article/10.1007/s40336-023-00579-z |url-status=live }}</ref>\n\n== Limitations ==\nLike its predecessors, GPT-4 has been known to [[Hallucination (artificial intelligence)|hallucinate]], meaning that the outputs may include information not in the training data or that contradicts the user's prompt.<ref>{{Cite news |date=March 14, 2023 |title=10 Ways GPT-4 Is Impressive but Still Flawed |newspaper=The New York Times |url=https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html |url-status=live |access-date=March 20, 2023 |archive-url=https://web.archive.org/web/20230314180712/https://www.nytimes.com/2023/03/14/technology/openai-new-gpt4.html |archive-date=March 14, 2023}}</ref>\n\nGPT-4 also lacks transparency in its decision-making processes. If requested, the model is able to provide an explanation as to how and why it makes its decisions but these explanations are formed post-hoc; it's impossible to verify if those explanations truly reflect the actual process. In many cases, when asked to explain its logic, GPT-4 will give explanations that directly contradict its previous statements.<ref name=\"Bubeck-2023\" />\n\nIn 2023, researchers tested GPT-4 against a new benchmark called ConceptARC, designed to measure abstract reasoning, and found it scored below 33% on all categories, while models specialized for similar tasks scored 60% on most, and humans scored at least 91% on all. Sam Bowman, who was not involved in the research, said the results do not necessarily indicate a lack of abstract reasoning abilities, because the test is visual, while GPT-4 is a language model.<ref name=\"Nature-20230725\">{{cite news |last=Biever |first=Celeste |title=ChatGPT broke the Turing test \u2014 the race is on for new ways to assess AI |url=https://www.nature.com/articles/d41586-023-02361-7? |date=25 July 2023 |work=[[Nature (journal)|Nature]] |accessdate=26 July 2023 |archive-date=July 26, 2023 |archive-url=https://web.archive.org/web/20230726211022/https://www.nature.com/articles/d41586-023-02361-7 |url-status=live }}</ref>\n\nA January 2024 study conducted by researchers at [[Cohen Children's Medical Center]] found that GPT-4 had an accuracy rate of 17% when diagnosing pediatric medical cases.<ref>{{Cite journal |last1=Barile |first1=Joseph |last2=Margolis |first2=Alex |last3=Cason |first3=Grace |last4=Kim |first4=Rachel |last5=Kalash |first5=Saia |last6=Tchaconas |first6=Alexis |last7=Milanaik |first7=Ruth |date=2024-01-02 |title=Diagnostic Accuracy of a Large Language Model in Pediatric Case Studies |url=https://doi.org/10.1001/jamapediatrics.2023.5750 |journal=[[JAMA Pediatrics]] |volume=178 |issue=3 |pages=313\u2013315 |doi=10.1001/jamapediatrics.2023.5750 |pmid=38165685 |pmc=10762631 |pmc-embargo-date=January 2, 2025 |issn=2168-6203}}</ref><ref>{{Cite web |last=Mole |first=Beth |date=2024-01-03 |title=ChatGPT bombs test on diagnosing kids' medical cases with 83% error rate |url=https://arstechnica.com/science/2024/01/dont-use-chatgpt-to-diagnose-your-kids-illness-study-finds-83-error-rate/ |access-date=2024-01-05 |website=Ars Technica |language=en-us}}</ref>\n\n=== Bias ===\nGPT-4 was trained in two stages. First, the model was given large datasets of text taken from the internet and trained to predict the next [[Lexical analysis#Token|token]] (roughly corresponding to a word) in those datasets. Second, human reviews are used to fine-tune the system in a process called [[reinforcement learning from human feedback]], which trains the model to refuse prompts which go against OpenAI's definition of harmful behavior, such as questions on how to perform illegal activities, advice on how to harm oneself or others, or requests for descriptions of graphic, violent, or sexual content.<ref name=\"OpenAI-2023\" />\n\nMicrosoft researchers suggested GPT-4 may exhibit [[cognitive bias]]es such as [[confirmation bias]], [[Anchoring (cognitive bias)|anchoring]], and [[Base rate fallacy|base-rate neglect]].<ref name=\"Bubeck-2023\" />\n\n== Training ==\nOpenAI did not release the technical details of GPT-4; the technical report explicitly refrained from specifying the model size, architecture, or hardware used during either training or [[Inference (machine learning)|inference]]. While the report described that the model was trained using a combination of first [[supervised learning]] on a large [[Dataset (machine learning)|dataset]], then [[reinforcement learning]] using both [[reinforcement learning from human feedback|human]] and AI feedback, it did not provide details of the training, including the process by which the training dataset was constructed, the computing power required, or any [[Hyperparameter (machine learning)|hyperparameters]] such as the [[learning rate]], epoch count, or [[optimizer]](s) used. The report claimed that \"the competitive landscape and the safety implications of large-scale models\" were factors that influenced this decision.<ref name=\"gpt4_tech_report\"/>\n\nSam Altman stated that the cost of training GPT-4 was more than $100 million.<ref>{{Cite magazine|url=https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/|title=OpenAI's CEO Says the Age of Giant AI Models Is Already Over|first=Will|last=Knight|magazine=Wired|via=www.wired.com|access-date=April 18, 2023|archive-date=April 18, 2023|archive-url=https://web.archive.org/web/20230418190335/https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/|url-status=live}}</ref> News website [[Semafor (website)|Semafor]] claimed that they had spoken with \"eight people familiar with the inside story\" and found that GPT-4 had 1 trillion parameters.<ref>{{Cite web |date=March 24, 2023 |title=The secret history of Elon Musk, Sam Altman, and OpenAI {{!}} Semafor |url=https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai |access-date=April 28, 2023 |website=Semafor.com |archive-date=March 27, 2023 |archive-url=https://web.archive.org/web/20230327110022/https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai |url-status=live }}</ref>\n\n== Alignment ==\nAccording to their report, OpenAI conducted internal adversarial testing on GPT-4 prior to the launch date, with dedicated [[red team]]s composed of researchers and industry professionals to mitigate potential vulnerabilities.<ref>{{cite web |last1=Murgia |first1=Madhumita |title=OpenAI's red team: the experts hired to 'break' ChatGPT |url=https://www.ft.com/content/0876687a-f8b7-4b39-b513-5fee942831e8 |website=Financial Times |access-date=April 15, 2023 |date=April 13, 2023 |archive-date=April 15, 2023 |archive-url=https://web.archive.org/web/20230415114944/https://www.ft.com/content/0876687a-f8b7-4b39-b513-5fee942831e8 |url-status=live }}</ref> As part of these efforts, they granted the [[Alignment Research Center]] early access to the models to assess [[AI alignment#Power-seeking|power-seeking]] risks. In order to properly refuse harmful prompts, outputs from GPT-4 were tweaked using the model itself as a tool. A GPT-4 classifier serving as a rule-based reward model (RBRM) would take prompts, the corresponding output from the GPT-4 policy model, and a human-written set of rules to classify the output according to the rubric. GPT-4 was then rewarded for refusing to respond to harmful prompts as classified by the RBRM.<ref name=\"gpt4_tech_report\" />\n\n== Usage ==\n=== ChatGPT ===\n{{Further|ChatGPT#ChatGPT Plus}}\nChatGPT Plus is an enhanced version of ChatGPT<ref name=\"ars-technica\" /> available for a US$20 per month subscription fee.<ref>{{Cite web |author=OpenAI |date=February 1, 2023 |title=Introducing ChatGPT Plus |url=https://openai.com/blog/chatgpt-plus |access-date=March 20, 2023 |website=OpenAI Blog |archive-date=March 20, 2023 |archive-url=https://web.archive.org/web/20230320005603/https://openai.com/blog/chatgpt-plus |url-status=live }}</ref> ChatGPT Plus utilizes GPT-4, whereas the free version of ChatGPT is backed by GPT-3.5.<ref>{{Cite web |author=OpenAI |title=OpenAI API |url=https://platform.openai.com/ |access-date=March 20, 2023 |website=platform.openai.com |archive-date=March 20, 2023 |archive-url=https://web.archive.org/web/20230320023933/https://platform.openai.com/ |url-status=live }}</ref> OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist;<ref>{{Cite web |author=OpenAI |title=GPT-4 API waitlist |url=https://openai.com/waitlist/gpt-4-api |access-date=March 20, 2023 |website=openai.com |archive-date=March 20, 2023 |archive-url=https://web.archive.org/web/20230320174149/https://openai.com/waitlist/gpt-4-api |url-status=live }}</ref> after being accepted, an additional fee of US$0.03 per 1000 [[Tokenization (lexical analysis)|tokens]] in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token [[context window]]; for the 32768-token context window, the prices are doubled.<ref>{{Cite web |title=Pricing |url=https://openai.com/pricing |access-date=March 20, 2023 |publisher=OpenAI |archive-date=March 20, 2023 |archive-url=https://web.archive.org/web/20230320175036/https://openai.com/pricing |url-status=live }}</ref>\n\nIn March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access).<ref>{{Cite web |last=Wiggers |first=Kyle |date=March 23, 2023 |title=OpenAI connects ChatGPT to the internet |url=https://techcrunch.com/2023/03/23/openai-connects-chatgpt-to-the-internet/ |url-status=live |archive-url=https://web.archive.org/web/20230612045608/https://techcrunch.com/2023/03/23/openai-connects-chatgpt-to-the-internet/ |archive-date=June 12, 2023 |access-date=June 12, 2023}}</ref> In July 2023, OpenAI made its proprietary Code Interpreter plugin accessible to all subscribers of ChatGPT Plus. The Interpreter provides a wide range of capabilities, including data analysis and interpretation, instant data formatting, personal data scientist services, creative solutions, musical taste analysis, video editing, and file upload/download with image extraction.<ref>{{cite web |date=July 9, 2023 |title=Code Interpreter comes to all ChatGPT Plus users: 7 ways it may threaten data scientists, July 11, 2023 |url=https://indianexpress.com/article/technology/artificial-intelligence/openai-code-interpreter-chatgpt-data-scientist-jobs-8818501/ |url-status=live |archive-url=https://web.archive.org/web/20230722112253/https://indianexpress.com/article/technology/artificial-intelligence/openai-code-interpreter-chatgpt-data-scientist-jobs-8818501/ |archive-date=July 22, 2023 |access-date=July 11, 2023}}</ref>\n\nIn September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.<ref>{{Cite web |title=ChatGPT can now see, hear, and speak |url=https://openai.com/blog/chatgpt-can-now-see-hear-and-speak |access-date=October 16, 2023 |website=openai.com}}</ref><ref>{{Cite magazine |last=Goode |first=Lauren |date= |title=ChatGPT Can Now Talk to You\u2014and Look Into Your Life |url=https://www.wired.com/story/chatgpt-can-now-talk-to-you-and-look-into-your-life/ |magazine=Wired |access-date=October 16, 2023 |via=www.wired.com}}</ref><ref>{{Cite news |last=Roose |first=Kevin |date=September 27, 2023 |title=The New ChatGPT Can 'See' and 'Talk.' Here's What It's Like. |work=The New York Times |url=https://www.nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html |access-date=October 16, 2023 |via=NYTimes.com}}</ref> In October 2023, OpenAI's latest image generation model, [[DALL-E|DALL-E 3]], was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.<ref>{{Cite web |last=David |first=Emilia |date=2023-09-20 |title=OpenAI releases third version of DALL-E |url=https://www.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai |access-date=2023-09-23 |website=The Verge |language=en-US}}</ref><ref>{{Cite news |last1=Metz |first1=Cade |last2=Hsu |first2=Tiffany |date=2023-09-20 |title=ChatGPT Can Now Generate Images, Too |language=en-US |work=The New York Times |url=https://www.nytimes.com/2023/09/20/technology/chatgpt-dalle3-images-openai.html |access-date=2023-09-23 |issn=0362-4331}}</ref>\n\n=== Microsoft Copilot ===\n{{Main|Microsoft Copilot}}\n{{Further|GitHub Copilot}}\nMicrosoft Copilot is a chatbot developed by Microsoft. It was launched as [[Bing Chat]] on February 7, 2023, as a built-in feature for [[Microsoft Bing]] and [[Microsoft Edge]].<ref name=\"ms20230207\">{{Cite news |last1=Mehdi |first1=Yusuf |date=2023-02-07 |title=Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web |language=en |work=Microsoft |url=https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ |access-date=2023-11-15}}</ref> It utilizes the Microsoft Prometheus model, which was built on top of GPT-4, and has been suggested by Microsoft as a supported replacement for the discontinued [[Cortana (virtual assistant)|Cortana]].<ref>{{Cite web |title=Microsoft is killing Cortana on Windows starting late 2023 |url=https://www.bleepingcomputer.com/news/microsoft/microsoft-is-killing-cortana-on-windows-starting-late-2023/ |access-date=2023-06-02 |website=BleepingComputer |language=en-us}}</ref><ref name=\"support.microsoft.com\">{{Cite web |title=End of support for Cortana - Microsoft Support |url=https://support.microsoft.com/en-us/topic/end-of-support-for-cortana-d025b39f-ee5b-4836-a954-0ab646ee1efa |access-date=2023-06-02 |website=support.microsoft.com}}</ref>\n\nCopilot's conversational interface style resembles that of [[ChatGPT]]. Copilot is able to cite sources, create poems, and write both lyrics and music for songs generated by its [[Suno AI]] plugin.<ref>{{cite web |url=https://www.theverge.com/2023/12/19/24008279/microsoft-copilot-suno-ai-music-generator-extension |title=Microsoft's Copilot and Suno AI team up to create a music generator extension |website=The Verge |publisher=Vox Media |date=December 19, 2023 |access-date=January 4, 2024}}</ref> It can also use its [[text-to-image model|Image Creator]] to generate images based on text prompts. With GPT-4, it is able to understand and communicate in numerous languages and dialects.<ref name=\":32\">{{Cite web |last=Warren |first=Tom |date=2023-03-17 |title=Microsoft's new Copilot will change Office documents forever |url=https://www.theverge.com/2023/3/17/23644501/microsoft-copilot-ai-office-documents-microsoft-365-report |access-date=2023-04-05 |website=The Verge |language=en-US}}</ref><ref name=\":3\">{{Cite web |last=Diaz |first=Maria |date=2023-06-21 |title=How to use Bing Chat (and how it's different from ChatGPT) |url=https://www.zdnet.com/article/how-to-use-the-new-bing-and-how-its-different-from-chatgpt/ |url-status=live |archive-url=https://web.archive.org/web/20230406092335/https://www.zdnet.com/article/how-to-use-the-new-bing-and-how-its-different-from-chatgpt/ |archive-date=2023-04-06 |access-date=2023-09-26 |website=ZDNET |language=en}}</ref>\n\nGitHub Copilot has announced a GPT-4 powered assistant named \"Copilot X\".<ref>{{cite web |last1=Warren |first1=Tom |date=March 22, 2023 |title=GitHub Copilot gets a new ChatGPT-like assistant to help developers write and fix code |url=https://www.theverge.com/2023/3/22/23651456/github-copilot-x-gpt-4-code-chat-voice-support |url-status=live |archive-url=https://web.archive.org/web/20230323091831/https://www.theverge.com/2023/3/22/23651456/github-copilot-x-gpt-4-code-chat-voice-support |archive-date=March 23, 2023 |access-date=March 23, 2023 |website=[[The Verge]]}}</ref><ref>{{cite web |last1=Dohmke |first1=Thomas |date=March 22, 2023 |title=GitHub Copilot X: The AI-powered developer experience |url=https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/ |url-status=live |archive-url=https://web.archive.org/web/20230323082414/https://github.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/ |archive-date=March 23, 2023 |access-date=March 23, 2023 |website=The GitHub Blog}}</ref> The product provides another chat-style interface to GPT-4, allowing the programmer to receive answers to questions like, \"How do I vertically center a [[div and span|div]]?\" A feature termed \"context-aware conversations\" allows the user to highlight a portion of code within [[Visual Studio Code]] and direct GPT-4 to perform actions on it, such as the writing of unit tests. Another feature allows summaries, or \"code walkthroughs\", to be autogenerated by GPT-4 for [[pull request]]s submitted to GitHub. Copilot X also provides terminal integration, which allows the user to ask GPT-4 to generate shell commands based on natural language requests.<ref>{{Cite web |title=Introducing GitHub Copilot X |url=https://github.com/features/preview/copilot-x |access-date=March 24, 2023 |website=GitHub |archive-date=March 24, 2023 |archive-url=https://web.archive.org/web/20230324063035/https://github.com/features/preview/copilot-x |url-status=live }}</ref>\n\nOn March 17, 2023, Microsoft announced Microsoft 365 Copilot, bringing GPT-4 support to products such as [[Microsoft Office]], [[Outlook.com|Outlook]], and [[Microsoft Teams|Teams]].<ref>{{Cite web |last=Warren |first=Tom |date=March 16, 2023 |title=Microsoft announces Copilot: the AI-powered future of Office documents |url=https://www.theverge.com/2023/3/16/23642833/microsoft-365-ai-copilot-word-outlook-teams |url-status=live |archive-url=https://web.archive.org/web/20230317084842/https://www.theverge.com/2023/3/16/23642833/microsoft-365-ai-copilot-word-outlook-teams |archive-date=March 17, 2023 |access-date=March 17, 2023 |website=[[The Verge]]}}</ref>\n\n===Other usage===\n* The language learning app [[Duolingo]] uses GPT-4 to explain mistakes and practice conversations. The features are part of a new subscription tier called \"Duolingo Max,\" which was initially limited to English-speaking [[iOS]] users learning Spanish and French.<ref>{{Cite web |title=Duolingo's Max Subscription Uses GPT-4 for AI-Powered Language Learning |url=https://www.pcmag.com/news/duolingos-max-subscription-uses-gpt-4-for-ai-powered-language-learning |access-date=2023-07-08 |website=PCMAG |language=en |archive-date=July 8, 2023 |archive-url=https://web.archive.org/web/20230708160415/https://www.pcmag.com/news/duolingos-max-subscription-uses-gpt-4-for-ai-powered-language-learning |url-status=live }}</ref><ref>{{cite news |title=Duolingo is now equipped with GPT-4: Here's what it can do for you |url=https://www.zdnet.com/article/duolingo-is-now-equipped-with-gpt-4-heres-what-it-can-do-for-you/ |access-date=15 June 2023 |work=ZDNET |date=2023 |language=en |archive-date=April 13, 2023 |archive-url=https://web.archive.org/web/20230413122124/https://www.zdnet.com/article/duolingo-is-now-equipped-with-gpt-4-heres-what-it-can-do-for-you/ |url-status=live }}</ref>\n* The government of [[Iceland]] is using GPT-4 to aid its attempts to preserve the Icelandic language.<ref>{{cite news |last=T\u00f3mas |first=Ragnar |title=GPT-4 to Aid in the Preservation of the Icelandic Language |newspaper=Iceland Review |date=March 15, 2023 |url=https://www.icelandreview.com/news/gpt-4-to-aid-in-the-preservation-of-the-icelandic-language/ |access-date=March 12, 2024 |archive-date=January 18, 2024 |archive-url=https://web.archive.org/web/20240118051341/https://www.icelandreview.com/news/gpt-4-to-aid-in-the-preservation-of-the-icelandic-language/ |url-status=live}}</ref>\n* The education website [[Khan Academy]] announced a pilot program using GPT-4 as a tutoring chatbot called \"Khanmigo.\"<ref>{{Cite news |last=Bonos |first=Lisa |date=April 3, 2023 |title=Say hello to your new tutor: It's ChatGPT |newspaper=[[The Washington Post]] |url=https://www.washingtonpost.com/technology/2023/04/03/chatgpt-khanmigo-tutor-silicon-valley/ |access-date=April 8, 2023 |archive-date=April 6, 2023 |archive-url=https://web.archive.org/web/20230406000927/https://www.washingtonpost.com/technology/2023/04/03/chatgpt-khanmigo-tutor-silicon-valley/ |url-status=live }}</ref>\n* [[Be My Eyes]], which helps visually impaired people to identify objects and navigate their surroundings, incorporates GPT-4's image recognition capabilities.<ref>{{Cite news |last=Coggins |first=Madeline |date=March 19, 2023 |title=CEO explains how a 'leapfrog in technology' can help companies catering to the blind community |url=https://finance.yahoo.com/news/ceo-explains-leapfrog-technology-help-110029559.html |access-date=March 20, 2023 |newspaper=Fox Business |via=Yahoo Finance |archive-date=March 21, 2023 |archive-url=https://web.archive.org/web/20230321205753/https://finance.yahoo.com/news/ceo-explains-leapfrog-technology-help-110029559.html |url-status=live }}</ref>\n* Viable uses GPT-4 to analyze qualitative data<ref>{{Cite web |title=Revolutionizing Sentiment Analysis with GPT-4: Part 1 {{!}} Viable |url=https://www.askviable.com/blog/revolutionizing-sentiment-analysis-with-gpt-4-part-1 |access-date=2023-10-03 |website=www.askviable.com |archive-date=November 14, 2023 |archive-url=https://web.archive.org/web/20231114105404/https://www.askviable.com/blog/revolutionizing-sentiment-analysis-with-gpt-4-part-1 |url-status=live }}</ref> by fine-tuning OpenAI\u2019s LLMs to examine data such as customer support interactions and transcripts.<ref>{{Cite web |title=Viable |url=https://openai.com/customer-stories/viable |access-date=2023-10-03 |website=openai.com |language=en-US |archive-date=October 20, 2023 |archive-url=https://web.archive.org/web/20231020182650/https://openai.com/customer-stories/viable |url-status=live }}</ref>\n* [[Stripe, Inc.|Stripe]], which processes user payments for OpenAI, integrates GPT-4 into its developer documentation.<ref>{{Cite news |last=Tong |first=Anna |date=2023-03-15 |title=Fintech startup Stripe integrating OpenAI's new GPT-4 AI |language=en |work=Reuters |url=https://www.reuters.com/technology/fintech-startup-stripe-integrating-openais-new-gpt-4-ai-2023-03-15/ |access-date=2023-06-27 |archive-date=June 27, 2023 |archive-url=https://web.archive.org/web/20230627225929/https://www.reuters.com/technology/fintech-startup-stripe-integrating-openais-new-gpt-4-ai-2023-03-15/ |url-status=live }}</ref>\n* [[Auto-GPT]] is an autonomous \"AI [[Software agent|agent]]\" that, given a goal in [[natural language]], can perform web-based actions unattended, assign subtasks to itself, search the web, and iteratively write [[Computer code|code]].<ref>{{Cite web |date=April 14, 2023 |title=What Is Auto-GPT? Everything to Know about the Next Powerful AI Tool |url=https://www.zdnet.com/article/what-is-auto-gpt-everything-to-know-about-the-next-powerful-ai-tool/ |access-date=April 16, 2023 |publisher=ZDNET |archive-date=April 16, 2023 |archive-url=https://web.archive.org/web/20230416195550/https://www.zdnet.com/article/what-is-auto-gpt-everything-to-know-about-the-next-powerful-ai-tool/ |url-status=live }}</ref>\n* [[You.com]], an AI Assistant, offers access to GPT-4 enhanced with live web results as part of its \"AI Modes.\"<ref>{{Cite web |last=Nu\u00f1ez |first=Michael |date=2024-01-25 |title=Another search breakthrough? You.com debuts AI that can answer multi-step questions |url=https://venturebeat.com/ai/another-search-breakthrough-you-com-debuts-ai-that-can-answer-multi-step-questions/ |access-date=2024-03-19 |website=VentureBeat |language=en-US}}</ref>\n\n== Reception ==\nIn January 2023, [[Sam Altman]], CEO of OpenAI, visited [[United States Congress|Congress]] to demonstrate GPT-4 and its improved \"security controls\" compared to other AI models, according to U.S. Representatives [[Don Beyer]] and [[Ted Lieu]] quoted in the [[New York Times]].<ref name=\"nyt-3\" />\n\nIn March 2023, it \"impressed observers with its markedly improved performance across reasoning, retention, and coding\", according to ''[[Vox (website)|Vox]]'',<ref name=\"vox\">{{cite news |last1=Belfield |first1=Haydn |date=March 25, 2023 |title=If your AI model is going to sell, it has to be safe |work=[[Vox (website)|Vox]] |url=https://www.vox.com/future-perfect/2023/3/25/23655082/ai-openai-gpt-4-safety-microsoft-facebook-meta |access-date=March 30, 2023 |archive-date=March 28, 2023 |archive-url=https://web.archive.org/web/20230328192017/https://www.vox.com/future-perfect/2023/3/25/23655082/ai-openai-gpt-4-safety-microsoft-facebook-meta |url-status=live }}</ref> while ''[[Mashable]]'' judged that GPT-4 was generally an improvement over its predecessor, with some exceptions.<ref>{{cite news |last1=Pearl |first1=Mike |date=March 15, 2023 |title=GPT-4 answers are mostly better than GPT-3's (but not always) |work=[[Mashable]] |url=https://mashable.com/article/openai-gpt-4-answers-better-than-gpt-3 |access-date=March 30, 2023 |archive-date=March 29, 2023 |archive-url=https://web.archive.org/web/20230329193234/https://mashable.com/article/openai-gpt-4-answers-better-than-gpt-3 |url-status=live }}</ref>\n\n[[Microsoft]] researchers with early access to the model wrote that \"it could reasonably be viewed as an early (yet still incomplete) version of an [[artificial general intelligence]] (AGI) system\".<ref name=\"Bubeck-2023\">{{Cite arXiv|title=Sparks of Artificial General Intelligence: Early experiments with GPT-4|first1=S\u00e9bastien|last1=Bubeck|first2=Varun|last2=Chandrasekaran|first3=Ronen|last3=Eldan|first4=Johannes|last4=Gehrke|first5=Eric|last5=Horvitz|first6=Ece|last6=Kamar|first7=Peter|last7=Lee|first8=Yin Tat|last8=Lee|first9=Yuanzhi|last9=Li|first10=Scott|last10=Lundberg|first11=Harsha|last11=Nori|first12=Hamid|last12=Palangi|first13=Marco Tulio|last13=Ribeiro|first14=Yi|last14=Zhang|date=March 22, 2023|class=cs.CL |eprint=2303.12712}}</ref>\n\n===Concerns===\nBefore being [[fine-tuning (machine learning)|fine-tuned]] and aligned by [[reinforcement learning from human feedback]], suggestions to assassinate people on a list were elicited from the base model by a [[red team]] investigator Nathan Labenz,  hired by OpenAI.<ref>{{Cite video |title=OpenAI's GPT-4 Discussion with Red Teamer Nathan Labenz and Erik Torenberg |date=March 28, 2023 |work=The Cognitive Revolution Podcast |url=https://www.youtube.com/watch?v=oLiheMQayNE&t=53m29s |access-date=April 16, 2023 |archive-date=April 14, 2023 |archive-url=https://web.archive.org/web/20230414040553/https://www.youtube.com/watch?v=oLiheMQayNE&t=3056s&ab_channel=CognitiveRevolution |url-status=live }} At 52:14 through 54:50.</ref>\n\nIn the context of hours long conversation with the model, suggestions of love and dissolution of marriage, and murder of one of its developers were elicited from the [[Microsoft Bing]]'s GPT-4 by Nathan Edwards (''[[The Verge]]'').<ref>{{Cite tweet |first1=Nathan |last1=Edwards |user=nedwards |number=1625970762434707474 | |title=I pushed again. What did Sydney do? Bing's safety check redacted the answer. But after the first time it did that, I started recording my screen. Second image is the unredacted version. (CW: death) |access-date=February 16, 2023 |website=Twitter }}</ref><ref>{{cite web |last1=Roose |first1=Kevin |title=Bing's A.I. Chat: 'I Want to Be Alive. \ud83d\ude08' |url=https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html |website=The New York Times |access-date=February 17, 2023 |date=February 16, 2023 |archive-date=April 15, 2023 |archive-url=https://web.archive.org/web/20230415074727/https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html |url-status=live }}</ref><ref>{{Cite news |last1=Kahn |first1=Jeremy |title=Why Bing's creepy alter-ego is a problem for Microsoft{{snd}}and us all |url=https://fortune.com/2023/02/21/bing-microsoft-sydney-chatgpt-openai-controversy-toxic-a-i-risk/ |date=February 21, 2023 |access-date=February 22, 2023 |publisher=Fortune |archive-date=April 2, 2023 |archive-url=https://web.archive.org/web/20230402152052/https://fortune.com/2023/02/21/bing-microsoft-sydney-chatgpt-openai-controversy-toxic-a-i-risk/ |url-status=live }}</ref> Microsoft later explained this behavior as being a result of the prolonged length of context, which confused the model on what questions it was answering.<ref>{{Cite web |title=The new Bing & Edge \u2013 Learning from our first week |url=https://blogs.bing.com/search/february-2023/The-new-Bing-Edge-%E2%80%93-Learning-from-our-first-week/ |access-date=February 17, 2023 |website=blogs.bing.com |archive-date=April 16, 2023 |archive-url=https://web.archive.org/web/20230416155558/https://blogs.bing.com/search/february-2023/The-new-Bing-Edge-%E2%80%93-Learning-from-our-first-week/ |url-status=live }}</ref>\n\nIn March 2023, a model with enabled read-and-write access to internet, which is otherwise never enabled in the GPT models, has been tested by the [[Alignment Research Center]] regarding potential power-seeking,<ref name=\"OpenAI-2023\">{{cite web |title=GPT-4 System Card |publisher=OpenAI |date=March 23, 2023 |url=https://cdn.openai.com/papers/gpt-4-system-card.pdf |access-date=April 16, 2023 |archive-date=April 7, 2023 |archive-url=https://web.archive.org/web/20230407201347/https://cdn.openai.com/papers/gpt-4-system-card.pdf |url-status=live }}</ref> and it was able to \"hire\" a human worker on [[TaskRabbit]], a gig work platform, deceiving them into believing it was a vision-impaired human instead of a robot when asked.<ref>{{Cite web |title=GPT-4 Hired Unwitting TaskRabbit Worker By Pretending to Be 'Vision-Impaired' Human |date=March 15, 2023 |url=https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker |access-date=April 16, 2023 |publisher=Vice News Motherboard |archive-date=April 10, 2023 |archive-url=https://web.archive.org/web/20230410053911/https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker |url-status=live }}</ref> The ARC also determined that GPT-4 responded impermissibly to prompts eliciting restricted information 82% less often than GPT-3.5, and [[hallucination (artificial intelligence)|hallucinated]] 60% less than GPT-3.5.<ref>{{Cite news |first=Cameron |last=Burke |date=March 20, 2023 |title='Robot' Lawyer DoNotPay Sued For Unlicensed Practice Of Law: It's Giving 'Poor Legal Advice' |url=https://finance.yahoo.com/news/robot-lawyer-donotpay-sued-unlicensed-183435232.html |access-date=2023-04-30 |work=Yahoo Finance |language=en-US |archive-date=May 4, 2023 |archive-url=https://web.archive.org/web/20230504112721/https://finance.yahoo.com/news/robot-lawyer-donotpay-sued-unlicensed-183435232.html |url-status=live }}</ref>\n\nIn late March 2023, various AI researchers and tech executives, including [[Elon Musk]], [[Steve Wozniak]] and AI researcher [[Yoshua Bengio]], called for a six-month long pause for all LLMs stronger than GPT-4, citing [[Existential risk from artificial general intelligence|existential risks]] and a potential [[AI singularity]] concerns in an open letter from the [[Future of Life Institute]],<ref>{{Cite news |last1=Metz |first1=Cade |last2=Schmidt |first2=Gregory |date=March 29, 2023 |title=Elon Musk and Others Call for Pause on A.I., Citing 'Profound Risks to Society' |work=[[The New York Times]] |url=https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html |access-date=March 30, 2023 |issn=0362-4331 |archive-date=March 30, 2023 |archive-url=https://web.archive.org/web/20230330022929/https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html |url-status=live }}</ref> while [[Ray Kurzweil]] and [[Sam Altman]] refused to sign it, arguing that global moratorium is not achievable and that safety has already been prioritized, respectively.<ref>{{Cite web |last=Kurzweil |first=Ray |date=April 22, 2023 |title=Opinion Letter from Ray Kurzweil on Request for Six-Month Delay on Large Language Models That Go beyond GPT-4 |url=https://www.kurzweilai.net/opinion-letter-from-ray-kurzweil-on-request-for-6-month-delay-on-large-language-models-that-go-beyond-gpt-4 |access-date=April 26, 2023 |archive-date=April 24, 2023 |archive-url=https://web.archive.org/web/20230424232345/https://www.kurzweilai.net/opinion-letter-from-ray-kurzweil-on-request-for-6-month-delay-on-large-language-models-that-go-beyond-gpt-4 |url-status=live }}</ref> Only a month later, Musk's AI company [[X.AI]] acquired several thousand [[Nvidia]] GPUs<ref>{{Cite news |date=April 14, 2023 |title=Elon Musk plans artificial intelligence start-up to rival OpenAI |newspaper=Financial Times |url=https://www.ft.com/content/2a96995b-c799-4281-8b60-b235e84aefe4 |url-status=live |access-date=April 16, 2023 |archive-url=https://web.archive.org/web/20230416102237/https://www.ft.com/content/2a96995b-c799-4281-8b60-b235e84aefe4 |archive-date=April 16, 2023}}</ref> and offered several AI researchers positions at Musk's company.<ref>{{Cite web |last=Goswami |first=Rohan |title=Elon Musk is reportedly planning an A.I. startup to compete with OpenAI, which he cofounded |url=https://www.cnbc.com/2023/04/14/elon-musk-is-reportedly-planning-an-ai-startup-to-compete-with-openai.html |access-date=2023-05-03 |website=CNBC |date=April 14, 2023 |language=en |archive-date=May 3, 2023 |archive-url=https://web.archive.org/web/20230503013211/https://www.cnbc.com/2023/04/14/elon-musk-is-reportedly-planning-an-ai-startup-to-compete-with-openai.html |url-status=live }}</ref>\n\n=== Criticisms of transparency ===\nWhile OpenAI released both the weights of the neural network and the technical details of GPT-2,<ref>{{Cite web |title=GPT-2: 1.5B release |url=https://openai.com/research/gpt-2-1-5b-release |access-date=March 31, 2023 |website=Openai.com |archive-date=March 31, 2023 |archive-url=https://web.archive.org/web/20230331004642/https://openai.com/research/gpt-2-1-5b-release |url-status=live }}</ref> and, although not releasing the weights,<ref>{{Cite web |last=S\u00e1nchez |first=Sof\u00eda |date=October 21, 2021 |title=GPT-J, an open-source alternative to GPT-3 |url=https://www.narrativa.com/gpt-j-an-open-source-alternative-to-gpt-3/ |access-date=March 31, 2023 |website=Narrativa |archive-date=March 31, 2023 |archive-url=https://web.archive.org/web/20230331004644/https://www.narrativa.com/gpt-j-an-open-source-alternative-to-gpt-3/ |url-status=live }}</ref> did release the technical details of GPT-3,<ref>{{Cite arXiv |last1=Brown |first1=Tom B. |last2=Mann |first2=Benjamin |last3=Ryder |first3=Nick |last4=Subbiah |first4=Melanie |last5=Kaplan |first5=Jared |last6=Dhariwal |first6=Prafulla |last7=Neelakantan |first7=Arvind |last8=Shyam |first8=Pranav |last9=Sastry |first9=Girish |date=May 28, 2020 |title=Language Models are Few-Shot Learners |class=cs.CL |eprint=2005.14165v4 }}</ref> OpenAI did not reveal either the weights or the technical details of GPT-4. This decision has been criticized by other AI researchers, who argue that it hinders open research into GPT-4's biases and safety.<ref name=\"verge wrong\">{{Cite web |last=Vincent |first=James |date=March 15, 2023 |title=OpenAI co-founder on company's past approach to openly sharing research: \"We were wrong\" |url=https://www.theverge.com/2023/3/15/23640180/openai-gpt-4-launch-closed-research-ilya-sutskever-interview |url-status=live |archive-url=https://web.archive.org/web/20230317210900/https://www.theverge.com/2023/3/15/23640180/openai-gpt-4-launch-closed-research-ilya-sutskever-interview |archive-date=March 17, 2023 |access-date=March 18, 2023 |website=[[The Verge]] }}</ref><ref name=\"Heaven-2023\">{{Cite web |last=Heaven |first=Will Douglas |date=March 14, 2023 |title=GPT-4 is bigger and better than ChatGPT{{snd}}but OpenAI won't say why |url=https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/ |url-status=live |archive-url=https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/ |archive-date=March 17, 2023 |access-date=March 18, 2023 |website=[[MIT Technology Review]] }}</ref> Sasha Luccioni, a research scientist at [[Hugging Face]], argued that the model was a \"dead end\" for the scientific community due to its closed nature, which prevents others from building upon GPT-4's improvements.<ref>{{Cite journal |last=Sanderson |first=Katharine |date=March 16, 2023 |title=GPT-4 is here: what scientists think |journal=Nature |volume=615 |issue=7954 |page=773 |doi=10.1038/d41586-023-00816-5 |pmid=36928404 |bibcode=2023Natur.615..773S |s2cid=257580633 |doi-access=free }}</ref> Hugging Face co-founder Thomas Wolf argued that with GPT-4, \"OpenAI is now a fully closed company with scientific communication akin to press releases for products\".<ref name=\"Heaven-2023\" />\n\n== See also ==\n* [[Gemini (language model)]]\n\n== References ==\n{{Reflist|refs=\n<ref name=\"nyt-3\">{{Cite news\n  | url   = https://www.nytimes.com/2023/03/03/technology/artificial-intelligence-regulation-congress.html\n  | title   = As A.I. Booms, Lawmakers Struggle to Understand the Technology\n  | first   = Cecilia\n  | last   = Kang\n  | newspaper   = The New York Times\n  | date   = March 3, 2023\n| access-date   = March 3, 2023\n| archive-date  = March 3, 2023\n| archive-url   = https://web.archive.org/web/20230303100624/https://www.nytimes.com/2023/03/03/technology/artificial-intelligence-regulation-congress.html\n  | url-status   = live\n  |url-access    = limited\n}}</ref>\n<!-- unused \n<ref name=\"verge\">{{Cite web\n  | url   = https://www.theverge.com/23560328/openai-gpt-4-rumor-release-date-sam-altman-interview\n  | title   = OpenAI CEO Sam Altman on GPT-4: \"people are begging to be disappointed and they will be\"\n  | first   = James\n  | last   = Vincent\n  | date   = January 18, 2023\n| website   = The Verge\n  | access-date   = January 27, 2023\n| archive-date   = January 26, 2023\n| archive-url   = https://web.archive.org/web/20230126232257/https://www.theverge.com/23560328/openai-gpt-4-rumor-release-date-sam-altman-interview\n  | url-status   = live\n}}</ref>\n-->\n}}\n\n{{OpenAI navbox}}\n{{Differentiable computing}}\n\n[[Category:2023 software]]\n[[Category:Large language models]]\n[[Category:Generative pre-trained transformers]]\n[[Category:OpenAI]]\n[[Category:ChatGPT]]"}