{"title": "OpenAI", "page_id": 48795986, "revision_id": 1130746249, "revision_timestamp": "2022-12-31T19:23:56Z", "content": "{{Short description|Artificial intelligence research laboratory}}\n{{Distinguish|OpenAL}}\n{{Use mdy dates|date=December 2022}}\n{{Use American English|date=December 2022}}\n<!-- this hatnote exists b/c User:Maths314 appears concerned people looking for OpenAL will come here since OpenAI looks like OpenAl in many fonts? Feel free to open thread in Talk if you want to remove hatnote. -->\n{{Infobox company\n| name = OpenAI\n| logo = OpenAI Logo.svg\n| logo_size = 270px\n| caption = \n| industry = [[Artificial intelligence]]\n| founders = {{Unbulleted list|\n| [[Sam Altman]]\n| [[Ilya Sutskever]]\n| Greg Brockman\n| [[Wojciech Zaremba]]\n| [[Elon Musk]]\n| John Schulman\n}}\n| founded = {{Start date and age|2015|12|11}}\n| hq_location = [[Pioneer Building (San Francisco)|Pioneer Building, San Francisco]], California, US<ref>{{cite news |url=https://www.nytimes.com/2015/12/12/science/artificial-intelligence-research-center-is-founded-by-silicon-valley-investors.html |title=Artificial-Intelligence Research Center Is Founded by Silicon Valley Investors |first1=John |last1=Markoff |date=December 11, 2015 |newspaper=[[The New York Times]] |access-date=December 12, 2015 }}</ref><ref name=technologyreview/>\n| key_people = {{Unbulleted list\n| Greg Brockman {{small|([[Chairperson|chairman]] & [[President (corporate title)|president]])}}\n| Sam Altman {{small|([[Chief executive officer|CEO]])}}\n| Ilya Sutskever {{small|([[Chief scientific officer|chief scientist]])}} \n}}\n| area_served = \n| products = {{Unbulleted list\n| [[DALL-E]]\n| [[GPT-3]]\n| [[GPT-2]]\n| [[OpenAI Five]]\n| [[ChatGPT]]\n}}\n| services = \n| revenue = \n| equity = \n| equity_year = \n| num_employees = >120 ({{as of|2020|lc=on}})<ref name=technologyreview/>\n| subsid = \n| owner = \n| homepage = {{Official URL}}\n| footnotes = \n}}\n\n'''OpenAI''' is an [[artificial intelligence]] (AI) research laboratory consisting of the for-profit corporation '''OpenAI LP''' and its [[parent company]], the non-profit '''OpenAI Inc.''' The company conducts research in the field of AI with the stated goal of promoting and developing [[friendly AI]] in a way that benefits humanity as a whole. The organization was founded in San Francisco in late 2015 by [[Sam Altman]], [[Elon Musk]], and others, who collectively pledged US$1&nbsp;billion. Musk resigned from the board in February 2018 but remained a donor. In 2019, OpenAI LP received a {{USD}}1&nbsp;billion investment from [[Microsoft]] and Matthew Brown Companies. OpenAI is headquartered at the [[Pioneer Building (San Francisco)|Pioneer Building]] in [[Mission District, San Francisco]].\n\n== History ==\n[[File:Pioneer Building, San Francisco (2019) -1.jpg|thumb|OpenAI is headquartered at the [[Pioneer Building (San Francisco)|Pioneer Building]] in San Francisco.|277x277px]]\n\nIn December 2015, [[Sam Altman]], [[Elon Musk]], Greg Brockman, [[Reid Hoffman]], [[Jessica Livingston]], [[Peter Thiel]], [[Amazon Web Services|Amazon Web Services (AWS)]], [[Infosys]], and [[YC research|YC Research]]  announced<ref>{{Cite web |date=2015-12-12 |title=Introducing OpenAI |url=https://openai.com/blog/introducing-openai/ |access-date=2022-12-23 |website=OpenAI |language=en}}</ref> the formation of OpenAI and pledged over {{USD}}1&nbsp;billion to the venture. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public.<ref>{{cite web |title=Introducing OpenAI |url=https://blog.openai.com/introducing-openai/ |website=OpenAI Blog |date=December 12, 2015}}</ref><ref name=\"bbc-giants\" /> OpenAI is headquartered at the [[Pioneer Building (San Francisco)|Pioneer Building]] in [[Mission District, San Francisco]].<ref name=\":02\">{{Cite news|url=https://gizmodo.com/elon-musks-neuralink-sought-to-open-an-animal-testing-f-1823167674|title=Elon Musk's Neuralink Sought to Open an Animal Testing Facility in San Francisco|last=Conger|first=Kate|work=Gizmodo|access-date=October 11, 2018|language=en-US}}</ref><ref name=technologyreview>{{Cite web|url=https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/|title=The messy, secretive reality behind OpenAI's bid to save the world|last=Hao|first=Karen|date=February 17, 2020|website=MIT Technology Review|language=en-US|access-date=March 9, 2020}}</ref>\n\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for [[reinforcement learning]] research.<ref name=\":0\" /> In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications<!--AI training ground that spans any software running on any machine, from games to web browsers to protein folders-->.<ref>{{cite magazine|url=https://www.wired.com/2016/12/openais-universe-computers-learn-use-apps-like-humans/|title=Elon Musk's Lab Wants to Teach Computers to Use Apps Just Like Humans Do|last1=Metz|first1=Cade|magazine=WIRED|access-date=December 31, 2016}}</ref><ref>{{cite news|url=https://techcrunch.com/2016/12/05/openais-universe-is-the-fun-parent-every-artificial-intelligence-deserves/|title=OpenAI's Universe is the fun parent every artificial intelligence deserves|last1=Mannes|first1=John|work=TechCrunch|access-date=December 31, 2016}}</ref><ref>{{cite web|url=https://universe.openai.com/|title=OpenAI \u2013 Universe|language=en-us|access-date=December 31, 2016}}</ref><ref>{{cite web|url=https://www.theregister.co.uk/2016/12/05/openai_universe_reinforcement_learning/|title=Elon Musk-backed OpenAI reveals Universe \u2013 a universal training ground for computers|last1=Claburn|first1=Thomas|website=The Register|access-date=December 31, 2016}}</ref>\n\nIn 2018, Musk resigned his board seat, citing \"a potential future conflict (of interest)\" with [[Tesla Autopilot|Tesla AI development for self driving cars]], but remained a donor.<ref name=\"musk_resigns\">{{Cite web|url=https://www.theverge.com/2018/2/21/17036214/elon-musk-openai-ai-safety-leaves-board|title=Elon Musk leaves board of AI safety group to avoid conflict of interest with Tesla|first=James|last=Vincent|date=February 21, 2018|website=The Verge}}</ref> \n\nIn 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with profit cap set to 100X on any investment. The company distributed [[Equity (finance)|equity]] to its employees and partnered with [[Microsoft]], who announced an investment package of {{USD}}1&nbsp;billion into the company. OpenAI then announced its intention to commercially license its technologies.<ref name=\":1\">{{Cite web|url=https://openai.com/blog/microsoft/|title=Microsoft Invests in and Partners with OpenAI to Support Us Building Beneficial AGI|date=July 22, 2019|website=OpenAI|language=en|access-date=February 21, 2020}}</ref> \n\nIn 2020, OpenAI announced [[GPT-3]], a language model trained on trillions of words from the Internet. It also announced that an associated [[Application programming interface|API]], named simply \"the API\", would form the heart of its first commercial product. GPT-3 is aimed at natural language answering of questions, but it can also translate between languages and coherently generate improvised text.<ref name=\"2020-06-11_Bloomberg\">{{cite news |url=https://www.bloomberg.com/news/articles/2020-06-11/trillions-of-words-analyzed-openai-sets-loose-ai-language-colossus |title=Trillions of Words Analyzed, OpenAI Sets Loose AI Language Colossus |last=Vance |first=Ashlee |date=June 11, 2020 |work=[[Bloomberg News]]}}</ref> \n\nIn 2021, OpenAI introduced [[DALL-E]]. One year later, their newest system, DALL\u00b7E 2, generates more realistic and accurate images with 4x greater resolution. \n\nIn 2022, OpenAI released a preview of [[ChatGPT]], which interacts using conversation, to the general public.<ref>{{Cite web|url=https://openai.com/blog/chatgpt/|title=ChatGPT: Optimizing Language Models for Dialogue|date=November 30, 2022|website=OpenAI}}</ref> \n\n=== Participants ===\n* CEO and co-founder:<ref>{{cite news|url=https://www.latimes.com/business/story/2019-07-22/microsoft-openai|title=Microsoft to invest $1&nbsp;billion in OpenAI|last1=Bass|first1=Dina|date=July 22, 2019|work=Los Angeles Times|access-date=July 22, 2019}}</ref> [[Sam Altman]], former president of the startup accelerator [[Y Combinator (company)|Y Combinator]]\n* President and co-founder:<ref name=\"auto\">{{Cite news|url=https://analyticsindiamag.com/openai-gets-a-new-president-cto-coo-in-the-latest-rejig//|title=OpenAI gets a new president, CTO & COO in the latest rejig|last=Bordoloi|first=Pritam|date=May 9, 2022|work=AIM|access-date=October 11, 2022}}</ref> Greg Brockman, former CTO, 3rd employee of [[Stripe (company)|Stripe]]<ref name=\"seattle-investors\"/>\n* Chief Scientist and co-founder [[Ilya Sutskever]], a former Google expert on machine learning<ref name=\"seattle-investors\"/>\n* Chief Technology Officer:<ref name=\"auto\"/> Mira Murati, previously at [[Leap Motion]] and [[Tesla, Inc.]]\n* Chief Operating Officer:<ref name=\"auto\"/> Brad Lightcap, previously at [[Y Combinator]] and [[JPMorgan Chase]]\n\nBoard of the OpenAI nonprofit:\n* Greg Brockman\n* [[Ilya Sutskever]]\n* [[Sam Altman]]\n* Adam D'Angelo\n* [[Reid Hoffman]]\n* [[Will Hurd]]\n* Tasha McCauley\n* Helen Toner\n* [[Shivon Zilis]]\n\nOther backers of the project include:<ref name=\"seattle-investors\"/>\n* [[Reid Hoffman]], [[LinkedIn]] co-founder<ref name=mercury-back>{{cite news|last1=Liedtke|first1=Michael|title=Elon Musk, Peter Thiel, Reid Hoffman, others back $1&nbsp;billion OpenAI research center|url=http://www.mercurynews.com/business/ci_29256196/elon-musk-peter-thiel-reid-hoffman-others-back|access-date=December 19, 2015|work=[[Mercury News]]}}</ref>\n* [[Peter Thiel]], [[PayPal]] co-founder<ref name=mercury-back/>\n* [[Jessica Livingston]], a founding partner of Y Combinator\n\nCompanies:\n* [[Microsoft]]<ref>{{cite web |last1=Vincent |first1=James |date=July 22, 2019 |title=Microsoft invests $1&nbsp;billion in OpenAI to pursue holy grail of artificial intelligence |url=https://www.theverge.com/2019/7/22/20703578/microsoft-openai-investment-partnership-1-billion-azure-artificial-general-intelligence-agi |access-date=July 23, 2019 |website=The Verge}}</ref>\n* [[Khosla Ventures]]<ref>{{Cite web |date=2015-12-11 |title=About OpenAI |url=https://openai.com/about/ |access-date=2022-12-23 |website=OpenAI |language=en}}</ref>\n* [[Infosys]]<ref>{{cite news |date=December 12, 2015 |title=Elon Musk, Infosys, others back OpenAI with $1 bn |work=Business Standard India |agency=Indo-Asian News Service |url=https://www.business-standard.com/article/news-ians/elon-musk-infosys-others-back-openai-with-1-bn-115121200862_1.html |access-date=August 30, 2019}}</ref>\n* Matthew Brown Companies\n\nThe group started in early January 2016 with nine researchers. According to ''[[Wired (magazine)|Wired]]'', Brockman met with [[Yoshua Bengio]], one of the \"founding fathers\" of the [[deep learning]] movement, and drew up a list of the \"best researchers in the field\". Microsoft's [[Peter Lee (computer scientist)|Peter Lee]] stated that the cost of a top AI researcher exceeds the cost of a top [[National Football League|NFL]] quarterback prospect. While OpenAI pays corporate-level (rather than nonprofit-level) salaries, it doesn't currently pay AI researchers salaries comparable to those of Facebook or Google. Nevertheless, Sutskever stated that he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI researcher [[Wojciech Zaremba]] stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.<ref name=wired_inside />\n\n== Motives ==\nSome scientists, such as [[Stephen Hawking]] and [[Stuart J. Russell|Stuart Russell]], have articulated concerns that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable \"[[intelligence explosion]]\" could lead to [[human extinction]]. Musk characterizes AI as humanity's \"biggest existential threat.\"<ref>{{cite news |last1=Piper |first1=Kelsey |title=Why Elon Musk fears artificial intelligence |url=https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai |access-date=March 10, 2021 |work=Vox |date=November 2, 2018 |language=en}}</ref> OpenAI's founders structured it as a non-profit so that they could focus its research on making positive long-term contributions to humanity.<ref name=bbc-giants>{{cite news|title=Tech giants pledge $1bn for 'altruistic AI' venture, OpenAI|url=https://www.bbc.com/news/technology-35082344|access-date=December 19, 2015|work=[[BBC News]]|date=December 12, 2015}}</ref>\n\nMusk and Altman have stated they are partly motivated by concerns about the [[existential risk from artificial general intelligence]].<ref name=csmonitor>{{cite news|last1=Lewontin|first1=Max|title=Open AI: Effort to democratize artificial intelligence research?|url=http://www.csmonitor.com/Technology/2015/1214/Open-AI-Effort-to-democratize-artificial-intelligence-research|access-date=December 19, 2015|work=[[The Christian Science Monitor]]|date=December 14, 2015}}</ref><ref name=wired_inside /> OpenAI states that \"it's hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\".<ref name=bbc-giants/> Research on safety cannot safely be postponed: \"because of AI's surprising history, it's hard to predict when human-level AI might come within reach.\"<ref>{{cite news|last1=Mendoza|first1=Jessica|title=Tech leaders launch nonprofit to save the world from killer robots|url=http://www.csmonitor.com/Science/2015/1214/Tech-leaders-launch-nonprofit-to-save-the-world-from-killer-robots|work=[[The Christian Science Monitor]]}}</ref> OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible...\".<ref name=bbc-giants/> Co-chair Sam Altman expects the decades-long project to surpass human intelligence.<ref name=wired_far_more>{{cite magazine|last1=Metz|first1=Cade|title=Elon Musk's Billion-Dollar AI Plan Is About Far More Than Saving the World|url=https://www.wired.com/2015/12/elon-musks-billion-dollar-ai-plan-is-about-far-more-than-saving-the-world/|access-date=December 19, 2015|magazine=[[Wired (magazine)|Wired]]|date=December 15, 2015|quote=\"Altman said they expect this decades-long project to surpass human intelligence.\"}}</ref>\n\n[[Vishal Sikka]], former CEO of [[Infosys]], stated that an \"openness\" where the endeavor would \"produce results generally in the greater interest of humanity\" was a fundamental requirement for his support, and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\".<ref>{{cite web|author1=Vishal Sikka|title=OpenAI: AI for All|url=http://www.infosysblogs.com/infytalk/2015/12/openai_ai_for_all.html|website=InfyTalk|publisher=[[Infosys]]|access-date=December 22, 2015|date=December 14, 2015|url-status=dead|archive-url=https://web.archive.org/web/20151222094518/http://www.infosysblogs.com/infytalk/2015/12/openai_ai_for_all.html|archive-date=December 22, 2015|author1-link=Vishal Sikka}}</ref> Cade Metz of ''[[Wired (magazine)|Wired]]'' suggests that corporations such as [[Amazon.com|Amazon]] may be motivated by a desire to use open-source software and data to level the playing field against corporations such as [[Google]] and [[Facebook]] that own enormous supplies of proprietary data. Altman states that [[Y Combinator (company)|Y Combinator]] companies will share their data with OpenAI.<ref name=wired_far_more/>\n\nIn 2019, OpenAI became a for-profit company called OpenAI LP to secure additional funding while staying controlled by a non-profit called OpenAI Inc in a structure that OpenAI calls \"capped-profit\",<ref name=capped-profit>{{Cite web |url= http://social.techcrunch.com/2019/03/11/openai-shifts-from-nonprofit-to-capped-profit-to-attract-capital/ |title=OpenAI shifts from nonprofit to 'capped-profit' to attract capital |website=TechCrunch |date=March 11, 2019 |language=en-US |access-date=May 10, 2019}}</ref> having previously been a [[501(c)(3) organization|501(c)(3)]] nonprofit organization.<ref name=\"medium\">{{cite web |url=https://medium.com/backchannel/how-elon-musk-and-y-combinator-plan-to-stop-computers-from-taking-over-17e0e27dd02a |title=How Elon Musk and Y Combinator Plan to Stop Computers From Taking Over |first1=Steven |last1=Levy |date=December 11, 2015 |publisher=[[Medium (publishing platform)|Medium]]/[[Backchannel (blog)|Backchannel]] |access-date=December 11, 2015 |quote=\"Elon Musk: ...we came to the conclusion that having a 501(c)(3)... would probably be a good thing to do\"}}</ref><ref>{{Cite web |url= https://twitter.com/gdb/status/848938670878900224 |title=Yes, we're a 501(c)(3). As you mention in /r/ControlProblem, we will file our 990 later this year as required. Not yet sure of exact date. |first=Greg |last=Brockman |date=April 3, 2017}}</ref>{{Primary source inline}}\n\n== Strategy ==\nMusk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" Musk acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; nonetheless, the best defense is \"to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.\"<ref name=\"seattle-investors\">{{cite news|title=Silicon Valley investors to bankroll artificial-intelligence center|url=http://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/|access-date=December 19, 2015|work=[[The Seattle Times]]|date=December 13, 2015}}</ref>\n\nMusk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with [[Existential risk from artificial general intelligence|existential risk from artificial intelligence]]. Philosopher [[Nick Bostrom]] is skeptical of Musk's approach: \"If you have a button that could do bad things to the world, you don't want to give it to everyone.\"<ref name=wired_inside>{{cite magazine|author1=Cade Metz|title=Inside OpenAI, Elon Musk's Wild Plan to Set Artificial Intelligence Free|url=https://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-set-artificial-intelligence-free/|access-date=April 28, 2016|magazine=[[Wired (magazine)|Wired]]|date=April 27, 2016|language=en-US}}</ref> During a 2016 conversation about the technological singularity, Altman said that \"we don't plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated that \"Our goal right now... is to do the best thing there is to do. It's a little vague.\"<ref>{{cite magazine|title=Sam Altman's Manifest Destiny|url=http://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny|access-date=October 4, 2016|magazine=The New Yorker|issue=October 10, 2016}}</ref>\n\nConversely, OpenAI's initial decision to withhold [[GPT-2]] due to a wish to \"err on the side of caution\" in the presence of potential misuse, has been criticized by advocates of openness. Delip Rao, an expert in text generation, stated \"I don't think {{bracket|OpenAI}} spent enough time proving {{bracket|GPT-2}} was actually dangerous.\" Other critics argued that open publication is necessary to replicate the research and to be able to come up with countermeasures.<ref>{{cite news |last1=Vincent |first1=James |title=AI researchers debate the ethics of sharing potentially harmful programs |url=https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harmful-programs-openai |access-date=March 6, 2020 |work=The Verge |date=February 21, 2019 |language=en}}</ref>\n\nIn the 2017 tax year, OpenAI spent US$7.9&nbsp;million, or a quarter of its functional expenses, on cloud computing alone.<ref>{{cite news |title=Microsoft to invest $1&nbsp;billion in OpenAI |url=https://www.reuters.com/article/us-microsoft-openai/microsoft-to-invest-1-billion-in-openai-idUSKCN1UH1H9 |access-date=March 6, 2020 |work=Reuters |date=July 22, 2019 |language=en}}</ref> In comparison, DeepMind's total expenses in 2017 were much larger, measuring US$442&nbsp;million. In Summer 2018, simply training OpenAI's ''Dota 2'' bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks. According to OpenAI, the capped-profit model adopted in March 2019 allows OpenAI LP to legally attract investment from venture funds, and in addition, to grant employees stakes in the company, the goal being that they can say \"I'm going to Open AI, but in the long term it's not going to be disadvantageous to us as a family.\"<ref name=\"wired investors\">{{cite magazine |title=To Compete With Google, OpenAI Seeks Investors\u2013and Profits |url=https://www.wired.com/story/compete-google-openai-seeks-investorsand-profits/ |access-date=March 6, 2020 |magazine=Wired |date=December 3, 2019 |language=en}}</ref> Many top researchers work for [[Google Brain]], DeepMind, or [[Facebook, Inc.]], which offer stock options that a nonprofit would be unable to.<ref name=\"bloomberg arm\">{{cite news |last1=Kahn |first1=Jeremy |title=AI Research Group Co-Founded by Elon Musk Starts For-Profit Arm |url=https://www.bloomberg.com/news/articles/2019-03-11/ai-research-group-co-founded-by-musk-starts-for-profit-arm |access-date=March 6, 2020 |work=[[Bloomberg News]] |date=March 11, 2019}}</ref> In June 2019, OpenAI LP raised a billion dollars from [[Microsoft]], a sum which OpenAI plans to have spent \"within five years, and possibly much faster\".<ref>{{cite news |last1=Murgia |first1=Madhumita |title=DeepMind runs up higher losses and debts in race for AI |url=https://www.ft.com/content/d4280856-b92d-11e9-8a88-aa6628ac896c |access-date=March 6, 2020 |work=[[Financial Times]] |date=August 7, 2019}}</ref> Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve [[artificial general intelligence]].<ref>{{cite news |title=OpenAI Will Need More Capital Than Any Non-Profit Has Ever Raised |url=https://fortune.com/2019/10/03/openai-will-need-more-capital-than-any-non-profit-has-ever-raised/ |access-date=March 6, 2020 |work=Fortune |language=en}}</ref>\n\nThe transition from a nonprofit to a capped-profit company was viewed with skepticism by [[Oren Etzioni]] of the nonprofit [[Allen Institute for AI]], who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can't compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then [[IBM]] would still be number one.\" Following the transition, public disclosure of the compensation of top employees at OpenAI LP is no longer legally required. The nonprofit, OpenAI Inc., is the sole [[controlling interest|controlling shareholder]] of OpenAI LP. OpenAI LP, despite being a for-profit company, retains a formal [[fiduciary duty|fiduciary responsibility]] to OpenAI's Inc.'s nonprofit charter. A majority of OpenAI Inc.'s board is barred from having financial stakes in OpenAI LP.<ref name=\"wired investors\"/> In addition, minority members with a stake in OpenAI LP are barred from certain votes due to [[conflict of interest]].<ref name=\"bloomberg arm\"/> Some researchers have argued that OpenAI LP's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.<ref>{{cite news |last1=Vincent |first1=James |title=Microsoft invests $1&nbsp;billion in OpenAI to pursue holy grail of artificial intelligence |url=https://www.theverge.com/2019/7/22/20703578/microsoft-openai-investment-partnership-1-billion-azure-artificial-general-intelligence-agi |access-date=March 6, 2020 |work=[[The Verge]] |date=July 22, 2019 |language=en}}</ref> A journalist at [[Vice News]] wrote that \"generally, we've never been able to rely on venture capitalists to better humanity\".<ref>{{cite news |last1=Haskins |first1=Caroline |title=OpenAI's Mission to Benefit Humanity Now Includes Seeking Profit |url=https://www.vice.com/en_us/article/kzdyme/openais-mission-to-benefit-humanity-now-includes-seeking-profit |access-date=March 6, 2020 |work=[[Vice News]] |date=March 12, 2019 |language=en}}</ref>\n\n== Products and applications ==\nOpenAI's research tend to focus on [[reinforcement learning]]. OpenAI is viewed as an important competitor to DeepMind.<ref>{{cite news |last1=Lee |first1=Dave |title=Robot solves Rubik's cube, but not grand challenge |url=https://www.bbc.com/news/technology-50064225 |access-date=February 29, 2020 |work=BBC News |date=October 15, 2019}}</ref>\n\n=== Gym ===\nGym aims to provide an easy to set up, general-intelligence [[Benchmark (computing)|benchmark]] with a wide variety of different environments\u2014somewhat akin to, but broader than, the [[ImageNet Large Scale Visual Recognition Challenge]] used in [[supervised learning]] research\u2014and that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible.<ref name=\":0\">{{cite news|author1=Dave Gershgorn|title=Elon Musk's Artificial Intelligence Group Opens A 'Gym' To Train A.I.|url=http://www.popsci.com/elon-musks-artificial-intelligence-group-opens-gym-to-train-ai|access-date=April 29, 2016|work=Popular Science|date=April 27, 2016}}</ref><ref>{{cite web|author1=Greg Brockman|author2=John Schulman|title=OpenAI Gym Beta|url=https://openai.com/blog/openai-gym-beta/|website=OpenAI Blog|publisher=OpenAI|access-date=April 29, 2016|language=en-us|date=April 27, 2016}}</ref> The project claims to provide the user with a simple interface. As of June{{Spaces}}2017, Gym can only be used with [[Python (programming language)|Python]].<ref>{{cite web|title=OpenAI Gym|url=https://gym.openai.com/|website=GitHub|access-date=May 8, 2017}}</ref> As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its [[GitHub]] page.<ref>{{Cite web|url=https://twitter.com/gdb/status/907855318591438848|title=Yep, the Github repo has been the focus of the project for the past year. The Gym site looks cool but hasn't been maintained.|last=Brockman|first=Greg|date= September 12, 2017|website=@gdb|language=en|access-date=November 7, 2017}}</ref>{{Primary source inline}}\n\n=== RoboSumo ===\nIn \"RoboSumo\", virtual humanoid \"[[Meta-learning (computer science)|metalearning]]\" robots initially lack knowledge of how to even walk, and given the goals of learning to move around, and pushing the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.<ref>{{cite magazine|title=AI Sumo Wrestlers Could Make Future Robots More Nimble|url=https://www.wired.com/story/ai-sumo-wrestlers-could-make-future-robots-more-nimble/|access-date=November 2, 2017|magazine=Wired|date=October 11, 2017}}</ref><ref>{{cite news|title=OpenAI's Goofy Sumo-Wrestling Bots Are Smarter Than They Look|url=https://www.technologyreview.com/the-download/609117/openais-goofy-sumo-wrestling-bots-are-smarter-than-they-look/|access-date=November 2, 2017|work=MIT Technology Review|language=en}}</ref> OpenAI's Igor Mordatch argues that competition between agents can create an intelligence \"arms race\" that can increase an agent's ability to function, even outside the context of the competition.\n\n=== Debate Game ===\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing [[explainable AI]].<ref>{{Cite news|url=https://thenextweb.com/artificial-intelligence/2018/05/04/openais-debate-game-teaches-you-and-your-friends-how-to-lie-like-robots/|title=OpenAI's Debate Game teaches you and your friends how to lie like robots|last=Greene|first=Tristan|date=May 4, 2018|work=The Next Web|access-date=May 31, 2018|language=en-US}}</ref><ref>{{cite news |title=Why Scientists Think AI Systems Should Debate Each Other |url=https://www.fastcompany.com/40569116/why-scientists-think-ai-systems-should-debate-each-other |access-date=June 2, 2018 |work=Fast Company |date=May 8, 2018}}</ref>\n\n=== Dactyl ===\nDactyl uses machine learning to train a [[Shadow Hand]], a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same reinforcement learning algorithms and training code as [[#OpenAI Five|OpenAI Five]]. OpenAI tackled the object orientation problem by using [[domain randomization]], a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.<ref>{{cite arXiv |author=OpenAI |last2=Andrychowicz |first2=Marcin |last3=Baker |first3=Bowen |last4=Chociej |first4=Maciek |last5=J\u00f3zefowicz |first5=Rafa\u0142 |last6=McGrew |first6=Bob |last7=Pachocki |first7=Jakub |last8=Petron |first8=Arthur |last9=Plappert |first9=Matthias |last10=Powell |first10=Glenn |last11=Ray |first11=Alex |last12=Schneider |first12=Jonas |last13=Sidor |first13=Szymon |last14=Tobin |first14=Josh |last15=Welinder |first15=Peter |last16=Weng |first16=Lilian |last17=Zaremba |first17=Wojciech |year=2019 |title=Learning Dexterous In-Hand Manipulation |eprint=1808.00177v5 |class=cs.LG}}</ref>\n\nIn 2019, OpenAI demonstrated that Dactyl could solve a [[Rubik's Cube]]. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik's Cube introduce complex physics that is harder to model. OpenAI solved this by improving the robustness of Dactyl to perturbations; they employed a technique called Automatic Domain Randomization (ADR), a simulation approach where progressively more difficult environments are endlessly generated. ADR differs from manual domain randomization by not needing there to be a human to specify randomization ranges.<ref>{{cite arXiv |author1=OpenAI |last2=Akkaya |first2=Ilge |last3=Andrychowicz |first3=Marcin |last4=Chociej |first4=Maciek |last5=Litwin |first5=Mateusz |last6=McGrew |first6=Bob |last7=Petron |first7=Arthur |last8=Paino |first8=Alex |last9=Plappert |first9=Matthias |last10=Powell |first10=Glenn |last11=Ribas |first11=Raphael |year=2019 |title=Solving Rubik's Cube with a Robot Hand |eprint=1910.07113v1 |class=cs.LG}}</ref>\n\n=== Generative models ===\n==== GPT ====\n[[File:Full GPT architecture.png|right|thumb|The GPT model]]\nThe original paper on generative pre-training (GPT) of a language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI's website on June 11, 2018.<ref>{{Cite web|url=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf|title=Improving Language Understanding by Generative Pre-Training|access-date=June 9, 2020}}</ref> It showed how a [[generative model]] of language is able to acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\n\n==== GPT-2 ====\n{{main|GPT-2}}\n[[File:GPT2-talks-about-GPT2.png|right|thumb|An instance of GPT-2 writing a paragraph based on a prompt from its own Wikipedia article in February 2021]]\nGenerative Pre-trained Transformer 2, commonly known by its abbreviated form GPT-2, is an [[Unsupervised learning|unsupervised]] [[Transformer (machine learning model)|transformer]] [[language model]] and the successor to GPT. GPT-2 was first announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released out of concern over potential misuse, including applications for writing [[fake news]].<ref name=\"gpt2-not-immediate-release\">{{Cite news|last = Hern|first = Alex| title = New AI fake text generator may be too dangerous to release, say creators| newspaper = The Guardian| date = February 14, 2019|url = https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction| access-date = February 14, 2019}}</ref> Some experts expressed skepticism that GPT-2 posed a significant threat. The [[Allen Institute for Artificial Intelligence]] responded to GPT-2 with a tool to detect \"neural fake news\".<ref>{{cite news |last1=Schwartz |first1=Oscar |title=Could 'fake text' be the next global political threat? |url=https://www.theguardian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information |access-date=July 16, 2019 |work=The Guardian |date=July 4, 2019}}</ref> Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\".<ref>{{cite news |last1=Vincent |first1=James |title=OpenAI's new multitalented AI writes, translates, and slanders |url=https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2 |access-date=July 16, 2019 |work=The Verge |date=February 14, 2019}}</ref> In November 2019, OpenAI released the complete version of the GPT-2 language model.<ref>{{Cite web|url=https://openai.com/blog/gpt-2-1-5b-release/|title=GPT-2: 1.5B Release|date=November 5, 2019|website=OpenAI|language=en|access-date=November 14, 2019}}</ref> Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.<ref>{{Cite web|url=https://transformer.huggingface.co/|title=Write With Transformer|access-date=December 4, 2019}}</ref><ref>{{Cite web|url=https://talktotransformer.com/|title=Talk to Transformer|access-date=December 4, 2019}}</ref><ref>{{Cite web|url=https://creativeengines.ai/|title=CreativeEngines|access-date=June 25, 2021}}</ref>\n\nGPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and [[perplexity]] on 7 of 8 [[One-shot learning (software)|zero-shot]] tasks (i.e. the model was not further trained on any task-specific input-output examples). The corpus it was trained on, called WebText, contains slightly over 8 million documents for a total of 40 GB of text from URLs shared in [[Reddit]] submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using [[byte pair encoding]]. This allows to represent any string of characters by encoding both individual characters and multiple-character tokens.<ref name=\"gpt2\">{{cite journal |title=Language Models are Unsupervised Multitask Learners |url=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf |access-date=December 4, 2019}}</ref>\n\n==== GPT-3 ====\n{{main|GPT-3}}\nGenerative Pre-trained{{efn|The term \"pre-training\" refers to general language training as distinct from fine-tuning for specific tasks.<ref>{{cite web |title=Pre-trained Language Models: Simplified |url=https://towardsdatascience.com/pre-trained-language-models-simplified-b8ec80c62217 |date=December 17, 2019 |last=Ganesh |first=Prakhar |access-date=September 9, 2020 |quote=\"The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language.\"}}</ref>}} Transformer 3, commonly known by its abbreviated form [[GPT-3]], is an unsupervised transformer language model and the successor to [[#GPT-2|GPT-2]].  It was first described in May 2020.<ref>{{Cite web|title=openai/gpt-3|date=May 29, 2020|url=https://github.com/openai/gpt-3|publisher=OpenAI|access-date=May 29, 2020}}</ref><ref>{{Cite web|last=Sagar|first=Ram|date=June 3, 2020|title=OpenAI Releases GPT-3, The Largest Model So Far|url=https://analyticsindiamag.com/open-ai-gpt-3-language-model/|access-date=June 14, 2020|website=Analytics India Magazine|language=en-US}}</ref><ref name=\"gpt3\"/>  OpenAI stated that full version of GPT-3 contains 175&nbsp;billion [[parameter (machine learning)|parameter]]s,<ref name=\"gpt3\"/> two [[orders of magnitude]] larger than the 1.5&nbsp;billion parameters<ref name=\"gpt2-with-quote\">{{cite journal |title=Language Models are Unsupervised Multitask Learners |url=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf |access-date=December 4, 2019|quote=\"GPT-2, is a 1.5B parameter Transformer\"}}</ref> in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).<ref name=\"gpt3-with-compare-quote\">{{Cite arXiv|last1=Brown|first1=Tom |last2=Mann|first2=Benjamin|last3=Ryder|first3=Nick|last4=Subbiah|first4=Melanie|last5=Kaplan|first5=Jared|last6=Dhariwal|first6=Prafulla|last7=Neelakantan|first7=Arvind|last8=Shyam|first8=Pranav|last9=Sastry|first9=Girish|last10=Askell|first10=Amanda|last11=Agarwal|first11=Sandhini|date=June 1, 2020|title=Language Models are Few-Shot Learners|class=cs.CL |eprint=2005.14165|quote=\"Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3\"}}</ref>\n\nOpenAI stated that GPT-3 succeeds at certain \"[[meta-learning]]\" tasks. It can [[transfer learning|generalize the purpose of a single input-output pair]]. The paper gives an example of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.<ref name=\"gpt3\">{{Cite arXiv|last1=Brown|first1=Tom |last2=Mann|first2=Benjamin|last3=Ryder|first3=Nick|last4=Subbiah|first4=Melanie|last5=Kaplan|first5=Jared|last6=Dhariwal|first6=Prafulla|last7=Neelakantan|first7=Arvind|last8=Shyam|first8=Pranav|last9=Sastry|first9=Girish|last10=Askell|first10=Amanda|last11=Agarwal|first11=Sandhini|date=June 1, 2020|title=Language Models are Few-Shot Learners|eprint=2005.14165|page=appendix|class=cs.CL }}</ref>\n\nGPT-3 dramatically improved benchmark results <!-- describe results --> over GPT-2. OpenAI cautioned that such scaling up of language models could be approaching or encountering the fundamental capability limitations of predictive language models.<ref name=\"zdnet-openai-statement\">{{cite news |last1=Ray |first1=Tiernan |title=OpenAI's gigantic GPT-3 hints at the limits of language models for AI |url=https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/ |access-date=June 5, 2020 |publisher=ZDNet |year=2020 |language=en}}</ref> Pre-training GPT-3 required several thousand petaflop/s-days{{efn|One petaflop/s-day is approximately equal to 10<sup>20</sup> neural net operations.<ref>{{cite web |title=AI and Compute |url=https://openai.com/blog/ai-and-compute/#fn2 |date=May 16, 2018 |last1=Amodei |first1=Dario |last2=Hernandez |first2=Danny |quote=\"A petaflop/s-day (pfs-day) consists of performing 10<sup>15</sup> neural net operations per second for one day, or a total of about 10<sup>20</sup> operations. The compute-time product serves as a mental convenience, similar to kW-hr for energy.\"}}</ref>}} of compute<!--\"compute\" is the correct technical term; do not correct to computations (see {{url|https://openai.com/blog/ai-and-compute/#fn2}})-->, compared to tens of petaflop/s-days for the full GPT-2 model.<ref name=\"gpt3\" /> Like that of its predecessor,<ref name=\"gpt2-not-immediate-release\" /> GPT-3's fully trained model was not immediately released to the public on the grounds of possible abuse, though OpenAI planned to allow access through a paid cloud [[Application programming interface|API]] after a two-month free private beta that began in June 2020.<ref name=\"gpt3-whynotfullmodel\">{{Cite web|date=June 11, 2020|title=OpenAI API|url=https://openai.com/blog/openai-api/|access-date=June 14, 2020|website=OpenAI|language=en|quote=\"Why did OpenAI choose to release an API instead of open-sourcing the models?<br/>There are three main reasons we did this. First, commercializing the technology helps us pay for our ongoing AI research, safety, and policy efforts. Second, many of the models underlying the API are very large, taking a lot of expertise to develop and deploy and making them very expensive to run. This makes it hard for anyone except larger companies to benefit from the underlying technology. We\u2019re hopeful that the API will make powerful AI systems more accessible to smaller businesses and organizations. Third, the API model allows us to more easily respond to misuse of the technology. Since it is hard to predict the downstream use cases of our models, it feels inherently safer to release them via an API and broaden access over time, rather than release an [[open source]] model where access cannot be adjusted if it turns out to have harmful applications.\"\n}}</ref><ref>{{Cite web|last=Eadicicco|first=Lisa|title=The artificial intelligence company that Elon Musk helped found is now selling the text-generation software it previously said was too dangerous to launch|url=https://www.businessinsider.com/elon-musk-openai-sell-text-tool-it-said-was-dangerous-2020-6|access-date=July 6, 2020|website=Business Insider}}</ref>\n\nOn September 23, 2020, GPT-3 was licensed exclusively to Microsoft.<ref>{{Cite web|title=OpenAI is giving Microsoft exclusive access to its GPT-3 language model|url=https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-model/|access-date=September 24, 2020|website=MIT Technology Review|language=en}}</ref><ref>{{Cite web|date=September 22, 2020|title=Microsoft gets exclusive license for OpenAI's GPT-3 language model|url=https://venturebeat.com/2020/09/22/microsoft-gets-exclusive-license-for-openais-gpt-3-language-model/|access-date=September 24, 2020|website=VentureBeat|language=en-US}}</ref>\n\n==== ChatGPT ====\n{{main|ChatGPT}}\n[[ChatGPT]] is an artificial intelligence tool that provides a conversational interface that allows you to ask questions in [[natural language]]. The system then responds with an answer within seconds. ChatGPT was launched in November 2022 and reached 1 million users only 5 days after its initial launch.<ref>{{cite web |title=Mira Murati via Twitter |date=Dec 5, 2022 |url=https://twitter.com/miramurati/status/1599796191243669504 |publisher=Mira Murati |access-date=December 15, 2022}}</ref>\n\n==== Music ====\nOpenAI's MuseNet (2019) is a deep neural net trained to predict subsequent musical notes in [[MIDI]] music files. It can generate songs with ten different instruments in fifteen different styles. According to ''[[The Verge]]'', a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays.<ref>{{cite news |title=OpenAI's MuseNet generates AI music at the push of a button |url=https://www.theverge.com/2019/4/26/18517803/openai-musenet-artificial-intelligence-ai-music-generation-lady-gaga-harry-potter-mozart |access-date=June 8, 2020 |work=The Verge |date=April 2019}}</ref><ref>{{cite web |title=MuseNet |date=April 25, 2019 |url=https://openai.com/blog/musenet/ |publisher=OpenAI |access-date=June 8, 2020}}</ref>\n\nOpenAI's Jukebox (2020) is an open-sourced algorithm to [[computer music|generate music]] with vocals. After training on 1.2&nbsp;million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence, follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. ''The Verge'' stated \"It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while ''[[Business Insider]]'' stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".<ref>{{cite news |title=OpenAI introduces Jukebox, a new AI model that generates genre-specific music |url=https://www.theverge.com/2020/4/30/21243038/openai-jukebox-model-raw-audio-lyrics-ai-generated-copyright |access-date=June 8, 2020 |work=The Verge |date=April 30, 2020}}</ref><ref>{{cite news |last1=Stephen |first1=Bijan |title=OpenAI introduces Jukebox, a new AI model that generates genre-specific music |url=https://www.businessinsider.com/jukebox-ai-music-generator-realistic-songs-machine-learning-algorithm-deepfakes-2020-5 |access-date=June 8, 2020 |work=Business Insider |date=April 30, 2020 |language=en}}</ref><ref>{{cite web |title=Jukebox |date=April 30, 2020 |url=https://openai.com/blog/jukebox/ |publisher=OpenAI |access-date=June 8, 2020}}</ref>\n\n==== API ====\nIn June 2020, OpenAI announced a multi-purpose [[Application programming interface|API]] which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task.\"<ref name=\"gpt3-whynotfullmodel\" /><ref name=\"tech_Tech\">{{Cite web |title=TechCrunch Startup and Technology News |work=TechCrunch |date=June 11, 2020 |access-date=June 11, 2020 |url= https://techcrunch.com/2020/06/11/openai-makes-an-all-purpose-api-for-its-text-based-ai-capabilities/ |quote=If you\u2019ve ever wanted to try out OpenAI's vaunted machine learning toolset, it just got a lot easier. The company has released an API that lets developers call its AI tools in on \"virtually any English language task.\" }}</ref>\n\n==== DALL-E and CLIP ====\n{{main|DALL-E}}\n[[File:DALL-E sample.png|thumb|300px|Images produced by DALL-E when given the text prompt \"a professional high-quality illustration of a giraffe dragon chimera. a giraffe imitating a dragon. a giraffe made of dragon.\"]]\nDALL-E is a Transformer model that creates images from textual descriptions, revealed by OpenAI in January 2021.<ref>{{cite web|url=https://openai.com/blog/dall-e/|title=DALL\u00b7E: Creating Images from Text|date=January 5, 2021}}</ref>\n\nCLIP does the opposite: it creates a description for a given image.<ref>{{cite web |title=CLIP: Connecting Text and Images |date=January 5, 2021|url=https://openai.com/blog/clip/}}</ref> DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\n\nIn March 2021, OpenAI released a paper titled ''Multimodal Neurons in Artificial Neural Networks'',<ref>{{cite web |title=Multimodal Neurons in Artificial Neural Networks |date=March 4, 2021|url=https://openai.com/blog/multimodal-neurons/}}</ref> where they showed a detailed analysis of CLIP (and GPT) models and their vulnerabilities. The new type of attacks on such models was described in this work.\n\n{{Blockquote\n| quote = We refer to these attacks as typographic attacks. We believe attacks such as those described above are far from simply an academic concern. By exploiting the model's ability to read text robustly, we find that even photographs of hand-written text can often fool the model. \n| author = ''Multimodal Neurons in Artificial Neural Networks'', OpenAI\n}}\n\nIn April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results.<ref>{{cite web |title=DALL\u00b7E 2 |url=https://openai.com/dall-e-2/ |access-date=April 6, 2022 |website=OpenAI |language=en}}</ref> In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.<ref>{{cite news |title=ChatGPT: A scientist explains the hidden genius and pitfalls of OpenAI's chatbot |url=https://www.sciencefocus.com/news/chatgpt-scientist-openai-chatbot/ |access-date=30 December 2022 |work=BBC Science Focus Magazine |date=2022 |language=en}}</ref>\n\n==== Microscope ====\nOpenAI Microscope<ref>{{cite web |title=OpenAI Microscope |date=April 14, 2020|url=https://openai.com/blog/microscope/}}</ref> is a collection of visualizations of every significant layer and neuron of eight different neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily.\nThe models included are [[AlexNet]], VGG 19, different versions of [[Inceptionv3|Inception]], and different versions of CLIP [[Residual neural network|Resnet]].<ref>{{Cite web|url=https://microscope.openai.com/models|title=OpenAI Microscope|website=OpenAI Microscope}}</ref>\n\n==== Codex ====\n{{main article|OpenAI Codex}}\n[[OpenAI Codex]] is a descendant of GPT-3 that has additionally been trained on code from 54 million [[GitHub]] repositories.<ref name=\"OAI-Codex\">{{Cite news|last=Alford|first=Anthony|date=August 31, 2021|title=OpenAI Announces 12 Billion Parameter Code-Generation AI Codex|work=InfoQ|url=https://www.infoq.com/news/2021/08/openai-codex/|access-date=September 3, 2021}}</ref><ref name=\"VB-Codex\">{{Cite news|last=Wiggers|first=Kyle|date=July 8, 2021|title=OpenAI warns AI behind GitHub's Copilot may be susceptible to bias|work=[[VentureBeat]]|url=https://venturebeat.com/2021/07/08/openai-warns-ai-behind-githubs-copilot-may-be-susceptible-to-bias/|access-date=September 3, 2021}}</ref> It was announced in mid-2021 as the AI powering the code [[autocompletion]] tool [[GitHub Copilot]].<ref name=\"VB-Codex\" /> In August 2021, an API was released in private beta.<ref>{{cite web|last=Zaremba|first=Wojciech|author-link=Wojciech Zaremba|date=August 10, 2021|title=OpenAI Codex|url=https://openai.com/blog/openai-codex/|url-status=live|access-date=September 3, 2021|website=OpenAI}}</ref> According to OpenAI, the model is able to create working code in over a dozen programming languages, most effectively in Python.<ref name=\"OAI-Codex\" />\n\nSeveral issues with glitches, design flaws, and security vulnerabilities have been brought up.<ref>{{Cite news|last=Dickson|first=Ben|date=August 16, 2021|title=What to expect from OpenAI's Codex API|work=[[VentureBeat]]|url=https://venturebeat.com/2021/08/16/what-to-expect-from-openais-codex-api/|access-date=September 3, 2021}}</ref><ref>{{Cite news|last=Claburn|first=Thomas|date=August 25, 2021|title=GitHub's Copilot may steer you into dangerous waters about 40% of the time \u2013 study|work=[[The Register]]|url=https://www.theregister.com/2021/08/25/github_copilot_study/|access-date=September 3, 2021}}</ref>\n\n=== Video game bots and benchmarks ===\n\n==== OpenAI Five ====\n{{Main|OpenAI Five}}\n[[OpenAI Five]] is the name of a team of five OpenAI-curated [[video game bot|bots]] that are used in the competitive five-on-five video game ''[[Dota 2]]'', who learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at [[The International 2017]], the annual premiere championship tournament for the game, where [[Dendi (Dota player)|Dendi]], a professional Ukrainian player, lost against a bot in a live [[1v1]] matchup.<ref>{{cite web |last1=Savov |first1=Vlad |title=My favorite game has been invaded by killer AI bots and Elon Musk hype |url=https://www.theverge.com/2017/8/14/16141938/dota-2-openai-bots-elon-musk-artificial-intelligence |website=The Verge |date=August 14, 2017 |access-date=June 25, 2018}}</ref><ref>{{cite web|last1=Frank|first1=Blair Hanley|title=OpenAI's bot beats top Dota 2 player so badly that he quits|url=https://venturebeat.com/2017/08/11/openais-bot-beats-top-dota-2-player-so-badly-that-he-quits/|website=Venture Beat|access-date=August 12, 2017|url-status=dead|archive-url=https://web.archive.org/web/20170812065202/https://venturebeat.com/2017/08/11/openais-bot-beats-top-dota-2-player-so-badly-that-he-quits/|archive-date=August 12, 2017}}</ref> After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of [[Elapsed real time|real time]], and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon.<ref>{{cite web|title=Dota 2|url=https://blog.openai.com/dota-2/|website=blog.openai.com|date=August 11, 2017|access-date=August 12, 2017}}</ref><ref>{{cite web|title=More on Dota 2|url=https://blog.openai.com/more-on-dota-2/|website=blog.openai.com|date=August 16, 2017|access-date=August 16, 2017}}</ref> The system uses a form of [[reinforcement learning]], as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.<ref>{{cite magazine |last1=Simonite |first1=Tom |title=Can Bots Outwit Humans in One of the Biggest Esports Games? |url=https://www.wired.com/story/can-bots-outwit-humans-in-one-of-the-biggest-esports-games/ |magazine=Wired |access-date=June 25, 2018}}</ref><ref>{{cite news |last1=Kahn |first1=Jeremy |title=A Bot Backed by Elon Musk Has Made an AI Breakthrough in Video Game World |newspaper=Bloomberg.com |date=June 25, 2018 |url=https://www.bloomberg.com/news/articles/2018-06-25/musk-backed-bot-conquers-e-gamer-teams-in-ai-breakthrough |publisher=Bloomberg L.P. |access-date=June 27, 2018}}</ref><ref>{{cite web |last1=Clifford |first1=Catherine |title=Bill Gates says gamer bots from Elon Musk-backed nonprofit are 'huge milestone' in A.I. |url=https://www.cnbc.com/2018/06/27/bill-gates-openai-robots-beating-humans-at-dota-2-is-ai-milestone.html |publisher=CNBC |date=June 28, 2018 |access-date=June 29, 2018}}</ref>\n\nBy June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players.<ref>{{cite web |title=OpenAI Five Benchmark |url=https://blog.openai.com/openai-five-benchmark/ |website=blog.openai.com |date=July 18, 2018 |access-date=August 25, 2018}}</ref><ref>{{cite magazine |last1=Simonite |first1=Tom |title=Can Bots Outwit Humans in One of the Biggest Esports Games? |url=https://www.wired.com/story/can-bots-outwit-humans-in-one-of-the-biggest-esports-games/ |magazine=Wired |access-date=June 25, 2018}}</ref><ref>{{cite web |last1=Vincent |first1=James |title=AI bots trained for 180 years a day to beat humans at Dota 2 |url=https://www.theverge.com/2018/6/25/17492918/openai-dota-2-bot-ai-five-5v5-matches |website=The Verge |date=June 25, 2018 |access-date=June 25, 2018}}</ref><ref>{{cite web |last1=Savov |first1=Vlad |title=The OpenAI Dota 2 bots just defeated a team of former pros |url=https://www.theverge.com/2018/8/6/17655086/dota2-openai-bots-professional-gaming-ai |website=The Verge |date=August 6, 2018 |access-date=August 7, 2018}}</ref> At [[The International 2018]], OpenAI Five played in two exhibition matches against professional players, but ended up losing both games.<ref>{{cite magazine |last1=Simonite |first1=Tom |title=Pro Gamers Fend off Elon Musk-Backed AI Bots\u2014for Now |url=https://www.wired.com/story/pro-gamers-fend-off-elon-musks-ai-bots/ |magazine=Wired |access-date=August 25, 2018}}</ref><ref>{{cite web |last1=Quach |first1=Katyanna |title=Game over, machines: Humans defeat OpenAI bots once again at video games Olympics |url=https://www.theregister.co.uk/2018/08/24/openai_bots_eliminated_dota_2/ |website=The Register |access-date=August 25, 2018}}</ref><ref>{{cite web |title=The International 2018: Results |url=https://blog.openai.com/the-international-2018-results/ |website=blog.openai.com |date=August 24, 2018 |access-date=August 25, 2018}}</ref> In April 2019, OpenAI Five defeated [[OG (esports)|OG]], the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.<ref>{{cite web |last1=Statt |first1=Nick |title=OpenAI's Dota 2 AI steamrolls world champion e-sports team with back-to-back victories|url=https://www.theverge.com/2019/4/13/18309459/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion |website=The Verge |date=April 13, 2019 |access-date=July 20, 2019}}</ref><ref>{{cite web |title=How to Train Your OpenAI Five|url=https://openai.com/blog/how-to-train-your-openai-five/ |website=OpenAI Blog |date=April 15, 2019 |access-date=July 20, 2019}}</ref> The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.<ref>{{cite web |last1=Wiggers |first1=Kyle |title=OpenAI's Dota 2 bot defeated 99.4% of players in public matches.|url=https://venturebeat.com/2019/04/22/openais-dota-2-bot-defeated-99-4-of-players-in-public-matches/ |website=Venture Beat |date=April 22, 2019 |access-date=April 22, 2019}}</ref>\n\n==== GYM Retro ====\nGym Retro is a platform for reinforcement learning research on video games. Gym Retro is used to research RL algorithms and study generalization. Prior research in RL has focused chiefly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\n\n== See also ==\n<!-- Please keep entries in alphabetical order & add a short description [[WP:SEEALSO]] -->\n{{div col|colwidth=20em|small=yes}}\n* [[DeepMind]]\n* [[Future of Humanity Institute]]\n* [[Future of Life Institute]]\n* [[Machine Intelligence Research Institute]]\n{{div col end}}\n<!-- please keep entries in alphabetical order -->\n\n== Notes ==\n{{notelist}}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Official website}}\n\n{{Differentiable computing}}\n{{Existential risk from artificial intelligence}}\n{{Elon Musk}}\n\n{{Portal bar|Companies|California}}\n\n{{Authority control}}\n\n{{Coord|37.7623|N|122.4148|W|region:US-CA_type:landmark|display=title}}\n\n{{DEFAULTSORT:OpenAI}}\n[[Category:OpenAI| ]]\n[[Category:2015 establishments in California]]\n[[Category:2015 in San Francisco]]\n[[Category:501(c)(3) organizations]]\n[[Category:American companies established in 2015]]\n[[Category:Artificial intelligence associations]]\n[[Category:Artificial intelligence laboratories]]\n[[Category:Elon Musk]]\n[[Category:Existential risk from artificial general intelligence]]\n[[Category:Existential risk organizations]]\n[[Category:Non-profit organizations based in San Francisco]]\n[[Category:Open-source artificial intelligence]]\n[[Category:Research institutes in the San Francisco Bay Area]]"}