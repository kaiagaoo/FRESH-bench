{"title": "Jan Leike", "page_id": 76964056, "revision_id": 1262783843, "revision_timestamp": "2024-12-13T03:38:43Z", "content": "{{Short description|AI alignment researcher}}\n{{Use dmy dates|date=June 2024}}\n'''Jan Leike''' (born {{birth based on age as of date|36|2023|9|7|noage=1}})<ref name=\":1\"/> is an [[AI alignment]] researcher who has worked at [[DeepMind]] and [[OpenAI]]. He joined [[Anthropic]] in May 2024.\n\n== Education ==\nJan Leike obtained his undergraduate degree from the [[University of Freiburg]] in Germany. After earning a master's degree in [[computer science]], he pursued a PhD in [[machine learning]] at the [[Australian National University]] under the supervision of [[Marcus Hutter]].<ref name=\":0\">{{Cite web |title=An AI safety researcher on how to become an AI safety researcher |url=https://80000hours.org/podcast/episodes/jan-leike-ml-alignment/ |access-date=19 May 2024 |website=80,000 Hours |language=en-US |archive-date=19 May 2024 |archive-url=https://web.archive.org/web/20240519232859/https://80000hours.org/podcast/episodes/jan-leike-ml-alignment/ |url-status=live }}</ref>\n\n== Career ==\nLeike made a six-month postdoctoral fellowship at the [[Future of Humanity Institute]] before joining DeepMind to focus on empirical [[AI safety]] research,<ref name=\":0\" /> where he collaborated with [[Shane Legg]].<ref name=\":1\">{{Cite magazine |date=7 September 2023 |title=TIME100 AI 2023: Jan Leike |url=https://time.com/collection/time100-ai/6310616/jan-leike/ |access-date=19 May 2024 |magazine=Time |language=en |archive-date=19 May 2024 |archive-url=https://web.archive.org/web/20240519023447/https://time.com/collection/time100-ai/6310616/jan-leike/ |url-status=live }}</ref>\n\n=== OpenAI ===\nIn 2021, Leike joined OpenAI.<ref name=\":1\" /> In June 2023, he and [[Ilya Sutskever]] became the co-leaders of the newly introduced \"superalignment\" project, which aimed to determine how to align future [[artificial superintelligence]]s within four years to ensure their safety. This project involved automating AI alignment research using relatively advanced AI systems. At the time, Sutskever was OpenAI's Chief Scientist, and Leike was the Head of Alignment.<ref>{{Cite web |last1=Leike |first1=Jan |last2=Sutskever |first2=Ilya |date=5 July 2023 |title=Introducing Superalignment |url=https://openai.com/index/introducing-superalignment/ |website=OpenAI |access-date=20 May 2024 |archive-date=25 May 2024 |archive-url=https://web.archive.org/web/20240525041645/https://openai.com/index/introducing-superalignment/ |url-status=live }}</ref><ref name=\":1\" /> Leike was featured in Time's list of the 100 most influential personalities in AI, both in 2023<ref name=\":1\" /> and in 2024.<ref>{{Cite web |last=Booth |first=Harry |date=2024-09-05 |title=TIME100 AI 2024: Jan Leike |url=https://time.com/7012867/jan-leike/ |access-date=2024-09-08 |website=TIME |language=en |archive-date=8 September 2024 |archive-url=https://web.archive.org/web/20240908004349/https://time.com/7012867/jan-leike/ |url-status=live }}</ref> In May 2024, Leike announced his resignation from OpenAI, following the departure of Ilya Sutskever, Daniel Kokotajlo and several other AI safety employees from the company. Leike wrote that \"Over the past years, safety culture and processes have taken a backseat to shiny products\", and that he \"gradually lost trust\" in OpenAI's leadership.<ref>{{Cite web |last=Samuel |first=Sigal |date=17 May 2024 |title=\"I lost trust\": Why the OpenAI team in charge of safeguarding humanity imploded |url=https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence |access-date=20 May 2024 |website=Vox |language=en |archive-date=18 May 2024 |archive-url=https://web.archive.org/web/20240518205458/https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence |url-status=live }}</ref><ref>{{Cite web |last=Bastian |first=Matthias |date=18 May 2024 |title=OpenAI's AI safety teams lost at least seven researchers in recent months |url=https://the-decoder.com/openais-ai-safety-teams-lost-at-least-seven-researchers-in-recent-months/ |access-date=20 May 2024 |website=the decoder |language=en-US |archive-date=20 May 2024 |archive-url=https://web.archive.org/web/20240520182733/https://the-decoder.com/openais-ai-safety-teams-lost-at-least-seven-researchers-in-recent-months/ |url-status=live }}</ref><ref>{{Cite news |last=Milmo |first=Dan |date=18 May 2024 |title=OpenAI putting 'shiny products' above safety, says departing researcher |url=https://www.theguardian.com/technology/article/2024/may/18/openai-putting-shiny-products-above-safety-says-departing-researcher |access-date=20 May 2024 |work=The Observer |language=en-GB |issn=0029-7712}}</ref>\n\nIn May 2024, Leike joined [[Anthropic]], an AI company founded by former OpenAI employees.<ref>{{cite web |url=https://www.theverge.com/2024/5/28/24166370/jan-leike-openai-anthropic-ai-safety-research |title=OpenAI researcher who resigned over safety concerns joins Anthropic |date=28 May 2024 |access-date=28 May 2024 |archive-date=28 May 2024 |archive-url=https://web.archive.org/web/20240528183442/https://www.theverge.com/2024/5/28/24166370/jan-leike-openai-anthropic-ai-safety-research |url-status=live }}</ref>\n\n== References ==\n<references />\n\n==External links==\n*[https://jan.leike.name/ Official website]\n*{{IMDb name|nm12101329}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Leike, Jan}}\n\n[[Category:Machine learning researchers]]\n[[Category:Australian National University alumni]]\n[[Category:Living people]]\n[[Category:1980s births]]\n[[Category:21st-century German scientists]]\n[[Category:OpenAI people]]\n[[Category:Anthropic people]]"}