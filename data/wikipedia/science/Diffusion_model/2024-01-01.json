{"title": "Diffusion model", "page_id": 71912239, "revision_id": 1190842257, "revision_timestamp": "2023-12-20T04:36:36Z", "content": "{{Short description|Deep learning algorithm}}\n{{Machine learning|Artificial neural network}}In [[machine learning]], '''diffusion models''', also known as '''diffusion probabilistic models''' or '''score-based generative models''', are a class of [[latent variable model|latent variable]] [[generative model|generative]] models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure.<ref name=\"chang23design\">{{cite arXiv |last1=Chang |first1=Ziyi |last2=Koulieris |first2=George Alex |last3=Shum |first3=Hubert P. H. |title=On the Design Fundamentals of Diffusion Models: A Survey |date=2023 |eprint=2306.04542 |class=cs.LG}}</ref> The goal of diffusion models is to learn a [[diffusion process]] that generates the probability distribution of a given dataset. They learn the latent structure of a dataset by modeling the way in which data points diffuse through their [[latent space]].<ref name=\"song\"/>\n\nIn the case of [[computer vision]], diffusion models can be applied to a variety of tasks, including [[image denoising]], [[inpainting]], [[super-resolution]], and [[text-to-image model|image generation]]. This typically involves training a neural network to sequentially [[denoise]] images blurred with [[Gaussian noise]].<ref name=\"song\">{{Cite arXiv |last1=Song |first1=Yang |last2=Sohl-Dickstein |first2=Jascha |last3=Kingma |first3=Diederik P. |last4=Kumar |first4=Abhishek |last5=Ermon |first5=Stefano |last6=Poole |first6=Ben |date=2021-02-10 |title=Score-Based Generative Modeling through Stochastic Differential Equations |class=cs.LG |eprint=2011.13456 }}</ref><ref name=\"gu\">{{cite arXiv |last1=Gu |first1=Shuyang |last2=Chen |first2=Dong |last3=Bao |first3=Jianmin |last4=Wen |first4=Fang |last5=Zhang |first5=Bo |last6=Chen |first6=Dongdong |last7=Yuan |first7=Lu |last8=Guo |first8=Baining |title=Vector Quantized Diffusion Model for Text-to-Image Synthesis |date=2021 |class=cs.CV |eprint=2111.14822}}</ref> The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise for the network to iteratively denoise. Announced on 13 April 2022, [[OpenAI]]'s text-to-image model [[DALL-E 2]] is an example that uses diffusion models for both the model's prior (which produces an image embedding given a text caption) and the decoder that generates the final image.<ref name=\"dalle2\"/>\n\nDiffusion models are typically formulated as [[markov chain]]s and trained using [[Variational Bayesian methods|variational inference]].<ref name=\"ho\"/> Examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.<ref>{{cite journal |last1= Croitoru |first1=Florinel-Alin |last2= Hondru |first2= Vlad |last3= Ionescu |first3=Radu Tudor |last4= Shah |first4= Mubarak |title=Diffusion Models in Vision: A Survey |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence |date=2023 |volume=45 |issue=9 |pages=10850\u201310869 |doi=10.1109/TPAMI.2023.3261988 |pmid=37030794 |arxiv=2209.04747|s2cid=252199918 }}</ref>\n\n== Denoising diffusion model ==\n\n=== Non-equilibrium thermodynamics ===\nDiffusion models were introduced in 2015 as a method to learn a model that can sample from a highly complex probability distribution. They used techniques from [[non-equilibrium thermodynamics]], especially [[diffusion]].<ref>{{Cite journal |last1=Sohl-Dickstein |first1=Jascha |last2=Weiss |first2=Eric |last3=Maheswaranathan |first3=Niru |last4=Ganguli |first4=Surya |date=2015-06-01 |title=Deep Unsupervised Learning using Nonequilibrium Thermodynamics |url=http://proceedings.mlr.press/v37/sohl-dickstein15.pdf |journal=Proceedings of the 32nd International Conference on Machine Learning |language=en |publisher=PMLR |volume=37 |pages=2256\u20132265}}</ref>\n\nConsider, for example, how one might model the distribution of all naturally-occurring photos. Each image is a point in the space of all images, and the distribution of naturally-occurring photos is a \"cloud\" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a [[Normal distribution|Gaussian distribution]] <math>N(0, I)</math>. A model that can approximately undo the diffusion can then be used to sample from the original distribution. This is studied in \"non-equilibrium\" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.\n\nThe equilibrium distribution is the Gaussian distribution <math>N(0, I)</math>, with pdf <math>\\rho(x) \\propto e^{-\\frac 12 \\|x\\|^2}</math>. This is just the [[Boltzmann distribution]] of particles in a potential well <math>V(x) = \\frac 12 \\|x\\|^2</math> at temperature 1. The initial distribution, being very much out of equilibrium, would diffuse towards the equilibrium distribution, making biased random steps that are a sum of pure randomness (like a [[Brownian motion|Brownian walker]]) and gradient descent down the potential well. The randomness is necessary: if the particles were to undergo only gradient descent, then they will all fall to the origin, collapsing the distribution.\n\n=== Denoising Diffusion Probabilistic Model (DDPM) ===\nThe 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method by [[Variational Bayesian methods|variational inference]].<ref name=\"ho\">{{Cite journal |last1=Ho |first1=Jonathan |last2=Jain |first2=Ajay |last3=Abbeel |first3=Pieter |date=2020 |title=Denoising Diffusion Probabilistic Models |url=https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Curran Associates, Inc. |volume=33 |pages=6840\u20136851}}</ref>\n\n==== Forward diffusion ====\nTo present the model, we need some notation.\n\n* <math>\\beta_1, ..., \\beta_T \\in (0, 1)</math> are fixed constants.\n* <math>\\alpha_t := 1-\\beta_t</math>\n* <math>\\bar \\alpha_t := \\alpha_1 \\cdots \\alpha_t</math>\n* <math>\\tilde \\beta_t := \\frac{1-\\bar \\alpha_{t-1}}{1-\\bar \\alpha_{t}}\\beta_t</math>\n* <math>\\tilde\\mu_t(x_t, x_0) :=\\frac{\\sqrt{\\alpha_{t}}(1-\\bar \\alpha_{t-1})x_t +\\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_{t})x_0}{1-\\bar\\alpha_{t}}</math>\n* <math>N(\\mu, \\Sigma)</math> is the normal distribution with mean <math>\\mu</math> and variance <math>\\Sigma</math>, and <math>N(x | \\mu, \\Sigma)</math> is the probability density at <math>x</math>.\n* A vertical bar denotes [[Conditioning (probability)|conditioning]].\n\nA '''forward diffusion process''' starts at some starting point <math>x_0 \\sim q</math>, where <math>q</math> is the probability distribution to be learned, then repeatedly add noise to it by<math display=\"block\">x_t = \\sqrt{1-\\beta_t} x_{t-1} + \\sqrt{\\beta_t} z_t</math>where <math>z_1, ..., z_T</math> are IID samples from <math>N(0, I)</math>. This is designed so that for any starting distribution of <math>x_0</math>, we have <math>\\lim_t x_t|x_0</math> converging to <math>N(0, I)</math>.\n\nThe entire diffusion process then satisfies<math display=\"block\">q(x_{0:T}) = q(x_0)q(x_1|x_0) \\cdots q(x_T|x_{T-1}) = q(x_0) N(x_1 | \\sqrt{\\alpha_1} x_0, \\beta_1 I) \\cdots N(x_T | \\sqrt{\\alpha_T} x_{T-1}, \\beta_T I)</math>or<math display=\"block\">\\ln q(x_{0:T}) = \\ln q(x_0) - \\sum_{t=1}^T \\frac{1}{2\\beta_t} \\| x_t - \\sqrt{1-\\beta_t}x_{t-1}\\|^2 + C</math>where <math>C</math> is a normalization constant and often omitted. In particular, we note that <math>x_{1:T}|x_0</math> is a [[gaussian process]], which affords us considerable freedom in [[Reparameterization trick|reparameterization]]. For example, by standard manipulation with gaussian process, <math display=\"block\">x_{t}|x_0 \\sim N\\left(\\sqrt{\\bar\\alpha_t} x_{0}, (1-\\bar\\alpha_t) I \\right)</math><math display=\"block\">x_{t-1} | x_t, x_0 \\sim N(\\tilde\\mu_t(x_t, x_0), \\tilde \\beta_t I)</math>In particular, notice that for large <math>t</math>, the variable <math>x_{t}|x_0 \\sim N\\left(\\sqrt{\\bar\\alpha_t} x_{0}, (1-\\bar\\alpha_t) I \\right)</math> converges to <math>N(0, I)</math>. That is, after a long enough diffusion process, we end up with some <math>x_T</math> that is very close to <math>N(0, I)</math>, with all traces of the original <math>x_0 \\sim q</math> gone.\n\nFor example, since<math display=\"block\">x_{t}|x_0 \\sim N\\left(\\sqrt{\\bar\\alpha_t} x_{0}, (1-\\bar\\alpha_t) I \\right)</math>we can sample <math>x_{t}|x_0</math> directly \"in one step\", instead of going through all the intermediate steps <math>x_1, x_2, ..., x_{t-1}</math>.\n\n{{Math proof|title=Derivation by reparameterization|proof=\n\nWe know <math display=\"inline\">x_{t-1}|x_0</math> is a gaussian, and <math display=\"inline\">x_t|x_{t-1}</math> is another gaussian. We also know that these are independent. Thus we can perform a reparameterization: <math display=\"block\">x_{t-1} = \\sqrt{\\bar\\alpha_{t-1}} x_{0} + \\sqrt{1-\\bar\\alpha_{t-1}} z</math> <math display=\"block\">x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1-\\alpha_t} z'</math> where <math display=\"inline\">z, z'</math> are IID gaussians.\n\nThere are 5 variables <math display=\"inline\">x_0, x_{t-1}, x_t, z, z'</math> and two linear equations. The two sources of randomness are <math display=\"inline\">z, z'</math>, which can be reparameterized by rotation, since the IID gaussian distribution is rotationally symmetric.\n\nBy plugging in the equations, we can solve for the first reparameterization: <math display=\"block\">x_t = \\sqrt{\\bar \\alpha_t}x_0 + \\underbrace{\\sqrt{\\alpha_t - \\bar\\alpha_t}z + \\sqrt{1-\\alpha_t}z'}_{= \\sqrt{1-\\bar\\alpha_t} z''}</math> where <math display=\"inline\">z''</math> is a gaussian with mean zero and variance one.\n\nTo find the second one, we complete the rotational matrix: <math display=\"block\">\\begin{bmatrix}z'' \\\\z'''\\end{bmatrix} =\n \\begin{bmatrix} \\frac{\\sqrt{\\alpha_t - \\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}} & \\frac{\\sqrt{\\beta_t}}{\\sqrt{1-\\bar\\alpha_t}} \\\\?&?\\end{bmatrix}\n \\begin{bmatrix} z\\\\z'\\end{bmatrix}</math>\n\nSince rotational matrices are all of the form <math display=\"inline\">\\begin{bmatrix} \\cos\\theta & \\sin\\theta\\\\ -\\sin\\theta & \\cos\\theta \\end{bmatrix}</math>, we know the matrix must be <math display=\"block\">\\begin{bmatrix}z'' \\\\z'''\\end{bmatrix} =\n \\begin{bmatrix} \\frac{\\sqrt{\\alpha_t - \\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}} & \\frac{\\sqrt{\\beta_t}}{\\sqrt{1-\\bar\\alpha_t}} \\\\- \\frac{\\sqrt{\\beta_t}}{\\sqrt{1-\\bar\\alpha_t}} & \\frac{\\sqrt{\\alpha_t - \\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}} \n  \\end{bmatrix}\n \\begin{bmatrix} z\\\\z'\\end{bmatrix}</math> and since the inverse of rotational matrix is its transpose,<br />\n<math display=\"block\">\\begin{bmatrix}z \\\\z'\\end{bmatrix} =\n \\begin{bmatrix} \\frac{\\sqrt{\\alpha_t - \\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}} & -\\frac{\\sqrt{\\beta_t}}{\\sqrt{1-\\bar\\alpha_t}} \\\\ \\frac{\\sqrt{\\beta_t}}{\\sqrt{1-\\bar\\alpha_t}} & \\frac{\\sqrt{\\alpha_t - \\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}} \n  \\end{bmatrix}\n \\begin{bmatrix} z''\\\\z'''\\end{bmatrix}</math>\n\nPlugging back, and simplifying, we have <math display=\"block\">x_t = \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{1-\\bar\\alpha_t}z''</math> <math display=\"block\">x_{t-1} = \\tilde\\mu_t(x_t, x_0) - \\sqrt{\\tilde \\beta_t} z'''</math>\n}}\n\n\n==== Backward diffusion ====\nThe key idea of DDPM is to use a neural network parametrized by <math>\\theta</math>. The network takes in two arguments <math>x_t, t</math>, and outputs a vector <math>\\mu_\\theta(x_t, t)</math> and a matrix <math>\\Sigma_\\theta(x_t, t)</math>, such that each step in the forward diffusion process can be approximately undone by <math>x_{t-1} \\sim N(\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))</math>. This then gives us a backward diffusion process <math>p_\\theta</math> defined by<math display=\"block\">p_\\theta(x_T) = N(x_T | 0, I)</math><math display=\"block\">p_\\theta(x_{t-1} | x_t) = N(x_{t-1} | \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))</math>The goal now is to learn the parameters such that <math>p_\\theta(x_0)</math> is as close to <math>q(x_0)</math> as possible. To do that, we use [[maximum likelihood estimation]] with variational inference.\n\n==== Variational inference ====\nThe [[Evidence lower bound|ELBO inequality]] states that <math>\\ln p_\\theta(x_0) \\geq E_{x_{1:T}\\sim q(\\cdot | x_0)}[ \\ln p_\\theta(x_{0:T}) - \\ln q(x_{1:T}|x_0)] </math>, and taking one more expectation, we get<math display=\"block\">E_{x_0 \\sim q}[\\ln p_\\theta(x_0)] \\geq E_{x_{0:T}\\sim q}[ \\ln p_\\theta(x_{0:T}) - \\ln q(x_{1:T}|x_0)] </math>We see that maximizing the quantity on the right would give us a lower bound on the likelihood of observed data. This allows us to perform variational inference.\n\nDefine the loss function<math display=\"block\">L(\\theta) := -E_{x_{0:T}\\sim q}[ \\ln p_\\theta(x_{0:T}) - \\ln q(x_{1:T}|x_0)]</math>and now the goal is to minimize the loss by stochastic gradient descent. The expression may be simplified to<ref>{{Cite web |last=Weng |first=Lilian |date=2021-07-11 |title=What are Diffusion Models? |url=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ |access-date=2023-09-24 |website=lilianweng.github.io |language=en}}</ref><math display=\"block\">L(\\theta) = \\sum_{t=1}^T E_{x_{t-1}, x_t\\sim q}[-\\ln p_\\theta(x_{t-1} | x_t)] + E_{x_0 \\sim q}[D_{KL}(q(x_T|x_0) \\| p_\\theta(x_T))] + C</math>where <math>C</math> does not depend on the parameter, and thus can be ignored. Since <math>p_\\theta(x_T) = N(x_T | 0, I)</math> also does not depend on the parameter, the term <math>E_{x_0 \\sim q}[D_{KL}(q(x_T|x_0) \\| p_\\theta(x_T))]</math> can also be ignored. This leaves just <math>L(\\theta ) = \\sum_{t=1}^T L_t</math> with <math>L_t =  E_{x_{t-1}, x_t\\sim q}[-\\ln p_\\theta(x_{t-1} | x_t)]</math> to be minimized.\n\n==== Noise prediction network ====\nSince <math>x_{t-1} | x_t, x_0 \\sim N(\\tilde\\mu_t(x_t, x_0), \\tilde \\beta_t I)</math>, this suggests that we should use <math>\\mu_\\theta(x_t, t) = \\tilde \\mu_t(x_t, x_0)</math>; however, the network does not have access to <math>x_0</math>, and so it has to estimate it instead. Now, since <math>x_{t}|x_0 \\sim N\\left(\\sqrt{\\bar\\alpha_t} x_{0}, (1-\\bar\\alpha_t) I \\right)</math>, we may write <math>x_t = \\sqrt{\\bar\\alpha_t} x_{0} + \\sqrt{1-\\bar\\alpha_t} z</math>, where <math>z</math> is some unknown gaussian noise. Now we see that estimating <math>x_0</math> is equivalent to estimating <math>z</math>.\n\nTherefore, let the network output a noise vector <math>\\epsilon_\\theta(x_t, t)</math>, and let it predict<math display=\"block\">\\mu_\\theta(x_t, t) =\\tilde\\mu_t\\left(x_t, \\frac{x_t - \\sqrt{1-\\bar\\alpha_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar\\alpha_t}}\\right) = \\frac{x_t - \\epsilon_\\theta(x_t, t)  \\beta_t/\\sqrt{1-\\bar\\alpha_t}}{\\sqrt{\\alpha_t}}</math>It remains to design <math>\\Sigma_\\theta(x_t, t)</math>. The DDPM paper suggested not learning it (since it resulted in \"unstable training and poorer sample quality\"), but fixing it at some value <math>\\Sigma_\\theta(x_t, t) = \\sigma_t^2 I</math>, where either <math>\\sigma_t^2 = \\beta_t \\text{ or } \\tilde \\beta_t</math> yielded similar performance.\n\nWith this, the loss simplifies to <math display=\"block\">L_t = \\frac{\\beta_t^2}{2\\alpha_t(1-\\bar\\alpha_t)\\sigma_t^2} E_{x_0\\sim q; z \\sim N(0, I)}\\left[ \\left\\| \\epsilon_\\theta(x_t, t) - z \\right\\|^2\\right] + C</math>which may be minimized by stochastic gradient descent. The paper noted empirically that an even simpler loss function<math display=\"block\">L_{simple, t} = E_{x_0\\sim q; z \\sim N(0, I)}\\left[ \\left\\| \\epsilon_\\theta(x_t, t) - z \\right\\|^2\\right]</math>resulted in better models.\n\n== Score-based generative model ==\nScore-based generative model is another formulation of diffusion modelling. They are also called noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD).<ref>{{Cite web |title=Generative Modeling by Estimating Gradients of the Data Distribution {{!}} Yang Song |url=https://yang-song.net/blog/2021/score/ |access-date=2023-09-24 |website=yang-song.net}}</ref><ref name=\":1\">{{Cite arXiv |eprint=2011.13456 |class=cs.LG |first1=Yang |last1=Song |first2=Jascha |last2=Sohl-Dickstein |title=Score-Based Generative Modeling through Stochastic Differential Equations |date=2021-02-10 |last3=Kingma |first3=Diederik P. |last4=Kumar |first4=Abhishek |last5=Ermon |first5=Stefano |last6=Poole |first6=Ben}}</ref>\n\n=== Score matching ===\n\n==== The idea of score functions ====\nConsider the problem of image generation. Let <math>x</math> represent an image, and let <math>q(x)</math> be the probability distribution over all possible images. If we have <math>q(x)</math> itself, then we can say for certain how likely a certain image is. However, this is intractable in general.\n\nMost often, we are uninterested in knowing the absolute probability of a certain image. Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors \u2014 e.g. how much more likely is an image of cat compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some Gaussian noise added?\n\nConsequently, we are actually quite uninterested in <math>q(x)</math> itself, but rather, <math>\\nabla_x \\ln q(x)</math>. This has two major effects:\n* One, we no longer need to normalize <math>q(x)</math>, but can use any <math>\\tilde q(x) = Cq(x)</math>, where <math>C = \\int \\tilde q(x) dx > 0</math> is any unknown constant that is of no concern to us.\n* Two, we are comparing <math>q(x)</math> neighbors <math>q(x + dx)</math>, by <math>\\frac{q(x)}{q(x+dx)} =e^{-\\langle \\nabla_x \\ln p, dx \\rangle}</math>\n\nLet the [[Score (statistics)|score function]] be <math>s(x) := \\nabla_x \\ln q(x)</math>; then consider what we can do with <math>s(x)</math>.\n\nAs it turns out, <math>s(x)</math> allows us to sample from <math>q(x)</math> using thermodynamics. Specifically, if we have a potential energy function <math>U(x) = -\\ln q(x)</math>, and a lot of particles in the potential well, then the distribution at thermodynamic equilibrium is the [[Boltzmann distribution]] <math>q_U(x) \\propto e^{-U(x)/k_B T} = q(x)^{1/k_BT}</math>. At temperature <math>k_BT=1</math>, the Boltzmann distribution is exactly <math>q(x)</math>.\n\nTherefore, to model <math>q(x)</math>, we may start with a particle sampled at any convenient distribution (such as the standard gaussian distribution), then simulate the motion of the particle forwards according to the [[Langevin equation]]<math display=\"block\">dx_{t}= -\\nabla_{x_t}U(x_t) d t+d W_t</math>and the Boltzmann distribution is, [[Fokker\u2013Planck equation#Boltzmann distribution at the thermodynamic equilibrium|by Fokker-Planck equation, the unique thermodynamic equilibrium]]. So no matter what distribution <math>x_0</math> has, the distribution of <math>x_t</math> converges in distribution to <math>q</math> as <math>t\\to \\infty</math>.\n\n==== Learning the score function ====\nGiven a density <math>q</math>, we wish to learn a score function approximation <math>f_\\theta \\approx \\nabla \\ln q</math>. This is '''score matching'''''.''<ref>{{Cite web |title=Sliced Score Matching: A Scalable Approach to Density and Score Estimation {{!}} Yang Song |url=https://yang-song.net/blog/2019/ssm/ |access-date=2023-09-24 |website=yang-song.net}}</ref> Typically, score matching is formalized as minimizing '''Fisher divergence''' function <math>E_q[\\|f_\\theta(x) - \\nabla \\ln q(x)\\|^2]</math>. By expanding the integral, and performing an integration by parts, <math display=\"block\">E_q[\\|f_\\theta(x) - \\nabla \\ln q(x)\\|^2] = E_q[\\|f_\\theta\\|^2 + 2\\nabla^2\\cdot f_\\theta] + C</math>giving us a loss function that can be minimized by stochastic gradient descent.\n\n==== Annealing the score function ====\nSuppose we need to model the distribution of images, and we want <math>x_0 \\sim N(0, I)</math>, a white-noise image. Now, most white-noise images do not look like real images, so <math>q(x_0) \\approx 0</math> for large swaths of <math>x_0 \\sim N(0, I)</math>. This presents a problem for learning the score function, because if there are no samples around a certain point, then we can't learn the score function at that point. If we do not know the score function <math>\\nabla_{x_t}\\ln q(x_t)</math> at that point, then we cannot impose the time-evolution equation on a particle:<math display=\"block\">dx_{t}= \\nabla_{x_t}\\ln q(x_t) d t+d W_t</math>To deal with this problem, we perform [[Simulated annealing|annealing]]. If <math>q</math> is too different from a white-noise distribution, then progressively add noise until it is indistinguishable from one. That is, we perform a forward diffusion, then learn the score function, then use the score function to perform a backward diffusion.\n\n=== Continuous diffusion processes ===\n\n==== Forward diffusion process ====\nConsider again the forward diffusion process, but this time in continuous time:<math display=\"block\">x_t = \\sqrt{1-\\beta_t} x_{t-1} + \\sqrt{\\beta_t} z_t</math>By taking the <math>\\beta_t \\to \\beta(t)dt, \\sqrt{dt}z_t \\to dW_t</math> limit, we obtain a continuous diffusion process, in the form of a [[stochastic differential equation]]:<math display=\"block\">dx_t = -\\frac 12 \\beta(t) x_t dt + \\sqrt{\\beta(t)} dW_t</math>where <math>W_t</math> is a [[Wiener process]] (multidimensional Brownian motion).\n\nNow, the equation is exactly a special case of the [[Brownian dynamics|overdamped Langevin equation]]<math display=\"block\">dx_t = -\\frac{D}{k_BT} (\\nabla_x U)dt + \\sqrt{2D}dW_t</math>where <math>D</math> is diffusion tensor, <math>T</math> is temperature, and <math>U</math> is potential energy field. If we substitute in <math>D= \\frac 12 \\beta(t)I, k_BT = 1, U = \\frac 12 \\|x\\|^2</math>, we recover the above equation. This explains why the phrase \"Langevin dynamics\" is sometimes used in diffusion models.\n\nNow the above equation is for the stochastic motion of a single particle. Suppose we have a cloud of particles distributed according to <math>q</math> at time <math>t=0</math>, then after a long time, the cloud of particles would settle into the stable distribution of <math>N(0, I)</math>. Let <math>\\rho_t</math> be the density of the cloud of particles at time <math>t</math>, then we have<math display=\"block\">\\rho_0 = q; \\quad \\rho_T \\approx N(0, I)</math>and the goal is to somehow reverse the process, so that we can start at the end and diffuse back to the beginning.\n\nBy [[Fokker\u2013Planck equation|Fokker-Planck equation]], the density of the cloud evolves according to<math display=\"block\">\\partial_t \\ln \\rho_t = \\frac 12 \\beta(t) \\left(\nn + (x+ \\nabla\\ln\\rho_t) \\cdot \\nabla \\ln\\rho_t + \\Delta\\ln\\rho_t\n\\right)</math>where <math>n</math> is the dimension of space, and <math>\\Delta</math> is the [[Laplace operator]].\n\n==== Backward diffusion process ====\nIf we have solved <math>\\rho_t</math> for time <math>t\\in [0, T]</math>, then we can exactly reverse the evolution of the cloud. Suppose we start with another cloud of particles with density <math>\\nu_0 = \\rho_T</math>, and let the particles in the cloud evolve according to<math display=\"block\">dy_t =  \\frac{1}{2} \\beta(T-t) y_{t} d t + \\beta(T-t) \\underbrace{\\nabla_{y_{t}} \\ln \\rho_{T-t}\\left(y_{t}\\right)}_{\\text {score function }} d t+\\sqrt{\\beta(T-t)} d W_t</math>then by plugging into the Fokker-Planck equation, we find that <math>\\partial_t \\rho_{T-t} = \\partial_t \\nu_t</math>. Thus this cloud of points is the original cloud, evolving backwards.<ref>{{Cite journal |last=Anderson |first=Brian D.O. |date=May 1982 |title=Reverse-time diffusion equation models |url=http://dx.doi.org/10.1016/0304-4149(82)90051-5 |journal=Stochastic Processes and Their Applications |volume=12 |issue=3 |pages=313\u2013326 |doi=10.1016/0304-4149(82)90051-5 |issn=0304-4149}}</ref>\n\n=== Noise conditional score network (NCSN) ===\nAt the continuous limit, <math display=\"block\">\\bar \\alpha_t = (1-\\beta_1) \\cdots (1-\\beta_t) = e^{\\sum_i \\ln(1-\\beta_i)} \\to e^{-\\int_0^t \\beta(t)dt} </math>and so <math display=\"block\">x_{t}|x_0 \\sim N\\left(e^{-\\frac 12\\int_0^t \\beta(t)dt} x_{0}, \\left(1- e^{-\\int_0^t \\beta(t)dt}\\right) I \\right)</math>In particular, we see that we can directly sample from any point in the continuous diffusion process without going through the intermediate steps, by first sampling <math>x_0 \\sim q, z \\sim N(0, I)</math>, then get <math>x_t = e^{-\\frac 12\\int_0^t \\beta(t)dt} x_{0} + \\left(1- e^{-\\int_0^t \\beta(t)dt}\\right) z</math>. That is, we can quickly sample <math>x_t \\sim \\rho_t</math> for any <math>t \\geq 0</math>.\n\nNow, define a certain probability distribution <math>\\gamma</math> over <math>[0, \\infty)</math>, then the score-matching loss function is defined as the expected Fisher divergence:<math display=\"block\">L(\\theta) = E_{t\\sim \\gamma, x_t \\sim \\rho_t}[\\|f_\\theta(x_t, t)\\|^2 + 2\\nabla\\cdot f_\\theta(x_t, t)]</math>After training, <math>f_\\theta(x_t, t) \\approx \\nabla \\ln\\rho_t</math>, so we can perform the backwards diffusion process by first sampling <math>x_T \\sim N(0, I)</math>, then integrating the SDE from <math>t=T</math> to <math>t=0</math>:<math display=\"block\">x_{t-dt}=x_t + \\frac{1}{2} \\beta(t) x_{t} d t + \\beta(t) f_\\theta(x_t, t) d t+\\sqrt{\\beta(t)} d W_t</math>This may be done by any SDE integration method, such as [[Euler\u2013Maruyama method]].\n\nThe name \"noise conditional score network\" is explained thus:\n\n* \"network\", because <math>f_\\theta</math> is implemented as a neural network.\n* \"score\", because the output of the network is interpreted as approximating the score function <math>\\nabla\\ln\\rho_t</math>.\n* \"noise conditional\", because <math>\\rho_t</math> is equal to <math>\\rho_0</math> blurred by an added gaussian noise that increases with time, and so the score function depends on the amount of noise added.\n\n== Their equivalence ==\nDDPM and score-based generative model are equivalent.<ref>{{Cite journal |last=Luo |first=Calvin |date=2022 |title=Understanding Diffusion Models: A Unified Perspective |arxiv=2208.11970}}</ref> This means that a network trained using DDPM can be used as a NCSN, and vice versa.\n\nWe know that <math>x_{t}|x_0 \\sim N\\left(\\sqrt{\\bar\\alpha_t} x_{0}, (1-\\bar\\alpha_t) I\\right)</math>, so by [[Maurice Tweedie#Tweedie's formula|Tweedie's formula]], we have<math display=\"block\">\\nabla_{x_t}\\ln q(x_t) = \\frac{1}{1-\\bar\\alpha_t}(-x_t + \\sqrt{\\bar\\alpha_t} E_q[x_0|x_t])</math>As described previously, the DDPM loss function is <math>\\sum_t L_{simple, t}</math> with<math display=\"block\">L_{simple, t} = E_{x_0\\sim q; z \\sim N(0, I)}\\left[ \\left\\| \\epsilon_\\theta(x_t, t) - z \\right\\|^2\\right]</math>where <math>x_t =\\sqrt{\\bar\\alpha_t} x_{0} + \\sqrt{1-\\bar\\alpha_t}z\n </math>. By a change of variables,<math display=\"block\">L_{simple, t} = E_{x_0, x_t\\sim q}\\left[ \\left\\| \\epsilon_\\theta(x_t, t) - \n\\frac{x_t -\\sqrt{\\bar\\alpha_t} x_{0}}{\\sqrt{1-\\bar\\alpha_t}} \\right\\|^2\\right] = E_{x_t\\sim q, x_0\\sim q(\\cdot | x_t)}\\left[ \\left\\| \\epsilon_\\theta(x_t, t) - \n\\frac{x_t -\\sqrt{\\bar\\alpha_t} x_{0}}{\\sqrt{1-\\bar\\alpha_t}} \\right\\|^2\\right]</math>and the term inside becomes a least squares regression, so if the network actually reaches the global minimum of loss, then we have <math>\\epsilon_\\theta(x_t, t) = \\frac{x_t -\\sqrt{\\bar\\alpha_t} E_q[x_0|x_t]}{\\sqrt{1-\\bar\\alpha_t}} = -\\sqrt{1-\\bar\\alpha_t}\\nabla_{x_t}\\ln q(x_t)</math>.\n\nNow, the continuous limit <math>x_{t-1} = x_{t-dt}, \\beta_t = \\beta(t) dt, z_t\\sqrt{dt} = dW_t</math> of the backward equation<math display=\"block\">x_{t-1} = \\frac{x_t}{\\sqrt{\\alpha_t}}- \\frac{ \\beta_t}{\\sqrt{\\alpha_t (1-\\bar\\alpha_t)}} \\epsilon_\\theta(x_t, t) + \\sqrt{\\beta_t} z_t; \\quad z_t \\sim N(0, I)</math>gives us precisely the same equation as score-based diffusion:<math display=\"block\">x_{t-dt} = x_t(1+\\beta(t)dt / 2) + \\beta(t) \\nabla_{x_t}\\ln q(x_t) dt + \\sqrt{\\beta(t)}dW_t</math>\n\n== Main variants ==\n\n=== Denoising Diffusion Implicit Model (DDIM) ===\nThe original DDPM method for generating images is slow, since the forward diffusion process usually takes <math>T \\sim 1000</math> to make the distribution of <math>x_T</math> to appear close to gaussian. However this means the backward diffusion process also take 1000 steps. Unlike the forward diffusion process, which can skip steps as <math>x_t | x_0</math> is gaussian for all <math>t \\geq 1</math>, the backward diffusion process does not allow skipping steps. For example, to sample <math>x_{t-2}|x_{t-1} \\sim N(\\mu_\\theta(x_{t-1}, t-1), \\Sigma_\\theta(x_{t-1}, t-1))</math> requires the model to first sample <math>x_{t-1}</math>. Attempting to directly sample <math>x_{t-2}|x_t</math> would require us to marginalize out <math>x_{t-1}</math>, which is generally intractable.\n\nDDIM<ref>{{Cite journal |last1=Song |first1=Jiaming |last2=Meng |first2=Chenlin |last3=Ermon |first3=Stefano |date=2020 |title=Denoising Diffusion Implicit Models |arxiv=2010.02502}}</ref> is a method to take any model trained on DDPM loss, and use it to sample with some steps skipped, sacrificing an adjustable amount of quality. The original DDPM is a special case of DDIM.\n\n=== Latent diffusion model (LDM) ===\nSince the diffusion model is a general method for modelling probability distributions, if one wants to model a distribution over images, one can first encode the images into a lower-dimensional space by an encoder, then use a diffusion model to model the distribution over encoded images. Then to generate an image, one can sample from the diffusion model, then use a decoder to decode it into an image.<ref name=\":2\">{{Cite journal |last1=Rombach |first1=Robin |last2=Blattmann |first2=Andreas |last3=Lorenz |first3=Dominik |last4=Esser |first4=Patrick |last5=Ommer |first5=Bj\u00f6rn |date=2022 |title=High-Resolution Image Synthesis With Latent Diffusion Models |url=https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html |language=en |pages=10684\u201310695|arxiv=2112.10752 }}</ref>\n\nThe encoder-decoder pair is most often a [[variational autoencoder]] (VAE).\n\n=== Classifier guidance ===\nSuppose we wish to sample not from the entire distribution of images, but conditional on the image description. We don't want to sample a generic image, but an image that fits the description \"black cat with red eyes\". Generally, we want to sample from the distribution <math>p(x|y)</math>, where <math>x</math> ranges over images, and <math>y</math> ranges over classes of images (a description \"black cat with red eyes\" is just a very detailed class, and a class \"cat\" is just a very vague description).\n\nTaking the perspective of the [[noisy channel model]], we can understand the process as follows: To generate an image <math>x</math> conditional on description <math>y</math>, we imagine that the requester really had in mind an image <math>x</math>, but the image is passed through a noisy channel and came out garbled, as <math>y</math>. Image generation is then nothing but inferring which <math>x</math> the requester had in mind.\n\nIn other words, conditional image generation is simply \"translating from a textual language into a pictorial language\". Then, as in noisy-channel model, we use Bayes theorem to get<math display=\"block\">p(x|y) \\propto p(y|x)p(x) </math>in other words, if we have a good model of the space of all images, and a good image-to-class translator, we get a class-to-image translator \"for free\". In the equation for backward diffusion, the score <math>\\nabla \\ln p(x) </math> can be replaced by<math display=\"block\">\\nabla_x \\ln p(x|y) = \\nabla_x \\ln p(y|x) + \\nabla_x \\ln p(x) </math>where <math>\\nabla_x \\ln p(x)</math> is the score function, trained as previously described, and <math>\\nabla_x \\ln p(y|x)</math> is found by using a differentiable image classifier.\n\n=== With temperature ===\nThe classifier-guided diffusion model samples from <math>p(x|y)</math>, which is concentrated around the [[Maximum a posteriori estimation|maximum a posteriori estimate]] <math>\\arg\\max_x p(x|y)</math>. If we want to force the model to move towards the [[Maximum likelihood estimation|maximum likelihood estimate]] <math>\\arg\\max_x p(y|x)</math>, we can use <math display=\"block\">p_\\beta(x|y) \\propto p(y|x)^\\beta p(x) </math>where <math>\\beta > 0 </math> is interpretable as ''[[Thermodynamic beta|inverse temperature]]''. In the context of diffusion models, it is usually called the '''guidance scale'''. A high <math>\\beta </math> would force the model to sample from a distribution concentrated around <math>\\arg\\max_x p(y|x)</math>. This often improves quality of generated images.<ref>{{Cite arXiv |last1=Dhariwal |first1=Prafulla |last2=Nichol |first2=Alex |date=2021-06-01 |title=Diffusion Models Beat GANs on Image Synthesis |class=cs.LG |eprint=2105.05233 }}</ref>\n\nThis can be done simply by SGLD with<math display=\"block\">\\nabla_x \\ln p_\\beta(x|y) = \\beta\\nabla_x \\ln p(y|x) + \\nabla_x \\ln p(x) </math>\n\n=== Classifier-free guidance (CFG) ===\n\nIf we do not have a classifier <math>p(y|x)</math>, we could still extract one out of the image model itself:<ref>{{Cite arXiv |last1=Ho |first1=Jonathan |last2=Salimans |first2=Tim |date=2022-07-25 |title=Classifier-Free Diffusion Guidance |class=cs.LG |eprint=2207.12598 }}</ref><math display=\"block\">\\nabla_x \\ln p_\\beta(x|y) = (1-\\beta) \\nabla_x \\ln p(x) + \\beta \\nabla_x \\ln p(x|y) </math>Such a model is usually trained by presenting it with both <math>(x, y) </math> and <math>(x, None) </math>, allowing it to model both <math>\\nabla_x\\ln p(x|y) </math> and <math>\\nabla_x\\ln p(x) </math>.\n\n=== Samplers ===\nGiven a diffusion model, one may regard it either as a continuous process, and sample from it by integrating a SDE, or one can regard it as a discrete process, and sample from it by iterating the discrete steps. The choice of the \"noise schedule\" <math>\\beta_t</math> can also affect the quality of samples. In the DDPM perspective, one can use the DDPM itself (with noise), or DDIM (with adjustable amount of noise). The case where one adds noise is sometimes called ancestral sampling.<ref>{{Cite journal |last1=Yang |first1=Ling |last2=Zhang |first2=Zhilong |last3=Song |first3=Yang |last4=Hong |first4=Shenda |last5=Xu |first5=Runsheng |last6=Zhao |first6=Yue |last7=Zhang |first7=Wentao |last8=Cui |first8=Bin |last9=Yang |first9=Ming-Hsuan |date=2022 |title=Diffusion Models: A Comprehensive Survey of Methods and Applications |arxiv=2209.00796}}</ref> One can interpolate between noise and no noise. The amount of noise is denoted <math>\\eta</math> (\"eta value\") in the DDIM paper, with <math>\\eta = 0</math> denoting no noise (as in ''deterministic'' DDIM), and <math>\\eta = 1</math> denoting full noise (as in DDPM).\n\nIn the perspective of SDE, one can use any of the [[Numerical methods for ordinary differential equations|numerical integration methods]], such as [[Euler\u2013Maruyama method]], [[Heun's method]], [[linear multistep method]]s, etc. Just as in the discrete case, one can add an adjustable amount of noise during the integration.\n\nA survey and comparison of samplers in the context of image generation is in.<ref>{{Cite journal |last1=Karras |first1=Tero |last2=Aittala |first2=Miika |last3=Aila |first3=Timo |last4=Laine |first4=Samuli |date=2022 |title=Elucidating the Design Space of Diffusion-Based Generative Models |arxiv=2206.00364}}</ref>\n\n== Choice of architecture ==\n[[File:U-net-architecture.png|thumb|The standard U-Net architecture.]]\n[[File:Stable Diffusion architecture.png|thumb|287x287px|Architecture of Stable Diffusion.]]\n[[File:X-Y plot of algorithmically-generated AI art of European-style castle in Japan demonstrating DDIM diffusion steps.png|thumb|304x304px|The denoising process used by Stable Diffusion.]]\n\n=== Diffusion model ===\nFor generating images by DDPM, we need a neural network that takes a time <math>t</math> and a noisy image <math>x_t</math>, and predicts a noise <math>\\epsilon_\\theta(x_t, t)</math> from it. Since predicting the noise is the same as predicting the denoised image, then subtracting it from <math>x_t</math>, denoising architectures tend to work well. For example, the most common architecture is [[U-Net]], which is also good at denoising images.<ref name=\":3\">{{Cite journal |last1=Ho |first1=Jonathan |last2=Saharia |first2=Chitwan |last3=Chan |first3=William |last4=Fleet |first4=David J. |last5=Norouzi |first5=Mohammad |last6=Salimans |first6=Tim |date=2022-01-01 |title=Cascaded diffusion models for high fidelity image generation |url=https://dl.acm.org/doi/abs/10.5555/3586589.3586636 |journal=The Journal of Machine Learning Research |volume=23 |issue=1 |pages=47:2249\u201347:2281 |arxiv=2106.15282 |issn=1532-4435}}</ref>\n\nFor non-image data, we can use other architectures. For example,<ref name=\":4\">{{Cite journal |last1=Tevet |first1=Guy |last2=Raab |first2=Sigal |last3=Gordon |first3=Brian |last4=Shafir |first4=Yonatan |last5=Cohen-Or |first5=Daniel |last6=Bermano |first6=Amit H. |date=2022 |title=Human Motion Diffusion Model |arxiv=2209.14916}}</ref> models human motion trajectory by DDPM. Each human motion trajectory is a sequence of poses, represented by either joint rotations or positions. It uses a [[Transformer (machine learning model)|Transformer]] network to generate a less noisy trajectory out of a noisy one.\n\n=== Conditioning ===\nThe base diffusion model can only generate unconditionally from the whole distribution. For example, a diffusion model learned on [[ImageNet]] would generate images that look like a random image from ImageNet. To generate images from just one category, one would need to impose the condition. Whatever condition one wants to impose, one needs to first convert the conditioning into a vector of floating point numbers, then feed it into the underlying diffusion model neural network. However, one has freedom in choosing how to convert the conditioning into a vector.\n\nStable Diffusion, for example, imposes conditioning in the form of [[Attention (machine learning)|cross-attention mechanism]], where the query is an intermediate representation of the image in the U-Net, and both key and value are the conditioning vectors.<ref name=\"ReferenceA\">{{Cite journal |last1=Zhang |first1=Lvmin |last2=Rao |first2=Anyi |last3=Agrawala |first3=Maneesh |date=2023 |title=Adding Conditional Control to Text-to-Image Diffusion Models |arxiv=2302.05543}}</ref> The conditioning can be selectively applied to only parts of an image, and new kinds of conditionings can be finetuned upon the base model, as used in ControlNet.<ref name=\"ReferenceA\"/>\n\nAs a particularly simple example, consider [[Inpainting|image inpainting]]. The conditions are <math>\\tilde x</math>, the reference image, and <math>m</math>, the inpainting [[Mask (computing)#Image masks|mask]]. The conditioning is imposed at each step of the backward diffusion process, by first sampling <math>\\tilde x_t \\sim N\\left(\\sqrt{\\bar\\alpha_t} \\tilde x, (1-\\bar\\alpha_t) I \\right)</math>, a noisy version of <math>\\tilde x</math>, then replacing <math>x_t</math> with <math>(1-m) \\odot x_t + m \\odot \\tilde x_t</math>, where <math>\\odot</math> means [[Hadamard product (matrices)|elementwise multiplication]].<ref>{{Cite journal |last=Lugmayr |first=Andreas |last2=Danelljan |first2=Martin |last3=Romero |first3=Andres |last4=Yu |first4=Fisher |last5=Timofte |first5=Radu |last6=Van Gool |first6=Luc |date=2022 |title=RePaint: Inpainting Using Denoising Diffusion Probabilistic Models |url=https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html |language=en |pages=11461\u201311471}}</ref>\n\nConditioning is not limited to just generating images from a specific category, or according to a specific caption (as in text-to-image). For example,<ref name=\":4\" /> demonstrated generating human motion, conditioned on an audio clip of human walking (allowing syncing motion to a soundtrack), or video of human running, or a text description of human motion, etc.\n\n=== Upscaling ===\nAs generating an image takes a long time, one can try to generate a small image by a base diffusion model, then upscale it by other models. Upscaling can be done by [[Generative adversarial network|GAN]],<ref>{{Cite journal |last1=Wang |first1=Xintao |last2=Xie |first2=Liangbin |last3=Dong |first3=Chao |last4=Shan |first4=Ying |date=2021 |title=Real-ESRGAN: Training Real-World Blind Super-Resolution With Pure Synthetic Data |url=https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Wang_Real-ESRGAN_Training_Real-World_Blind_Super-Resolution_With_Pure_Synthetic_Data_ICCVW_2021_paper.html |language=en |pages=1905\u20131914|arxiv=2107.10833 }}</ref> [[Transformer (machine learning model)|Transformer]],<ref>{{Cite journal |last1=Liang |first1=Jingyun |last2=Cao |first2=Jiezhang |last3=Sun |first3=Guolei |last4=Zhang |first4=Kai |last5=Van Gool |first5=Luc |last6=Timofte |first6=Radu |date=2021 |title=SwinIR: Image Restoration Using Swin Transformer |url=https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html |language=en |pages=1833\u20131844|arxiv=2108.10257 }}</ref> or signal processing methods like [[Lanczos resampling]].\n\nDiffusion models themselves can be used to perform upscaling. Cascading diffusion model stacks multiple diffusion models one after another, in the style of [[StyleGAN#Progressive GAN|Progressive GAN]]. The lowest level is a standard diffusion model that generate 32x32 image, then the image would be upscaled by a diffusion model specifically trained for upscaling, and the process repeats.<ref name=\":3\" />\n\n== Examples ==\nThis section collects some notable diffusion models, and briefly describes their architecture.\n\n=== OpenAI ===\n{{Main|DALL-E}}\nThe DALL-E series by OpenAI are text-conditional diffusion models of images.\n\nThe first version of DALL-E (2021) is not actually a diffusion model. Instead, it uses a Transformer architecture that generates a sequence of tokens, which is then converted to an image by the decoder of a discrete VAE. Released with DALL-E was the CLIP classifier, which was used by DALL-E to rank generated images according to how close the image fits the text.\n\nGLIDE (2022-03)<ref>{{Cite arXiv |eprint=2112.10741 |class=cs.CV |first1=Alex |last1=Nichol |first2=Prafulla |last2=Dhariwal |title=GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models |date=2022-03-08 |last3=Ramesh |first3=Aditya |last4=Shyam |first4=Pranav |last5=Mishkin |first5=Pamela |last6=McGrew |first6=Bob |last7=Sutskever |first7=Ilya |last8=Chen |first8=Mark}}</ref> is a 3.5-billion diffusion model, and a small version was released publicly.<ref name=\"dalle2\">{{Citation |title=GLIDE |date=2023-09-22 |url=https://github.com/openai/glide-text2im |access-date=2023-09-24 |publisher=OpenAI}}</ref> Soon after, DALL-E 2 was released (2022-04).<ref>{{Cite arXiv |eprint=2204.06125 |class=cs.CV |first1=Aditya |last1=Ramesh |first2=Prafulla |last2=Dhariwal |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |date=2022-04-12 |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark}}</ref> DALL-E 2 is a 3.5-billion cascaded diffusion model that generates images from text by \"inverting the CLIP image encoder\", the technique which they termed \"unCLIP\".\n\n=== Stability AI ===\n{{Main|Stable Diffusion}}\n[[Stable Diffusion]] (2022-08), released by Stability AI, consists of a latent diffusion model (860 million parameters), a VAE, and a text encoder. The diffusion model is a U-Net, with cross-attention blocks to allow for conditional image generation.<ref name=\":02\">{{Cite web |last=Alammar |first=Jay |title=The Illustrated Stable Diffusion |url=https://jalammar.github.io/illustrated-stable-diffusion/ |access-date=2022-10-31 |website=jalammar.github.io}}</ref><ref name=\":2\" />\n\n==See also==\n* [[Diffusion process]]\n* [[Markov chain]]\n* [[Variational inference]]\n* [[Variational autoencoder]]\n\n== Further reading ==\n* [https://benanne.github.io/2022/05/26/guidance.html Guidance: a cheat code for diffusion models]. Overview of classifier guidance and classifier-free guidance, light on mathematical details.\n* Mathematical details omitted in the article.\n** {{Cite web |date=2022-09-25 |title=Power of Diffusion Models |url=https://astralord.github.io/posts/power-of-diffusion-models/ |access-date=2023-09-25 |website=AstraBlog |language=en}}\n** {{Cite web |last=Weng |first=Lilian |date=2021-07-11 |title=What are Diffusion Models? |url=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ |access-date=2023-09-25 |website=lilianweng.github.io |language=en}}\n\n==References==\n{{reflist}}\n\n[[Category:Markov models]]\n[[Category:Machine learning algorithms]]"}