{"title": "Diffusion model", "page_id": 71912239, "revision_id": 1145991165, "revision_timestamp": "2023-03-22T03:48:30Z", "content": "{{Short description|Deep learning algorithm}}\nIn [[machine learning]], '''diffusion models''', also known as '''diffusion probabilistic models''', are a class of [[latent variable model]]s. They are [[Markov chain]]s trained using [[Variational Bayesian methods|variational inference]].<ref name=\":0\">{{cite journal |last1=Ho |first1=Jonathan |last2=Jain |first2=Ajay |last3=Abbeel |first3=Pieter |title=Denoising Diffusion Probabilistic Models |date=19 June 2020 |arxiv=2006.11239}}</ref> The goal of diffusion models is to learn the latent structure of a dataset by modeling the way in which data points diffuse through the [[latent space]]. In [[computer vision]], this means that a neural network is trained to [[denoise]] images blurred with [[Gaussian noise]] by learning to reverse the diffusion process.<ref name=\":1\">{{Cite arXiv |last1=Song |first1=Yang |last2=Sohl-Dickstein |first2=Jascha |last3=Kingma |first3=Diederik P. |last4=Kumar |first4=Abhishek |last5=Ermon |first5=Stefano |last6=Poole |first6=Ben |date=2021-02-10 |title=Score-Based Generative Modeling through Stochastic Differential Equations |class=cs.LG |eprint=2011.13456 }}</ref><ref>{{cite arXiv |last1=Gu |first1=Shuyang |last2=Chen |first2=Dong |last3=Bao |first3=Jianmin |last4=Wen |first4=Fang |last5=Zhang |first5=Bo |last6=Chen |first6=Dongdong |last7=Yuan |first7=Lu |last8=Guo |first8=Baining |title=Vector Quantized Diffusion Model for Text-to-Image Synthesis |date=2021 |class=cs.CV |eprint=2111.14822}}</ref> Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.<ref>{{cite arXiv |last1= Croitoru |first1=Florinel-Alin |last2= Hondru |first2= Vlad |last3= Ionescu |first3=Radu Tudor |last4= Shah |first4= Mubarak |title=Diffusion models in vision: A survey |date=2022 |class=cs.CV |eprint=2209.04747}}</ref>\n\nDiffusion models were introduced in 2015 with a motivation from [[non-equilibrium thermodynamics]].<ref>{{Cite journal |last1=Sohl-Dickstein |first1=Jascha |last2=Weiss |first2=Eric |last3=Maheswaranathan |first3=Niru |last4=Ganguli |first4=Surya |date=2015-06-01 |title=Deep Unsupervised Learning using Nonequilibrium Thermodynamics |url=http://proceedings.mlr.press/v37/sohl-dickstein15.pdf |journal=Proceedings of the 32nd International Conference on Machine Learning |language=en |publisher=PMLR |volume=37 |pages=2256\u20132265}}</ref>\n\nDiffusion models can be applied to a variety of tasks, including [[image denoising]], [[inpainting]], [[super-resolution]], and [[text-to-image model|image generation]]. For example, an image generation model would start with a random noise image and then, after having been trained reversing the diffusion process on natural images, the model would be able to generate new natural images. Announced on 13 April 2022, [[OpenAI]]'s text-to-image model [[DALL-E 2]] is a recent example. It uses diffusion models for both the model's prior (which produces an image embedding given a text caption) and the decoder that generates the final image.<ref>{{cite arXiv |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |date=2022 |class=cs.CV |eprint=2204.06125}}</ref>\n\n== Mathematical principles ==\n{{More citations needed section|date=February 2023}}\n=== Generating an image in the space of all images ===\nConsider the problem of image generation. Let <math>x</math> represent an image, and let <math>p(x)</math> be the probability distribution over all possible images. If we have <math>p(x)</math> itself, then we can say for certain how likely a certain image is. However, this is intractable in general.\n\nMost often, we are uninterested in knowing the absolute probability that a certain image is -- when, if ever, are we interested in how likely an image is in the space of all possible images? Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors -- how more likely is this image of cat, compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some gaussian noise added?\n\nConsequently, we are actually quite uninterested in <math>p(x)</math> itself, but rather, <math>\\nabla_x \\ln p(x)</math>. This performs two effects\n* One, we no longer need to normalize <math>p(x)</math>, but can use any <math>\\tilde p(x) = Cp(x)</math>, where <math>C = \\int \\tilde p(x) dx > 0</math> is any unknown constant that is of no concern to us.\n* Two, we are comparing <math>p(x)</math> neighbors <math>p(x + dx)</math>, by <math>\\frac{p(x)}{p(x+dx)} =e^{-\\langle \\nabla_x \\ln p, dx \\rangle}</math>\n\nLet the [[Score (statistics)|score function]] be <math>s(x) := \\nabla_x \\ln p(x)</math>, then consider what we can do with <math>s(x)</math>.\n\nAs it turns out, <math>s(x)</math> allows us to sample from <math>p(x)</math> using [[stochastic gradient Langevin dynamics]], which is essentially an infinitesimal version of [[Markov chain Monte Carlo]].<ref name=\":1\" />\n\n=== Learning the score function ===\nThe score function can be learned by noising-denoising.<ref name=\":0\" />\n\n== Main variants ==\n=== Classifier guidance ===\nSuppose we wish to sample not from the entire distribution of images, but conditional on the image description. We don't want to sample a generic image, but an image that fits the description \"black cat with red eyes\". Generally, we want to sample from the distribution <math>p(x|y)</math>, where <math>x</math> ranges over images, and <math>y</math> ranges over classes of images (a description \"black cat with red eyes\" is just a very detailed class, and a class \"cat\" is just a very vague description).\n\nTaking the perspective of the [[noisy channel model]], we can understand the process as follows: To generate an image <math>x</math> conditional on description <math>y</math>, we imagine that the requester really had in mind an image <math>x</math>, but the image is passed through a noisy channel and came out garbled, as <math>y</math>. Image generation is then nothing but inferring which <math>x</math> the requester had in mind.\n\nIn other words, conditional image generation is simply \"translating from a textual language into a pictorial language\". Then, as in noisy-channel model, we use Bayes theorem to get<math display=\"block\">p(x|y) \\propto p(y|x)p(x) </math>in other words, if we have a good model of the space of all images, and a good image-to-class translator, we get a class-to-image translator \"for free\".\n\nThe [[Stochastic_gradient_Langevin_dynamics|SGLD]] uses<math display=\"block\">\\nabla_x \\ln p(x|y) = \\nabla_x \\ln p(y|x) + \\nabla_x \\ln p(x) </math>where <math>\\nabla_x \\ln p(x)</math> is the score function, trained as previously described, and <math>\\nabla_x \\ln p(y|x)</math> is found by using a differentiable image classifier.\n\n=== With temperature ===\nThe classifier-guided diffusion model samples from <math>p(x|y)</math>, which is concentrated around the [[Maximum a posteriori estimation|maximum a posteriori estimate]] <math>\\arg\\max_x p(x|y)</math>. If we want to force the model to move towards the [[Maximum likelihood estimation|maximum likelihood estimate]] <math>\\arg\\max_x p(y|x)</math>, we can use <math display=\"block\">p_\\beta(x|y) \\propto p(y|x)^\\beta p(x) </math>where <math>\\beta > 0 </math> is interpretable as ''[[Thermodynamic beta|inverse temperature]]''. In the context of diffusion models, it is usually called the '''guidance scale'''. A high <math>\\beta </math> would force the model to sample from a distribution concentrated around <math>\\arg\\max_x p(y|x)</math>. This often improves quality of generated images.<ref>{{Cite arXiv |last1=Dhariwal |first1=Prafulla |last2=Nichol |first2=Alex |date=2021-06-01 |title=Diffusion Models Beat GANs on Image Synthesis |class=cs.LG |eprint=2105.05233 }}</ref>\n\nThis can be done simply by SGLD with<math display=\"block\">\\nabla_x \\ln p_\\beta(x|y) = \\beta\\nabla_x \\ln p(y|x) + \\nabla_x \\ln p(x) </math>\n\n=== Classifier-free guidance ===\n\nIf we do not have a classifier <math>p(y|x)</math>, we could still extract one out of the image model itself:<ref>{{Cite arXiv |last1=Ho |first1=Jonathan |last2=Salimans |first2=Tim |date=2022-07-25 |title=Classifier-Free Diffusion Guidance |class=cs.LG |eprint=2207.12598 }}</ref><math display=\"block\">\\nabla_x \\ln p_\\beta(x|y) = (1-\\beta) \\nabla_x \\ln p(x) + \\beta \\nabla_x \\ln p(x|y) </math>Such a model is usually trained by presenting it with both <math>(x, y) </math> and <math>(x, None) </math>, allowing it to model both <math>\\nabla_x\\ln p(x|y) </math> and <math>\\nabla_x\\ln p(x) </math>.\n\nThis is an integral part of systems like GLIDE,<ref>{{Cite arXiv |last1=Nichol |first1=Alex |last2=Dhariwal |first2=Prafulla |last3=Ramesh |first3=Aditya |last4=Shyam |first4=Pranav |last5=Mishkin |first5=Pamela |last6=McGrew |first6=Bob |last7=Sutskever |first7=Ilya |last8=Chen |first8=Mark |date=2022-03-08 |title=GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models |class=cs.CV |eprint=2112.10741 }}</ref> [[DALL-E]]<ref>{{Cite arXiv |last1=Ramesh |first1=Aditya |last2=Dhariwal |first2=Prafulla |last3=Nichol |first3=Alex |last4=Chu |first4=Casey |last5=Chen |first5=Mark |date=2022-04-12 |title=Hierarchical Text-Conditional Image Generation with CLIP Latents |class=cs.CV |eprint=2204.06125 }}</ref> and Google Imagen.<ref>{{Cite arXiv |last1=Saharia |first1=Chitwan |last2=Chan |first2=William |last3=Saxena |first3=Saurabh |last4=Li |first4=Lala |last5=Whang |first5=Jay |last6=Denton |first6=Emily |last7=Ghasemipour |first7=Seyed Kamyar Seyed |last8=Ayan |first8=Burcu Karagol |last9=Mahdavi |first9=S. Sara |last10=Lopes |first10=Rapha Gontijo |last11=Salimans |first11=Tim |last12=Ho |first12=Jonathan |last13=Fleet |first13=David J. |last14=Norouzi |first14=Mohammad |date=2022-05-23 |title=Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding |class=cs.CV |eprint=2205.11487 }}</ref>\n\n==See also==\n* [[Diffusion process]]\n* [[Markov chain]]\n* [[Variational inference]]\n* [[Variational autoencoder]]\n\n== Further reading ==\n* [https://benanne.github.io/2022/05/26/guidance.html Guidance: a cheat code for diffusion models]. Good overview up to 2022.\n\n==References==\n{{reflist}}\n\n[[Category:Markov models]]\n[[Category:Machine learning algorithms]]\n\n\n{{Artificial-intelligence-stub}}"}