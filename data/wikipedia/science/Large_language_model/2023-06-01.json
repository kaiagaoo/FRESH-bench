{"title": "Large language model", "page_id": 73248112, "revision_id": 1157904541, "revision_timestamp": "2023-05-31T18:26:00Z", "content": "{{short description|Language model consisting of a neural network}}\nA '''large language model''' ('''LLM''') is a [[language model]] consisting of a [[Artificial neural network|neural network]] with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using [[self-supervised learning]] or [[semi-supervised learning]].<ref>{{Cite web|url=https://analyticsindiamag.com/self-supervised-learning-vs-semi-supervised-learning-how-they-differ/|title=Self-Supervised Learning Vs Semi-Supervised Learning: How They Differ|first=Shraddha|last=Goled|date=May 7, 2021|website=Analytics India Magazine}}</ref> LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of [[natural language processing]] research away from the previous paradigm of training specialized [[supervised learning|supervised]] models for specific tasks.<ref name=Manning-2022/>\n\nThough the term ''large language model'' has no formal definition, it often refers to [[deep learning]] models having a parameter count on the order of billions or more.<ref name=\"extracting\" /> LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as [[sentiment analysis]], [[named entity recognition]], or [[mathematical reasoning]]).<ref name=\"Manning-2022\" /><ref name=\"emergentpaper\" /> The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.<ref name=\"Bowman\">{{cite journal |last=Bowman |first=Samuel R. |title=Eight Things to Know about Large Language Models |year=2023 |arxiv=2304.00612 |url=https://cims.nyu.edu/~sbowman/eightthings.pdf}}</ref>\n\nThough trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate considerable general knowledge about the world, and are able to \"memorize\" a great quantity of facts during training.<ref name=\"Manning-2022\" />\n\n==Properties==\n=== Pretraining datasets ===\n{{See also|list of datasets for machine-learning research#Internet}}\n\nLLMs are pre-trained on large textual datasets. Some commonly used textual datasets are [[Common Crawl]], [[The Pile (dataset)|The Pile]], MassiveText,<ref>{{Cite web |title=Papers with Code - MassiveText Dataset |url=https://paperswithcode.com/dataset/massivetext |access-date=2023-04-26 |website=paperswithcode.com |language=en}}</ref> [[Wikipedia]], and [[GitHub]]. The datasets run up to 10 trillion words in size.\n\nThe stock of high-quality language data is within 4.6-17 trillion words, which is within an order of magnitude for the largest textual datasets.<ref>{{Cite arXiv |last1=Villalobos |first1=Pablo |last2=Sevilla |first2=Jaime |last3=Heim |first3=Lennart |last4=Besiroglu |first4=Tamay |last5=Hobbhahn |first5=Marius |last6=Ho |first6=Anson |date=2022-10-25 |title=Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning |class=cs.LG |eprint=2211.04325 }}</ref>\n\n=== Scaling laws ===\n{{Main|Neural scaling law}}\nIn general, a LLM can be characterized by 4 parameters: size of the model, size of the training dataset, cost of training, performance after training. Each of these four variables can be precisely defined into a real number, and they are empirically found to be related by simple [[Empirical statistical laws|statistical laws]], called \"scaling laws\".\n\nOne particular scaling law (\"[[Chinchilla AI|Chinchilla scaling]]\") for LLM autoregressively trained for one epoch, with a [[log-log plot|log-log]] [[learning rate]] schedule, states that:<ref>{{Cite arXiv |last1=Hoffmann |first1=Jordan |last2=Borgeaud |first2=Sebastian |last3=Mensch |first3=Arthur |last4=Buchatskaya |first4=Elena |last5=Cai |first5=Trevor |last6=Rutherford |first6=Eliza |last7=Casas |first7=Diego de Las |last8=Hendricks |first8=Lisa Anne |last9=Welbl |first9=Johannes |last10=Clark |first10=Aidan |last11=Hennigan |first11=Tom |last12=Noland |first12=Eric |last13=Millican |first13=Katie |last14=Driessche |first14=George van den |last15=Damoc |first15=Bogdan |date=2022-03-29 |title=Training Compute-Optimal Large Language Models |class=cs.CL |eprint=2203.15556 }}</ref><math display=\"block\">\\begin{cases}\nC = C_0 ND\\\\\nL = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\n\\end{cases}</math>where the variables are\n* <math>C</math> is the cost of training the model, in [[FLOPS|FLOPs]].\n* <math>N</math> is the number of parameters in the model.\n* <math>D</math> is the number of tokens in the training set.\n* <math>L</math> is the average negative log-likelihood loss per token ([[Nat (unit)|nats]]/token), achieved by the trained LLM on the test dataset.\nand the statistical parameters are\n\n* <math> C_0 = 6</math>, meaning that it costs 6 FLOPs per parameter to train on one token.<ref name=\"kaplan-scaling\" /> Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n* <math>\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69</math>.\n\n=== Emergent abilities ===\n[[File:LLM emergent benchmarks.png|thumb|On a number of natural language benchmarks involving tasks such as question answering, models perform no better than random chance until they reach a certain scale (in this case, measured by training computation), at which point their performance sharply increases. These are examples of emergent abilities.]]\nWhile it is generally the case that performance of large models on various tasks can be extrapolated based on the performance of similar smaller models, sometimes \"[[Neural scaling law#Broken Neural Scaling Laws (BNSL)|breaks]]\"<ref>Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). [[arxiv:2210.14891|Broken Neural Scaling Laws]]. International Conference on Learning Representations (ICLR), 2023.</ref> in downstream scaling laws occur such that larger models suddenly acquire substantial abilities at a different rate than in smaller models. These are often referred to as \"emergent abilities\", and have been the subject of substantial study. Researchers note that such abilities often \"cannot be predicted simply by extrapolating the performance of smaller models\".<ref name=\"emergentpaper\">{{cite journal |last1=Wei |first1=Jason |last2=Tay |first2=Yi |last3=Bommasani |first3=Rishi |last4=Raffel |first4=Colin |last5=Zoph |first5=Barret |last6=Borgeaud |first6=Sebastian |last7=Yogatama |first7=Dani |last8=Bosma |first8=Maarten |last9=Zhou |first9=Denny |last10=Metzler |first10=Donald |last11=Chi |first11=Ed H. |last12=Hashimoto |first12=Tatsunori |last13=Vinyals |first13=Oriol |last14=Liang |first14=Percy |last15=Dean |first15=Jeff |last16=Fedus |first16=William |title=Emergent Abilities of Large Language Models |journal=Transactions on Machine Learning Research |date=31 August 2022 |url=https://openreview.net/forum?id=yzkSU5zdwD |language=en |issn=2835-8856}}</ref> These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed.<ref name=Bowman /> Hundreds of emergent abilities have been described. Examples include multi-step arithmetic, taking college-level exams, identifying the intended meaning of a word,<ref name=\"emergentpaper\"/> [[chain-of-thought prompting]],<ref name=\"emergentpaper\"/> decoding the [[International Phonetic Alphabet]], unscrambling a word\u2019s letters, identifying offensive content in paragraphs of [[Hinglish]] (a combination of Hindi and English), and generating a similar English equivalent of [[Kiswahili]] proverbs.<ref>{{Cite web|url=https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/|title=The Unpredictable Abilities Emerging From Large AI Models|first=Stephen|last=Ornes|date=March 16, 2023|website=Quanta Magazine}}</ref>\n\n=== Hallucination ===\nGenerative LLMs have been observed to confidently assert claims of fact which do not seem to be [[Justification (epistemology)|justified]] by their [[training data]], a phenomenon which has been termed \"[[hallucination (artificial intelligence)|hallucination]]\".<ref name=\"hallucination-survey\">{{cite journal |last1=Ji |first1=Ziwei |last2=Lee |first2=Nayeon |last3=Frieske |first3=Rita |last4=Yu |first4=Tiezheng |last5=Su |first5=Dan |last6=Xu |first6=Yan |last7=Ishii |first7=Etsuko |last8=Bang |first8=Yejin |last9=Dai |first9=Wenliang |last10=Madotto |first10=Andrea |last11=Fung |first11=Pascale |date=November 2022 |title=Survey of Hallucination in Natural Language Generation |url=https://dl.acm.org/doi/pdf/10.1145/3571730 |format=pdf |journal=ACM Computing Surveys |publisher=[[Association for Computing Machinery]] |volume=55 |issue=12 |pages=1\u201338 |arxiv=2202.03629 |doi=10.1145/3571730 |s2cid=246652372 |access-date=15 January 2023}}</ref>\n\n==Architecture==\nLarge language models have most commonly used the [[transformer (machine learning)|transformer]] architecture, which, since 2018, has become the standard deep learning technique for sequential data (previously, recurrent architectures such as the [[LSTM]] were most common).<ref name=Manning-2022/>\n\n=== Tokenization ===\nLLMs are mathematical functions whose input and output are lists of numbers. Consequently, words must be converted to numbers.\n\nIn general, a LLM uses a separate [[Lexical analysis|tokenizer]]. A tokenizer maps between texts and lists of [[integer]]s. The tokenizer is generally adapted to the entire training dataset first, then ''frozen'', before the LLM is trained. A common choice is [[byte pair encoding]].\n\nAnother function of tokenizers is [[Data compression|text compression]], which saves [[Computing|compute]]. Common words or phrases like \"where is\" can be encoded into one token, instead of 7 characters. The OpenAI GPT series uses a tokenizer where 1 token maps to around 4 characters, or around 0.75 words, in common English text.<ref>{{Cite web |title=OpenAI API |url=https://platform.openai.com/ |archive-url=https://web.archive.org/web/20230423211308/https://platform.openai.com/tokenizer |archive-date=April 23, 2023 |access-date=2023-04-30 |website=platform.openai.com |language=en}}</ref> Uncommon English text is less predictable, thus less compressible, thus requiring more tokens to encode.\n\nTokenizer cannot output arbitrary integers. They generally output only integers in the range <math>\\{0, 1, 2, ..., V-1\\}</math>, where <math>V</math> is called its vocabulary size.\n\nSome tokenizers are capable of handling arbitrary text (generally by operating directly on [[Unicode]]), but some do not. When encountering un-encodable text, a tokenizer would output a special token (often 0) that represents \"unknown text\". This is often written as [UNK], such as in the BERT paper.\n\nAnother special token commonly used is [PAD] (often 1), for \"padding\". This is used because LLMs are generally used on batches of text at one time, and these texts do not encode to the same length. Since LLMs generally require input to be an array that is not [[Jagged array|jagged]], the shorter encoded texts must be padded until they match the length of the longest one.\n\n=== Output ===\nThe output of a LLM is a probability distribution over its vocabulary. This is usually implemented as follows:\n\n* Upon receiving a text, the bulk of the LLM outputs a vector <math>y\\in \\R^V</math> where <math>V</math> is its vocabulary size (defined above).\n* The vector <math>y</math> is passed through a [[softmax function]] to obtain <math>\\textit{softmax}(y)</math>.\n\nIn the process, the vector <math>y</math> is usually called the unnormalized [[logit]] vector, and the vector <math>\\textit{softmax}(y)</math> is called the probability vector. Since the vector <math>\\textit{softmax}(y)</math> has <math>V</math> entries, all non-negative, and they sum to 1, we can interpret it as a probability distribution over <math>\\{0, 1, 2, ..., V-1\\}</math>\u2014that is, it is a probability distribution over the LLM's vocabulary.\n\nNote that the softmax function is defined mathematically with no parameters to vary. Consequently it is not trained.\n\n== Training ==\nMost LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:<ref>{{cite journal |last1=Zaib |first1=Munazza |last2=Sheng |first2=Quan Z. |last3=Emma Zhang |first3=Wei |date=4 February 2020 |title=A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP |url=https://www.researchgate.net/publication/338931711 |journal=Proceedings of the Australasian Computer Science Week Multiconference |pages=1\u20134 |arxiv=2104.10810 |doi=10.1145/3373017.3373028 |isbn=9781450376976 |s2cid=211040895}}</ref>\n\n* autoregressive ([[Generative pretrained transformer|GPT]]-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the ''next'' tokens, like \"ice cream\".\n* masked (\"BERT-style\",<ref name=\"jm\" /> \"[[cloze test]]\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\nLLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.<ref name=\"jm\" />\n\nUsually, LLMs are trained to minimize a specific loss function: the average negative [[Log-likelihood|log likelihood]] per token (also called [[Cross entropy|cross-entropy loss]]).{{citation needed|date=May 2023}} For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution <math>Pr( \\cdot | \\text{I like to eat})</math> then the negative log likelihood loss on this token is <math>-\\log Pr( \\text{ice} | \\text{I like to eat}) </math>.\n\nDuring training, [[Regularization (mathematics)|regularization]] loss is also used to stabilize training. However regularization loss is usually not used during [[Training, validation, and test data sets|testing]] and evaluation. There are also many more evaluation criteria than just negative log likelihood. See [[Large language model#Evaluation|the section below]] for details.\n\n=== Training dataset size ===\nThe earliest LLMs were trained on [[text corpus|corpora]] having on the order of billions of words.\n\n[[OpenAI#OpenAI's original GPT model (\"GPT-1\")|GPT-1]], the first model in [[OpenAI]]'s numbered series of [[generative pre-trained transformer]] models, was trained in 2018 on [[BookCorpus]], consisting of 985 million words.<ref>{{cite journal |last1=Zhu |first1=Yukun |last2=Kiros |first2=Ryan |last3=Zemel |first3=Rich |last4=Salakhutdinov |first4=Ruslan |last5=Urtasun |first5=Raquel |last6=Torralba |first6=Antonio |last7=Fidler |first7=Sanja |title=Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books |journal=2015 IEEE International Conference on Computer Vision (ICCV) |date=December 2015 |pages=19\u201327 |doi=10.1109/ICCV.2015.11 |arxiv=1506.06724 |isbn=978-1-4673-8391-2 |s2cid=6866988 |url=https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf |access-date=11 April 2023}}</ref> In the same year, [[BERT (language model)|BERT]] was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.<ref name=\"jm\" /> Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.<ref name=\"jm\" />\n\n=== Training cost ===\nLLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at $1.6 million.<ref name=Wiggers/> Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 [[Ampere (microarchitecture)|A100-GPU]]-hours to train a 12 billion parameter model.<ref name=Pythia>{{cite arXiv |last1=Biderman |first1=Stella |last2=Schoelkopf |first2=Hailey |last3=Anthony |first3=Quentin |last4=Bradley |first4=Herbie |last5= Khan |first5=Mohammad Aflah |last6=Purohit |first6=Shivanshu |last7=Prashanth |first7=USVSN Sai|title= Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling |date=April 2023 |class=cs.CL |eprint=2304.01373 }}</ref>\n\nFor Transformer-based LLM, it costs 6 FLOPs per parameter to train on one token.<ref name=\"kaplan-scaling\" /> Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n==Application to downstream tasks==\nBetween 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to [[fine-tuning (machine learning)|fine tune]] the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as [[GPT-3]] can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.<ref name=Manning-2022/>\n\n===Fine-tuning===\n{{main|Fine-tuning (machine learning)}}\n\nFine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. [[sentiment analysis]], [[named-entity recognition]], or [[part-of-speech tagging]]). It is a form of [[transfer learning]]. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).<ref name=jm/>\n\n===Prompting===\n{{see also|Prompt engineering|Few-shot learning (natural language processing)}}\nIn the prompting paradigm, popularized by GPT-3,<ref name=\"emergentpaper\"/> the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via [[inference (machine learning)|inference]]). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.<ref name=Manning-2022/> For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:<ref name=\"emergentpaper\"/>\n\n<pre>Review: This movie stinks.\nSentiment: negative\n\nReview: This movie is fantastic!\nSentiment:</pre>\n\nIf the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.<ref name=Wiggers/><ref name=\"few-shot-learners\"/> An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".<ref name=flan-blog/>\n\nFew-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are [[machine translation|translation]], [[question answering]], [[cloze test|cloze]] tasks, unscrambling words, and using a novel word in a sentence.<ref name=\"few-shot-learners\"/> The creation and optimisation of such prompts is called [[prompt engineering]].\n\n===Instruction tuning===\nInstruction tuning is a form of fine-tuning designed to facilitate more natural and accurate zero-shot prompting interactions. Given a text input, a pretrained language model will generate a completion which matches the distribution of text on which it was trained. A naive language model given the prompt \"Write an essay about the main themes of ''Hamlet''.\" might provide a completion such as \"A late penalty of 10% per day will be applied to submissions received after March 17.\" In instruction tuning, the language model is trained on many examples of tasks formulated as natural language instructions, along with appropriate responses.\n\nVarious techniques for instruction tuning have been applied in practice. One example, \"self-instruct\", fine-tunes the language model on a training set of examples which are themselves generated by an LLM ([[bootstrapping|bootstrapped]] from a small initial set of human-generated examples).<ref name=\"self-instruct-paper\" />\n\n=== Reinforcement learning ===\nOpenAI's InstructGPT protocol involves supervised fine-tuning on a dataset of human-generated (prompt, response) pairs, followed by [[reinforcement learning from human feedback]] (RLHF), in which a reward model was supervised-learned on a dataset of human preferences, then this reward model was used to train the LLM itself by [[Proximal Policy Optimization|proximal policy optimization]].<ref name=\"instructGPT-paper\" />\n\n==Evaluation==\n===Perplexity===\nThe most commonly used measure of a language model's performance is its [[perplexity]] on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:<math display=\"block\">\\log(\\text{Perplexity}) = -\\frac{1}{N} \\sum_{i=1}^N \\log(Pr(\\text{token}_i | \\text{context for token}_i))</math>here <math>N</math> is the number of tokens in the text corpus, and \"context for token i\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token i\" is the segment of text appearing before token i. If the LLM is masked, then \"context for token i\" is the segment of text surrounding token i.\n\nBecause language models may [[overfit]] to their training data, models are usually evaluated by their perplexity on a [[test set]] of unseen data.<ref name=\"jm\" /> This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models' training data inadvertently includes portions of any given test set.<ref name=\"few-shot-learners\" />\n\n===Task-specific datasets and benchmarks===\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\n\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").<ref name=boolq/> A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"<ref name=boolq/>). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training.<ref name=survey/> Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.<ref name=survey/>\n\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".<ref name=\"few-shot-learners\"/>\n\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.<ref name=Huyen/><ref name=survey/>\n\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of ''n'' in ''n''-shot prompting).\n\n====Adversarially constructed evaluations====\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.<ref name=bigbench/>\n\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom ''[[wikt:you can't teach an old dog new tricks|you can't teach an old dog new tricks]]'', even though this is not literally true.<ref name=truthfulqa/>\n\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\n\n<blockquote>\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...<br/>\na) demonstrates how to increase efficient exercise work by running up and down balls.<br/>\nb) moves all his arms and legs and builds up a lot of muscle.<br/>\nc) then plays the ball and we see a graphics and hedge trimming demonstration.<br/>\nd) performs sits ups while on the ball and talking.<ref name=hellaswag/></blockquote>\n\n[[BERT (language model)|BERT]] selects b) as the most likely completion, though the correct answer is d).<ref name=hellaswag/>\n\n==List of large language models==\n\n{| class=\"wikitable sortable\"\n|+ List of large language models\n|-\n! Name !! Release date{{efn|This is the date that documentation describing the model's architecture was first released.}} !! Developer !! Number of parameters{{efn|In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.}} !! Corpus size !! License{{efn|This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.}} !! Notes\n|-\n| [[BERT (language model)|BERT]] || {{dts|2018}} || [[Google]] || {{sort|340000000|340 million}}<ref name=bert-paper/> || {{sort|3300000000|3.3 billion}} words<ref name=bert-paper/> || {{yes|Apache 2.0}}<ref name=bert-web>{{Cite web|url=https://github.com/google-research/bert|title=BERT|date=March 13, 2023|via=GitHub}}</ref>\n| An early and influential language model,<ref name=Manning-2022/> but encoder-only and thus not built to be prompted or generative<ref>{{cite arXiv |last1=Patel |first1=Ajay |last2=Li |first2=Bryan |last3=Rasooli |first3=Mohammad Sadegh |last4=Constant |first4=Noah |last5=Raffel |first5=Colin |last6=Callison-Burch |first6=Chris |title=Bidirectional Language Models Are Also Few-shot Learners |date=2022 |class=cs.LG |eprint=2209.14500}}</ref>\n|-\n| XLNet || {{dts|2019}} || [[Google]] || {{sort|340000000|~340 million}}<ref>{{Cite web|url=https://www.kdnuggets.com/bert-roberta-distilbert-xlnet-which-one-to-use.html|title=BERT, RoBERTa, DistilBERT, XLNet: Which one to use?}}</ref> || {{sort|3300000000|33 billion}} words ||\n| An alternative to BERT; designed as encoder-only<ref>{{Cite web|url=https://analyticsindiamag.com/google-introduces-new-architecture-to-reduce-cost-of-transformers/|title=Google Introduces New Architecture To Reduce Cost Of Transformers|first=Amit Raja|last=Naik|date=September 23, 2021|website=Analytics India Magazine}}</ref><ref>{{cite arXiv |last1=Yang |first1=Zhilin |last2=Dai |first2=Zihang |last3=Yang |first3=Yiming |last4=Carbonell |first4=Jaime |last5=Salakhutdinov |first5=Ruslan |last6=Le |first6=Quoc V. |title=XLNet: Generalized Autoregressive Pretraining for Language Understanding |date=2 January 2020 |class=cs.CL |eprint=1906.08237 }}</ref>\n|-\n| [[GPT-2]] || {{dts|2019}} || [[OpenAI]] || {{sort|1500000000|1.5 billion}}<ref name=\"15Brelease\"/> || 40GB<ref>{{cite web |title=Better language models and their implications |url=https://openai.com/research/better-language-models |website=openai.com}}</ref> (~{{sort|10000000000|10 billion}} tokens)<ref name=\"LambdaLabs\">{{cite web |title=OpenAI's GPT-3 Language Model: A Technical Overview |url=https://lambdalabs.com/blog/demystifying-gpt-3 |website=lambdalabs.com |date=3 June 2020 |language=en}}</ref> || {{yes|MIT}}<ref>{{cite web|work=GitHub|title=gpt-2|url=https://github.com/openai/gpt-2|access-date=13 March 2023}}</ref>\n| general-purpose model based on transformer architecture\n|-\n| [[GPT-3]] || {{dts|2020}} || OpenAI || {{sort|175000000000|175 billion}}<ref name=Wiggers/> || {{sort|300000000000|300 billion}} tokens<ref name=\"LambdaLabs\"/> || {{partial success|public web API}}\n| A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called [[ChatGPT]] in 2022.<ref name=chatgpt-blog/>\n|-\n| GPT-Neo || {{dts|March 2021}} || [[EleutherAI]] || {{sort|2700000000|2.7 billion}}<ref name=\"gpt-neo\">{{Cite web|url=https://github.com/EleutherAI/gpt-neo|title=GPT Neo|date=March 15, 2023|via=GitHub}}</ref> || 825 GiB<ref name=\"Pile\">{{cite arXiv |last1=Gao |first1=Leo |last2=Biderman |first2=Stella |last3=Black |first3=Sid |last4=Golding |first4=Laurence |last5=Hoppe |first5=Travis |last6=Foster |first6=Charles |last7=Phang |first7=Jason |last8=He |first8=Horace |last9=Thite |first9=Anish |last10=Nabeshima |first10=Noa |last11=Presser |first11=Shawn |last12=Leahy |first12=Connor |title=The Pile: An 800GB Dataset of Diverse Text for Language Modeling |eprint=2101.00027|date=31 December 2020 |class=cs.CL }}</ref> || {{yes|MIT}}<ref name=vb-gpt-neo/>\n| The first of [[EleutherAI#GPT models|a series of free GPT-3 alternatives]] released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.<ref name=vb-gpt-neo/>\n|-\n| [[GPT-J]] || {{dts|June 2021}} || [[EleutherAI]] || {{sort|6000000000|6 billion}}<ref>{{Cite web |title=GPT-J-6B: An Introduction to the Largest Open Source GPT Model {{!}} Forefront |url=https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model |access-date=2023-02-28 |website=www.forefront.ai |language=en}}</ref> || 825 GiB<ref name=\"Pile\"/> || {{yes|Apache 2.0}}\n| GPT-3-style language model\n|-\n| Megatron-Turing NLG || {{dts|October 2021}}<ref>{{cite web |last1=Alvi |first1=Ali |last2=Kharya |first2=Paresh |title=Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model |url=https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/ |website=Microsoft Research |date=11 October 2021}}</ref> || [[Microsoft]] and [[Nvidia]] || {{sort|530000000000|530 billion}}<ref name=mtnlg-preprint/> || {{sort|338600000000|338.6 billion}} tokens<ref name=mtnlg-preprint/> || {{no|Restricted web access}}\n| Standard architecture but trained on a supercomputing cluster.\n|-\n| Ernie 3.0 Titan || {{dts|December 2021}} || [[Baidu]] || {{sort|260000000000|260 billion}}<ref>{{Cite journal|title=ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation|first1=Shuohuan|last1=Wang|first2=Yu|last2=Sun|first3=Yang|last3=Xiang|first4=Zhihua|last4=Wu|first5=Siyu|last5=Ding|first6=Weibao|last6=Gong|first7=Shikun|last7=Feng|first8=Junyuan|last8=Shang|first9=Yanbin|last9=Zhao|first10=Chao|last10=Pang|first11=Jiaxiang|last11=Liu|first12=Xuyi|last12=Chen|first13=Yuxiang|last13=Lu|first14=Weixin|last14=Liu|first15=Xi|last15=Wang|first16=Yangfan|last16=Bai|first17=Qiuliang|last17=Chen|first18=Li|last18=Zhao|first19=Shiyong|last19=Li|first20=Peng|last20=Sun|first21=Dianhai|last21=Yu|first22=Yanjun|last22=Ma|first23=Hao|last23=Tian|first24=Hua|last24=Wu|first25=Tian|last25=Wu|first26=Wei|last26=Zeng|first27=Ge|last27=Li|first28=Wen|last28=Gao|first29=Haifeng|last29=Wang|date=December 23, 2021|arxiv=2112.12731}}</ref> || 4 Tb || {{no|Proprietary}}\n| Chinese-language LLM. [[Ernie Bot]] is based on this model.\n|-\n| Claude<ref>{{cite web |title=Product |url=https://www.anthropic.com/product |website=Anthropic |access-date=14 March 2023 |language=en}}</ref> || {{dts|December 2021}} || [[Anthropic]] || {{sort|52000000000|52 billion}}<ref name=\"AnthroArch\">{{cite arXiv |last1=Askell |first1=Amanda |last2=Bai |first2=Yuntao |last3=Chen |first3=Anna |last4=Drain |first4=Dawn |last5=Ganguli |first5=Deep |last6=Henighan |first6=Tom |last7=Jones |first7=Andy |last8=Joseph |first8=Nicholas |last9=Mann |first9=Ben |last10=DasSarma |first10=Nova |last11=Elhage |first11=Nelson |last12=Hatfield-Dodds |first12=Zac |last13=Hernandez |first13=Danny |last14=Kernion |first14=Jackson |last15=Ndousse |first15=Kamal |last16=Olsson |first16=Catherine |last17=Amodei |first17=Dario |last18=Brown |first18=Tom |last19=Clark |first19=Jack |last20=McCandlish |first20=Sam |last21=Olah |first21=Chris |last22=Kaplan |first22=Jared |display-authors=3 |title=A General Language Assistant as a Laboratory for Alignment |eprint=2112.00861 |date=9 December 2021 |class=cs.CL }}</ref> || {{sort|400000000000|400 billion}} tokens<ref name=\"AnthroArch\"/> || {{partial success|Closed beta}}\n| Fine-tuned for desirable behavior in conversations.<ref>{{cite arXiv |last1=Bai |first1=Yuntao |last2=Kadavath |first2=Saurav |last3=Kundu |first3=Sandipan |last4=Askell |first4=Amanda |last5=Kernion |first5=Jackson |last6=Jones |first6=Andy |last7=Chen |first7=Anna |last8=Goldie |first8=Anna |last9=Mirhoseini |first9=Azalia |last10=McKinnon |first10=Cameron |last11=Chen |first11=Carol |last12=Olsson |first12=Catherine |last13=Olah |first13=Christopher |last14=Hernandez |first14=Danny |last15=Drain |first15=Dawn |last16=Ganguli |first16=Deep |last17=Li |first17=Dustin |last18=Tran-Johnson |first18=Eli |last19=Perez |first19=Ethan |last20=Kerr |first20=Jamie |last21=Mueller |first21=Jared |last22=Ladish |first22=Jeffrey |last23=Landau |first23=Joshua |last24=Ndousse |first24=Kamal |last25=Lukosuite |first25=Kamile |last26=Lovitt |first26=Liane |last27=Sellitto |first27=Michael |last28=Elhage |first28=Nelson |last29=Schiefer |first29=Nicholas |last30=Mercado |first30=Noemi |last31=DasSarma |first31=Nova |last32=Lasenby |first32=Robert |last33=Larson |first33=Robin |last34=Ringer |first34=Sam |last35=Johnston |first35=Scott |last36=Kravec |first36=Shauna |last37=Showk |first37=Sheer El |last38=Fort |first38=Stanislav |last39=Lanham |first39=Tamera |last40=Telleen-Lawton |first40=Timothy |last41=Conerly |first41=Tom |last42=Henighan |first42=Tom |last43=Hume |first43=Tristan |last44=Bowman |first44=Samuel R. |last45=Hatfield-Dodds |first45=Zac |last46=Mann |first46=Ben |last47=Amodei |first47=Dario |last48=Joseph |first48=Nicholas |last49=McCandlish |first49=Sam |last50=Brown |first50=Tom |last51=Kaplan |first51=Jared |display-authors=3 |title=Constitutional AI: Harmlessness from AI Feedback |eprint=2212.08073 |date=15 December 2022 |class=cs.CL }}</ref>\n|-\n| GLaM (Generalist Language Model) || {{dts|December 2021}} || Google || {{sort|1200000000000|1.2 trillion}}<ref name=glam-blog/> || {{sort|1600000000000|1.6 trillion}} tokens<ref name=glam-blog/> || {{no|Proprietary}}\n| Sparse mixture-of-experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\n|-\n| Gopher || {{dts|December 2021}} || [[DeepMind]] || {{sort|280000000000|280 billion}}<ref>{{cite web |title=Language modelling at scale: Gopher, ethical considerations, and retrieval |url=https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval |website=www.deepmind.com |access-date=20 March 2023 |language=en}}</ref> || {{sort|300000000000|300 billion}} tokens<ref name=hoffman/> || {{no|Proprietary}}\n| \n|-\n| [[LaMDA]] (Language Models for Dialog Applications) || {{dts|January 2022}} || Google || {{sort|137000000000|137 billion}}<ref name=lamda-blog/> ||  1.56T words,<ref name=lamda-blog/> {{sort|168000000000|168 billion}} tokens<ref name=hoffman/> || {{no|Proprietary}}\n| Specialized for response generation in conversations.\n|-\n| GPT-NeoX || {{dts|February 2022}} || [[EleutherAI]] || {{sort|20000000000|20 billion}}<ref name=\"gpt-neox-20b\">{{cite conference |title=GPT-NeoX-20B: An Open-Source Autoregressive Language Model |conference=Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models |date=2022-05-01 |last1=Black |first1=Sidney |last2=Biderman |first2=Stella |last3=Hallahan |first3=Eric |display-authors=etal |volume=Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models |pages=95\u2013136 |url=https://aclanthology.org/2022.bigscience-1.9/ |accessdate=2022-12-19 }}</ref> || 825 GiB<ref name=\"Pile\"/> || {{yes|Apache 2.0}}\n| based on the Megatron architecture\n|-\n| [[Chinchilla AI|Chinchilla]] || {{dts|March 2022}} || [[DeepMind]] || {{sort|70000000000|70 billion}}<ref name=chinchilla-blog/> || {{sort|1400000000000|1.4 trillion}} tokens<ref name=chinchilla-blog/><ref name=hoffman>{{cite arXiv |last1=Hoffmann |first1=Jordan |last2=Borgeaud |first2=Sebastian |last3=Mensch |first3=Arthur |last4=Buchatskaya |first4=Elena |last5=Cai |first5=Trevor |last6=Rutherford |first6=Eliza |last7=Casas |first7=Diego de Las |last8=Hendricks |first8=Lisa Anne |last9=Welbl |first9=Johannes |last10=Clark |first10=Aidan |last11=Hennigan |first11=Tom |last12=Noland |first12=Eric |last13=Millican |first13=Katie |last14=Driessche |first14=George van den |last15=Damoc |first15=Bogdan |last16=Guy |first16=Aurelia |last17=Osindero |first17=Simon |last18=Simonyan |first18=Karen |last19=Elsen |first19=Erich |last20=Rae |first20=Jack W. |last21=Vinyals |first21=Oriol |last22=Sifre |first22=Laurent |title=Training Compute-Optimal Large Language Models |eprint=2203.15556 |date=29 March 2022 |class=cs.CL |display-authors=3}}</ref> || {{no|Proprietary}}\n| Reduced-parameter model trained on more data. Used in the [[Sparrow (bot)|Sparrow]] bot.\n|-\n| [[PaLM]] (Pathways Language Model) || {{dts|April 2022}} || Google || {{sort|540000000000|540 billion}}<ref name=palm-blog/> || {{sort|768000000000|768 billion}} tokens<ref name=chinchilla-blog/> || {{no|Proprietary}}\n| aimed to reach the practical limits of model scale\n|-\n| OPT (Open Pretrained Transformer) || {{dts|May 2022}} || [[Meta Platforms|Meta]] || {{sort|175000000000|175 billion}}<ref>{{cite web |title=Democratizing access to large-scale language models with OPT-175B |url=https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/ |website=ai.facebook.com |language=en}}</ref> || {{sort|180000000000|180 billion}} tokens<ref>{{cite arXiv |last1=Zhang |first1=Susan |last2=Roller |first2=Stephen |last3=Goyal |first3=Naman |last4=Artetxe |first4=Mikel |last5=Chen |first5=Moya |last6=Chen |first6=Shuohui |last7=Dewan |first7=Christopher |last8=Diab |first8=Mona |last9=Li |first9=Xian |last10=Lin |first10=Xi Victoria |last11=Mihaylov |first11=Todor |last12=Ott |first12=Myle |last13=Shleifer |first13=Sam |last14=Shuster |first14=Kurt |last15=Simig |first15=Daniel |last16=Koura |first16=Punit Singh |last17=Sridhar |first17=Anjali |last18=Wang |first18=Tianlu |last19=Zettlemoyer |first19=Luke |title=OPT: Open Pre-trained Transformer Language Models |eprint=2205.01068 |date=21 June 2022|class=cs.CL }}</ref> || {{partial success|Non-commercial research}}{{efn|The smaller models including 66B are publicly available, while the 175B model is available on request.}}\n| GPT-3 architecture with some adaptations from Megatron\n|-\n|YaLM 100B\n|{{dts|June 2022}}\n|[[Yandex]]\n|{{sort|100000000000|100 billion}}<ref name=\"yalm-repo\">{{Citation |last1=Khrushchev |first1=Mikhail |title=YaLM 100B |date=2022-06-22 |url=https://github.com/yandex/YaLM-100B |access-date=2023-03-18 |last2=Vasilev |first2=Ruslan |last3=Petrov |first3=Alexey |last4=Zinov |first4=Nikolay}}</ref>\n|1.7TB<ref name=\"yalm-repo\" />\n|{{Yes|Apache 2.0}}\n|English-Russian model based on Microsoft's Megatron-LM.\n|-\n| Minerva || {{dts|June 2022}} || Google || {{sort|540000000000|540 billion}}<ref name=minerva-paper/> || 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server<ref name=minerva-paper>{{cite arXiv |last1=Lewkowycz |first1=Aitor |last2=Andreassen |first2=Anders |last3=Dohan |first3=David |last4=Dyer |first4=Ethan |last5=Michalewski |first5=Henryk |last6=Ramasesh |first6=Vinay |last7=Slone |first7=Ambrose |last8=Anil |first8=Cem |last9=Schlag |first9=Imanol |last10=Gutman-Solo |first10=Theo |last11=Wu |first11=Yuhuai |last12=Neyshabur |first12=Behnam |last13=Gur-Ari |first13=Guy |last14=Misra |first14=Vedant |title=Solving Quantitative Reasoning Problems with Language Models |date=30 June 2022 |class=cs.CL |eprint=2206.14858 }}</ref>  || {{no|Proprietary}}\n| LLM trained for solving \"mathematical and scientific questions using step-by-step reasoning\".<ref>{{cite web |title=Minerva: Solving Quantitative Reasoning Problems with Language Models |url=https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html |website=ai.googleblog.com |date=30 June 2022 |access-date=20 March 2023 |language=en}}</ref> Minerva is based on PaLM model, further trained on mathematical and scientific data.\n|-\n| [[BLOOM (language model)|BLOOM]] || {{dts|July 2022}} || Large collaboration led by [[Hugging Face]] || {{sort|175000000000|175 billion}}<ref name=bigger-better/> || {{sort|350000000000|350 billion}} tokens (1.6TB)<ref>{{cite web |title=bigscience/bloom \u00b7 Hugging Face |url=https://huggingface.co/bigscience/bloom |website=huggingface.co}}</ref> || {{yes|Responsible AI}}\n| Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\n|-\n| Galactica || {{dts|November 2022}} || [[Meta Platforms|Meta]] || {{sort|120000000000|120 billion}} || {{sort|350000000000|106 billion}} tokens<ref>{{cite arXiv |last1=Taylor |first1=Ross |last2=Kardas |first2=Marcin |last3=Cucurull |first3=Guillem |last4=Scialom |first4=Thomas |last5=Hartshorn |first5=Anthony |last6=Saravia |first6=Elvis |last7=Poulton |first7=Andrew |last8=Kerkez |first8=Viktor |last9=Stojnic |first9=Robert |title=Galactica: A Large Language Model for Science |date=16 November 2022 |class=cs.CL |eprint=2211.09085 }}</ref> || {{partial success|CC-BY-NC-4.0}}\n| Trained on scientific text and modalities.\n|-\n| AlexaTM (Teacher Models) || {{dts|November 2022}} || [[Amazon (company)|Amazon]] || {{sort|20000000000|20 billion}}<ref>{{cite web |title=20B-parameter Alexa model sets new marks in few-shot learning |url=https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning |website=Amazon Science |language=en |date=2 August 2022}}</ref> || {{sort|1300000000000|1.3 trillion}}<ref>{{cite arXiv |last1=Soltan |first1=Saleh |last2=Ananthakrishnan |first2=Shankar |last3=FitzGerald |first3=Jack |last4=Gupta |first4=Rahul |last5=Hamza |first5=Wael |last6=Khan |first6=Haidar |last7=Peris |first7=Charith |last8=Rawls |first8=Stephen |last9=Rosenbaum |first9=Andy |last10=Rumshisky |first10=Anna |last11=Prakash |first11=Chandana Satya |last12=Sridhar |first12=Mukund |last13=Triefenbach |first13=Fabian |last14=Verma |first14=Apurv |last15=Tur |first15=Gokhan |last16=Natarajan |first16=Prem |display-authors=3|title=AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model |eprint=2208.01448 |date=3 August 2022|class=cs.CL }}</ref> || {{partial success|public web API}}<ref>{{cite web |title=AlexaTM 20B is now available in Amazon SageMaker JumpStart {{!}} AWS Machine Learning Blog |url=https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/ |website=aws.amazon.com |access-date=13 March 2023 |date=17 November 2022}}</ref>\n| bidirectional sequence-to-sequence architecture\n|-\n| [[LLaMA]] (Large Language Model Meta AI) || {{dts|February 2023}} || [[Meta Platforms|Meta]] || {{sort|65000000000|65 billion}}<ref name=llama-blog/> || {{sort|1400000000000|1.4 trillion}}<ref name=llama-blog/> || {{partial success|Non-commercial research}}{{efn|Facebook's license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.}}\n| Trained on a large 20-language corpus to aim for better performance with fewer parameters.<ref name=llama-blog/> Researchers from Stanford University trained a fine-tuned model based on LLaMA weights, called Alpaca.<ref>{{Cite web|url=https://crfm.stanford.edu/2023/03/13/alpaca.html|title=Stanford CRFM|website=crfm.stanford.edu}}</ref>\n|-\n| [[GPT-4]] || {{dts|March 2023}} || OpenAI || Exact number unknown, approximately {{sort|1000000000000|1 trillion}} {{efn|As stated in Technical report: \"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\"<ref name=\"GPT4Tech\">{{Cite web |date=2023 |title=GPT-4 Technical Report |url=https://cdn.openai.com/papers/gpt-4.pdf |website=[[OpenAI]] |access-date=March 14, 2023 |archive-date=March 14, 2023 |archive-url=https://web.archive.org/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf |url-status=live }} </ref> Approximate number in the comparison chart that compares the relative storage, from the same report.}} || Unknown || {{partial success|public web API}}\n| Available for ChatGPT Plus users and used in [[GPT-4#Usage|several products]].\n|-\n|Cerebras-GPT\n|{{dts|March 2023}}\n|Cerebras\n|{{sort|13000000000|13 billion}}<ref>{{Cite web|url=https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/|title=Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models|first=Nolan|last=Dey|date=March 28, 2023|website=Cerebras}}</ref>\n| || {{yes|Apache 2.0}}\n| Trained with Chinchilla formula.\n|-\n| Falcon || {{dts|March 2023}} || [[Technology Innovation Institute]] || {{sort|40000000000|40 billion}}<ref name='falcon'>{{cite web |title=Abu Dhabi-based TII launches its own version of ChatGPT |url=https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/ |website=tii.ae}}</ref> || {{sort|1000000000000|1 Trillion}} tokens (1TB)<ref name='falcon'/> || {{yes|Apache 2.0}}<ref>[https://www.businesswire.com/news/home/20230531005608/en/UAE\u2019s-Falcon-40B-World\u2019s-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free UAE\u2019s Falcon 40B, World\u2019s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free], 31 May 2023</ref>\n| The model is claimed to use only 75% of GPT-3's training compute, 40% of Chinchilla's, and 80% of PaLM-62B's.\n|-\n| BloombergGPT || {{dts|March 2023}} || [[Bloomberg L.P.]] || {{sort|50000000000|50 billion}} || 363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets<ref>{{Cite journal|title=BloombergGPT: A Large Language Model for Finance|first1=Shijie|last1=Wu|first2=Ozan|last2=Irsoy|first3=Steven|last3=Lu|first4=Vadim|last4=Dabravolski|first5=Mark|last5=Dredze|first6=Sebastian|last6=Gehrmann|first7=Prabhanjan|last7=Kambadur|first8=David|last8=Rosenberg|first9=Gideon|last9=Mann|date=March 30, 2023|arxiv=2303.17564}}</ref> || {{no|Proprietary}}\n| LLM trained on financial data from proprietary sources, that \"outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks\" \n|-\n| PanGu-\u03a3 || {{dts|March 2023}} || [[Huawei]] || {{sort|1085000000000|1.085 trillion}} || 329 billion tokens<ref>{{Cite journal|title=PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing|first1=Xiaozhe|last1=Ren|first2=Pingyi|last2=Zhou|first3=Xinfan|last3=Meng|first4=Xinjing|last4=Huang|first5=Yadao|last5=Wang|first6=Weichao|last6=Wang|first7=Pengfei|last7=Li|first8=Xiaoda|last8=Zhang|first9=Alexander|last9=Podolskiy|first10=Grigory|last10=Arshinov|first11=Andrey|last11=Bout|first12=Irina|last12=Piontkovskaya|first13=Jiansheng|last13=Wei|first14=Xin|last14=Jiang|first15=Teng|last15=Su|first16=Qun|last16=Liu|first17=Jun|last17=Yao|date=March 19, 2023|arxiv=2303.10845}}</ref> || {{no|Proprietary}}\n| \n|-\n| OpenAssistant<ref>{{Cite arXiv |last1=K\u00f6pf |first1=Andreas |last2=Kilcher |first2=Yannic |last3=von R\u00fctte |first3=Dimitri |last4=Anagnostidis |first4=Sotiris |last5=Tam |first5=Zhi-Rui |last6=Stevens |first6=Keith |last7=Barhoum |first7=Abdullah |last8=Duc |first8=Nguyen Minh |last9=Stanley |first9=Oliver |last10=Nagyfi |first10=Rich\u00e1rd |last11=ES |first11=Shahul |last12=Suri |first12=Sameer |last13=Glushkov |first13=David |last14=Dantuluri |first14=Arnav |last15=Maguire |first15=Andrew |date=2023-04-14 |title=OpenAssistant Conversations -- Democratizing Large Language Model Alignment |class=cs.CL |eprint=2304.07327 }}</ref> || {{dts|March 2023}} || [[LAION]] || {{sort|17000000000|17 billion}} || 1.5 trillion tokens || {{yes|Apache 2.0}}\n| Trained on crowdsourced open data\n|-\n| [[PaLM|PaLM 2]] (Pathways Language Model 2) || {{dts|May 2023}} || Google || {{sort|340000000000|340 billion}}<ref name=\"cnbc-20230516\">{{cite web |last=Elias |first=Jennifer |url=https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html |title=Google's newest A.I. model uses nearly five times more text data for training than its predecessor |work=[[CNBC]] |date=16 May 2023 |access-date=18 May 2023}}</ref> || {{sort|3600000000000|3.6 trillion}} tokens<ref name=\"cnbc-20230516\" /> || {{no|Proprietary}}\n| Used in [[Bard (chatbot)|Bard chatbot]].<ref>{{Cite web|url=https://blog.google/technology/ai/google-palm-2-ai-large-language-model/|title=Introducing PaLM 2|date=May 10, 2023|website=Google}}</ref>\n|}\n\n==See also==\n* [[Foundation models]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|refs=\n<ref name=jm>{{cite book\n|last1=Jurafsky |first1=Dan |last2=Martin |first2=James H. \n|title=Speech and Language Processing \n|date=7 January 2023 |edition=3rd edition draft\n|url=https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf\n|access-date=24 May 2022 \n}}</ref>\n<ref name=extracting>{{Cite conference \n|last1=Carlini |first1=Nicholas |last2=Tramer |first2=Florian |last3=Wallace |first3=Eric |last4=Jagielski |first4=Matthew |last5=Herbert-Voss |first5=Ariel |last6=Lee |first6=Katherine |last7=Roberts |first7=Adam |last8=Brown |first8=Tom B |last9=Song |first9=Dawn |last10=Erlingsson |first10=Ulfar\n|title=Extracting Training Data from Large Language Models\n|conference=USENIX Security Symposium |volume=6 |year=2021\n|url=https://www.usenix.org/system/files/sec21-carlini-extracting.pdf\n}}</ref>\n<ref name=Manning-2022>{{cite journal\n|last=Manning|first=Christopher D.|author-link=Christopher D. Manning\n|title=Human Language Understanding & Reasoning\n|journal=Daedalus\n|year=2022\n|volume=151 |issue=2 |pages=127\u2013138 |doi=10.1162/daed_a_01905 |s2cid=248377870 |url=https://www.amacad.org/publication/human-language-understanding-reasoning\n}}</ref>\n<ref name=Wiggers>{{cite web\n|work=TechCrunch\n|last=Wiggers|first=Kyle\n|date=28 April 2022\n|title=The emerging types of language models and why they matter\n|url=https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/\n}}</ref>\n<ref name=kaplan-scaling>{{Cite journal |last1=Kaplan |first1=Jared |last2=McCandlish |first2=Sam |last3=Henighan |first3=Tom |last4=Brown |first4=Tom B. |last5=Chess |first5=Benjamin |last6=Child |first6=Rewon |last7=Gray |first7=Scott |last8=Radford |first8=Alec |last9=Wu |first9=Jeffrey |last10=Amodei |first10=Dario |title=Scaling Laws for Neural Language Models |journal=CoRR |volume=abs/2001.08361 |year=2020 |arxiv=2001.08361}}</ref>\n<ref name=bigger-better>{{cite journal\n|journal=Nature\n|last=Ananthaswamy|first=Anil\n|title=In AI, is bigger always better?\n|date=8 March 2023\n|volume=615 |issue=7951 |pages=202\u2013205 |doi=10.1038/d41586-023-00641-w |pmid=36890378 |bibcode=2023Natur.615..202A |s2cid=257380916 |url=https://www.nature.com/articles/d41586-023-00641-w\n}}</ref>\n<ref name=\"few-shot-learners\">{{cite journal |first1=Tom B.|last1=Brown|first2=Benjamin|last2=Mann|last3=Ryder|last25=Chess|last20=Hesse|first20=Christopher|last21=Chen|first21=Mark|last22=Sigler|first22=Eric|last23=Litwin|first23=Mateusz|last24=Gray|first24=Scott|first26=Jack|first25=Benjamin|last26=Clark|last19=Winter|last27=Berner|first27=Christopher|last28=McCandlish|first28=Sam|last29=Radford|first29=Alec|last30=Sutskever|first30=Ilya|last31=Amodei|first31=Dario|first19=Clemens|first18=Jeffrey|first3=Nick|last10=Askell|last4=Subbiah|first4=Melanie|last5=Kaplan|first5=Jared|last6=Dhariwal|first6=Prafulla|last7=Neelakantan|first7=Arvind|last8=Shyam|first8=Pranav|last9=Sastry|first9=Girish|first10=Amanda|last18=Wu|last11=Agarwal|first11=Sandhini|last12=Herbert-Voss|first12=Ariel|last13=Krueger|first13=Gretchen|last14=Henighan|first14=Tom|last15=Child|first15=Rewon|last16=Ramesh|first16=Aditya|last17=Ziegler|first17=Daniel M. |editor1-last=Larochelle |editor1-first=H. |editor2-last=Ranzato |editor2-first=M. |editor3-last=Hadsell |editor3-first=R. |editor4-last=Balcan |editor4-first=M.F. |editor5-last=Lin |editor5-first=H. |title=Language Models are Few-Shot Learners |journal=Advances in Neural Information Processing Systems |date=Dec 2020 |volume=33 |pages=1877\u20131901 |url=https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf |publisher=Curran Associates, Inc.}}</ref>\n<ref name=flan-blog>{{cite web\n|title=Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning\n|date=6 October 2021\n|work=Google Research\n|last1=Bosma|first1=Maarten|last2=Wei|first2=Jason\n|url=https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html\n}}</ref>\n<ref name=self-instruct-paper>{{Cite arXiv |last1=Wang |first1=Yizhong |last2=Kordi |first2=Yeganeh |last3=Mishra |first3=Swaroop |last4=Liu |first4=Alisa |last5=Smith |first5=Noah A. |last6=Khashabi |first6=Daniel |last7=Hajishirzi |first7=Hannaneh |title=Self-Instruct: Aligning Language Model with Self Generated Instructions |eprint=2212.10560 |date=2022|class=cs.CL }}</ref>\n<ref name=instructGPT-paper>{{Cite arXiv |last1=Ouyang |first1=Long |last2=Wu |first2=Jeff |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |eprint=2203.02155 |date=2022|class=cs.CL }}</ref>\n<ref name=boolq>{{cite arXiv|eprint=1905.10044 |last1=Clark |first1=Christopher |last2=Lee |first2=Kenton |last3=Chang |first3=Ming-Wei |last4=Kwiatkowski |first4=Tom |last5=Collins |first5=Michael |last6=Toutanova |first6=Kristina |title=BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions |year=2019 |class=cs.CL }}</ref>\n<ref name=hellaswag>{{cite arXiv|eprint=1905.07830 |last1=Zellers |first1=Rowan |last2=Holtzman |first2=Ari |last3=Bisk |first3=Yonatan |last4=Farhadi |first4=Ali |last5=Choi |first5=Yejin |title=HellaSwag: Can a Machine Really Finish Your Sentence? |year=2019 |class=cs.CL }}</ref>\n<ref name=truthfulqa>{{cite arXiv|eprint=2109.07958 |last1=Lin |first1=Stephanie |last2=Hilton |first2=Jacob |last3=Evans |first3=Owain |title=TruthfulQA: Measuring How Models Mimic Human Falsehoods |year=2021 |class=cs.CL }}</ref>\n<ref name=bigbench>{{cite arXiv|eprint=2206.04615 |last1=Srivastava |first1=Aarohi |last2=Rastogi |first2=Abhinav |last3=Rao |first3=Abhishek |author4=Abu Awal Md Shoeb |last5=Abid |first5=Abubakar |last6=Fisch |first6=Adam |last7=Brown |first7=Adam R. |last8=Santoro |first8=Adam |last9=Gupta |first9=Aditya |last10=Garriga-Alonso |first10=Adri\u00e0 |last11=Kluska |first11=Agnieszka |last12=Lewkowycz |first12=Aitor |last13=Agarwal |first13=Akshat |last14=Power |first14=Alethea |last15=Ray |first15=Alex |last16=Warstadt |first16=Alex |last17=Kocurek |first17=Alexander W. |last18=Safaya |first18=Ali |last19=Tazarv |first19=Ali |last20=Xiang |first20=Alice |last21=Parrish |first21=Alicia |last22=Nie |first22=Allen |last23=Hussain |first23=Aman |last24=Askell |first24=Amanda |last25=Dsouza |first25=Amanda |last26=Slone |first26=Ambrose |last27=Rahane |first27=Ameet |last28=Iyer |first28=Anantharaman S. |last29=Andreassen |first29=Anders |last30=Madotto |first30=Andrea |title=Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models |year=2022 |class=cs.CL |display-authors=1 }}</ref>\n<ref name=survey>{{cite arXiv|eprint=2303.18223 |author1=Wayne Xin Zhao |last2=Zhou |first2=Kun |last3=Li |first3=Junyi |last4=Tang |first4=Tianyi |last5=Wang |first5=Xiaolei |last6=Hou |first6=Yupeng |last7=Min |first7=Yingqian |last8=Zhang |first8=Beichen |last9=Zhang |first9=Junjie |last10=Dong |first10=Zican |last11=Du |first11=Yifan |last12=Yang |first12=Chen |last13=Chen |first13=Yushuo |last14=Chen |first14=Zhipeng |last15=Jiang |first15=Jinhao |last16=Ren |first16=Ruiyang |last17=Li |first17=Yifan |last18=Tang |first18=Xinyu |last19=Liu |first19=Zikang |last20=Liu |first20=Peiyu |last21=Nie |first21=Jian-Yun |last22=Wen |first22=Ji-Rong |title=A Survey of Large Language Models |year=2023 |class=cs.CL }}</ref>\n<ref name=Huyen>{{cite web\n|work=The Gradient\n|last=Huyen|first=Chip\n|title=Evaluation Metrics for Language Modeling\n|date=18 October 2019\n|url=https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n}}</ref>\n\n<!-- Refs below are specific to the \"List of large language models\" section. (Keeping separate in case that section is split off into a standalone list article in the future.) -->\n<ref name=palm-blog>{{Cite web |last1=Narang |first1=Sharan |last2=Chowdhery |first2=Aakanksha |date=April 4, 2022 |title=Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance |url=https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html |access-date=2023-03-09 |website=ai.googleblog.com |language=en\n}}</ref>\n<ref name=glam-blog>{{Cite web |last1=Dai |first1=Andrew M |last2=Du |first2=Nan |date=December 9, 2021 |title=More Efficient In-Context Learning with GLaM |url=https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html |access-date=2023-03-09 |website=ai.googleblog.com |language=en}}</ref>\n<ref name=lamda-blog>{{Cite web |last1=Cheng |first1=Heng-Tze |last2=Thoppilan |first2=Romal |date=January 21, 2022 |title=LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything |url=https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html |access-date=2023-03-09 |website=ai.googleblog.com |language=en}}</ref>\n<ref name=mtnlg-preprint>{{Cite preprint |last1=Smith |first1=Shaden |last2=Patwary |first2=Mostofa |last3=Norick |first3=Brandon |last4=LeGresley |first4=Patrick |last5=Rajbhandari |first5=Samyam |last6=Casper |first6=Jared |last7=Liu |first7=Zhun |last8=Prabhumoye |first8=Shrimai |last9=Zerveas |first9=George |last10=Korthikanti |first10=Vijay |last11=Zhang |first11=Elton |last12=Child |first12=Rewon |last13=Aminabadi |first13=Reza Yazdani |last14=Bernauer |first14=Julie |last15=Song |first15=Xia |date=2022-02-04 |title=Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model |arxiv=2201.11990 }}</ref>\n<ref name=llama-blog>{{cite web\n|work=Meta AI\n|title=Introducing LLaMA: A foundational, 65-billion-parameter large language model\n|date=24 February 2023\n|url=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n}}</ref>\n<ref name=\"15Brelease\">{{Cite web\n |url          = https://openai.com/blog/gpt-2-1-5b-release/\n |title        = GPT-2: 1.5B Release\n |date         = 2019-11-05\n |website      = OpenAI\n |language     = en\n |access-date  = 2019-11-14\n |archive-date = 2019-11-14\n |archive-url  = https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2-1-5b-release/\n |url-status   = live\n}}</ref>\n<ref name=chinchilla-blog>{{cite web\n|work=Deepmind Blog\n|title=An empirical analysis of compute-optimal large language model training\n|first1=Jordan|last1=Hoffmann|first2=Sebastian|last2=Borgeaud\n|first3=Arthur|last3=Mensch|first4=Laurent|last4=Sifre\n|date=12 April 2022\n|url=https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training\n}}</ref>\n<ref name=bert-paper>{{cite arXiv |last1=Devlin |first1=Jacob |last2=Chang |first2=Ming-Wei |last3=Lee |first3=Kenton |last4=Toutanova |first4=Kristina |title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding |date=11 October 2018 |eprint=1810.04805v2|class=cs.CL }}</ref>\n<ref name=chatgpt-blog>{{Cite web |date=2022-11-30 |title=ChatGPT: Optimizing Language Models for Dialogue |url=https://openai.com/blog/chatgpt/ |access-date=2023-01-13 |website=OpenAI |language=en}}</ref>\n<ref name=vb-gpt-neo>{{cite web\n|work=VentureBeat\n|last=Iyer|first=Abhishek\n|title=GPT-3's free alternative GPT-Neo is something to be excited about\n|date=15 May 2021\n|url=https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/\n}}</ref>\n}}\n\n{{Natural language processing}}\n\n[[Category:Large language models| ]]\n[[Category:Deep learning]]\n[[Category:Natural language processing]]"}