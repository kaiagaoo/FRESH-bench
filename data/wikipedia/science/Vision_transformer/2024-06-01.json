{"title": "Vision transformer", "page_id": 68212199, "revision_id": 1222137990, "revision_timestamp": "2024-05-04T03:31:02Z", "content": "{{Short description|Machine learning algorithm for vision processing}}\nA '''vision transformer''' ('''ViT''') is a [[Transformer (machine learning model)|transformer]] designed for computer vision.<ref name=\":3\">{{cite arXiv |eprint=2010.11929 |class=cs.CV |first1=Alexey |last1=Dosovitskiy |first2=Lucas |last2=Beyer |title=An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale |date=2021-06-03 |last3=Kolesnikov |first3=Alexander |last4=Weissenborn |first4=Dirk |last5=Zhai |first5=Xiaohua |last6=Unterthiner |first6=Thomas |last7=Dehghani |first7=Mostafa |last8=Minderer |first8=Matthias |last9=Heigold |first9=Georg |last10=Gelly |first10=Sylvain |last11=Uszkoreit |first11=Jakob}}</ref> A ViT breaks down an input image into a series of patches (rather than breaking up text into [[Byte pair encoding|tokens]]), serialises each patch into a vector, and maps it to a smaller dimension with a single [[matrix multiplication]]. These vector embeddings are then processed by a [[BERT (language model)|transformer encoder]] as if they were token embeddings.\n\nViT has found applications in [[image recognition]], [[image segmentation]], and [[autonomous driving]].<ref name=\":0\">{{Cite web |last=Sarkar |first=Arjun |date=2021-05-20 |title=Are Transformers better than CNN's at Image Recognition? |url=https://towardsdatascience.com/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8 |access-date=2021-07-11 |website=Medium |language=en}}</ref>\n\n== History ==\nTransformers were introduced in 2017, in a paper \"[[Attention Is All You Need]]\",<ref>{{cite journal |last1=Vaswani |first1=Ashish |author1-link= Ashish Vaswani |last2=Shazeer |first2=Noam |last3=Parmar |first3=Niki |last4=Uszkoreit |first4=Jakob |last5=Jones |first5=Llion |last6=Gomez |first6=Aidan N  |author6-link= Aidan Gomez |last7=Kaiser |first7=\u0141ukasz |last8=Polosukhin |first8=Illia |title=Attention is All you Need |journal=Advances in Neural Information Processing Systems |date=2017 |volume=30 |url=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf |publisher=Curran Associates, Inc.}}</ref> and have found widespread use in [[natural language processing]]. In 2020, they were adapted for computer vision, yielding ViT.<ref name=\":3\"/> \n\nIn 2021 a pure transformer model demonstrated better performance and greater efficiency than CNNs on image classification.<ref name=\":0\" />\n\nA study in June 2021 added a transformer backend to [[Residual neural network|ResNet]], which dramatically reduced costs and increased accuracy.<ref name=\":1\" /><ref>{{Cite arXiv|eprint=2006.03677|class=cs.CV|first1=Bichen|last1=Wu|first2=Chenfeng|last2=Xu|title=Visual Transformers: Token-based Image Representation and Processing for Computer Vision |last3=Dai|first8=Joseph|year=2020|last10=Vajda|first10=Peter|first9=Kurt|last9=Keutzer|last8=Gonzalez|first3=Xiaoliang|first7=Tomizuka|first6=Zhicheng|last6=Yan|first5=Peizhao|last5=Zhang|first4=Alvin|last4=Wan|last7=Masayoshi}}</ref><ref name=\":2\" />\n\nIn the same year, some important variants of the Vision Transformers were proposed. These variants are mainly intended to be more efficient, more accurate or better suited to a specific domain. Among the most relevant is the Swin Transformer,<ref name=\":4\">{{cite arXiv|last1=Liu|first1=Ze|last2=Lin|first2=Yutong|last3=Cao|first3=Yue|last4=Hu|first4=Han|last5=Wei|first5=Yixuan|last6=Zhang|first6=Zheng|last7=Lin|first7=Stephen|last8=Guo|first8=Baining|date=2021-03-25|title=Swin Transformer: Hierarchical Vision Transformer using Shifted Windows|class=cs.CV |eprint=2103.14030|language=en}}</ref> which through some modifications to the attention mechanism and a multi-stage approach achieved state-of-the-art results on some object detection datasets such as [[COCO (dataset)|COCO]]. Another interesting variant is the TimeSformer, designed for video understanding tasks and able to capture spatial and temporal information through the use of divided space-time attention.<ref>{{cite arXiv|last1=Bertasius|first1=Gedas|last2=Wang|first2=Heng|last3=Torresani|first3=Lorenzo|date=2021-02-09|title=Is Space-Time Attention All You Need for Video Understanding?|class=cs.CV |eprint=2102.05095|language=en}}</ref><ref>{{cite web|last=Coccomini|first=Davide|date=2021-03-31|title=On Transformers, TimeSformers, and Attention. An exciting revolution from text to videos|url=https://towardsdatascience.com/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b|url-access=subscription|website=Towards Data Science}}</ref>\n\n== Overview ==\nThe basic architecture, used by the original 2020 paper,<ref name=\":3\" /> is as follows. In summary, it is a BERT-like encoder-only Transformer.\n\nThe input image is of type <math>\\R^{H\\times W \\times C}</math>, where <math>H, W, C</math> are height, width, channel ([[RGB color model|RGB]]). It is then split into square-shaped patches of type <math>\\R^{P\\times P \\times C}</math>. \n\nFor each patch, the patch is pushed through a linear operator, to obtain a vector (\"patch embedding\"). The position of the patch is also transformed into a vector by \"position encoding\". The two vectors are added, then pushed through several Transformer encoders.\n\nThe attention mechanism in a ViT repeatedly transforms representation vectors of image patches, incorporating more and more semantic relations between image patches in an image. This is analogous to how in natural language processing, as representation vectors flow through a transformer, they incorporate more and more semantic relations between words, from syntax to semantics.\n\nThe above architecture turns an image into a sequence of vector representations. To use these for downstream applications, an additional head needs to be trained to interpret them.\n\nFor example, to use it for classification, one can add a shallow MLP on top of it that outputs a probability distribution over classes. The original paper uses a linear-[[Activation function|GeLU]]-linear-softmax network.<ref name=\":3\" />\n\n== Variants ==\n=== Original ViT ===\n[[File:Vision Transformer.gif|thumb|450x450px|Vision Transformer Architecture for Image Classification]]\nTransformers found their initial applications in [[natural language processing]] tasks, as demonstrated by [[language models]] such as [[BERT (language model)|BERT]] and [[GPT-3]]. By contrast the typical image processing system uses a [[convolutional neural network]] (CNN). Well-known projects include Xception, [[Residual neural network|ResNet]], EfficientNet,<ref>{{cite journal |last1=Tan |first1=Mingxing |last2=Le |first2=Quoc |date=23 June 2021 |title=EfficientNetV2: Smaller Models and Faster Training |url=https://proceedings.mlr.press/v139/tan21a/tan21a.pdf |journal=Proceedings of the 38th International Conference on Machine Learning (PMLR) |volume=139 |issue=<!--not supplied--> |pages=10096\u201310106 |doi=<!--not supplied--> |arxiv=2104.00298 |access-date=31 October 2023}}</ref> DenseNet,<ref>{{Cite arXiv |last1=Huang|first1=Gao|last2=Liu|first2=Zhuang|last3=van der Maaten|first3=Laurens|last4=Q. Weinberger|first4=Kilian|date= 28 Jan 2018|title=Densely Connected Convolutional Networks |class=cs.CV|eprint=1608.06993}}</ref> and Inception.<ref name=\":0\" />\n\nTransformers measure the relationships between pairs of input tokens (words in the case of text strings), termed [[Attention (machine learning)|attention]]. The cost is quadratic in the number of tokens. For images, the basic unit of analysis is the [[pixel]]. However, computing relationships for every pixel pair in a typical image is prohibitive in terms of memory and computation. Instead, ViT computes relationships among pixels in various small sections of the image (e.g., 16x16 pixels), at a drastically reduced cost. The sections (with positional embeddings) are placed in a sequence. The embeddings are learnable vectors. Each section is arranged into a linear sequence and multiplied by the embedding matrix. The result, with the position embedding is fed to the transformer.<ref name=\":0\" />\n\nAs in the case of [[BERT (language model)|BERT]], a fundamental role in classification tasks is played by the class token. A special token that is used as the only input of the final [[Multilayer perceptron|MLP]] Head as it has been influenced by all the others.\n\nThe architecture for image classification is the most common and uses only the Transformer Encoder in order to transform the various input tokens. However, there are also other applications in which the decoder part of the traditional Transformer Architecture is also used.\n\n=== Masked Autoencoder ===\nIn Masked Autoencoder, there are two ViTs put end-to-end. The first one takes in image patches with positional encoding, and outputs vectors representing each patch. The second one takes in vectors with positional encoding and outputs image patches again. During training, both ViTs are used. An image is cut into patches, and only 25% of the patches are put into the first ViT. The second ViT takes the encoded vectors and outputs a reconstruction of the full image. During use, only the first ViT is used.<ref>{{Cite arXiv |last1=He |first1=Kaiming |last2=Chen |first2=Xinlei |last3=Xie |first3=Saining |last4=Li |first4=Yanghao |last5=Doll\u00e1r |first5=Piotr |last6=Girshick |first6=Ross |date=2021 |title=Masked Autoencoders Are Scalable Vision Learners |class=cs.CV |eprint=2111.06377}}</ref>\n\n=== Swin Transformer ===\nThe Swin Transformer (\"'''S'''hifted '''win'''dows\")<ref name=\":4\" /> takes inspiration from standard convolutional neural networks:\n\n* Instead of performing self-attention over the entire sequence of tokens, one for each patch, it performs \"shifted window based\" self-attention, which means only performing attention over square-shaped blocks of patches. One block of patches is analogous to the receptive field of one convolution.\n* After every few attention blocks, there is a \"merge layer\", which merges neighboring 2x2 tokens into a single token. This is analogous to pooling (by 2x2 convolution kernels, with stride 2). Merging means concatenation followed by multiplication with a matrix.\n\nIt is improved by Swin Transformer V2,<ref>{{Cite web |last1=Liu |first1=Ze |last2=Hu |first2=Han |last3=Lin |first3=Yutong |last4=Yao |first4=Zhuliang |last5=Xie |first5=Zhenda |last6=Wei |first6=Yixuan |last7=Ning |first7=Jia |last8=Cao |first8=Yue |last9=Zhang |first9=Zheng |last10=Dong |first10=Li |last11=Wei |first11=Furu |last12=Guo |first12=Baining | publisher=Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition| date=2022 |title=Swin Transformer V2: Scaling Up Capacity and Resolution |url=https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html |language=en |pages=12009\u201312019}}</ref> which modifies upon the ViT by a different attention mechanism (Figure 1):\n\n* layernorm immediately after each attention and feedforward layer (\"res-post-norm\");\n* scaled cosine attention to replace the original dot product attention;\n* log-spaced continuous [[Transformer (machine learning model)#Alternative positional encodings|relative position bias]], which allows [[transfer learning]] across different window resolutions.\n\n=== ViT-VQGAN ===\nIn ViT-VQGAN,<ref>{{Cite arXiv |last1=Yu |first1=Jiahui |last2=Li |first2=Xin |last3=Koh |first3=Jing Yu |last4=Zhang |first4=Han |last5=Pang |first5=Ruoming |last6=Qin |first6=James |last7=Ku |first7=Alexander |last8=Xu |first8=Yuanzhong |last9=Baldridge |first9=Jason |last10=Wu |first10=Yonghui |date=2021 |title=Vector-quantized Image Modeling with Improved VQGAN |class=cs.CV |eprint=2110.04627}}</ref> there are two ViT encoders and a discriminator. One encodes 8x8 patches of an image into a list of vectors, one for each patch. The vectors can only come from a discrete set of \"codebook\", as in [[vector quantization]]. Another encodes the quantized vectors back to image patches. The training objective attempts to make the reconstruction image (the output image) faithful to the input image. The discriminator (usually a convolutional network, but other networks are allowed) attempts to decide if an image is an original real image, or a reconstructed image by the ViT.\n\nThe idea is essentially the same as vector quantized variational autoencoder (VQVAE) plus [[generative adversarial network]] (GAN).\n\nAfter such a ViT-VQGAN is trained, it can be used to code an arbitrary image into a list of symbols, and code an arbitrary list of symbols into an image. The list of symbols can be used to train into a standard autoregressive transformer (like GPT), for autoregressively generating an image. Further, one can take a list of caption-image pairs, convert the images into strings of symbols, and train a standard GPT-style transformer. Then at test time, one can just give an image caption, and have it autoregressively generate the image. This is the structure of Google Parti.<ref>{{Cite web |title=Parti: Pathways Autoregressive Text-to-Image Model |url=https://sites.research.google/parti/ |access-date=2023-11-03 |website=sites.research.google}}</ref>\n\n== Comparison with Convolutional Neural Networks ==\nDue to the commonly used (comparatively) large patch size, ViT performance depends more heavily on decisions including that of the optimizer, dataset-specific [[Hyperparameter (machine learning)|hyperparameters]], and network depth than convolutional networks. Preprocessing with a layer of smaller-size, overlapping (stride < size) convolutional filters helps with performance and stability.<ref name=\":2\">{{cite arXiv|last1=Xiao|first1=Tete|last2=Singh|first2=Mannat|last3=Mintun|first3=Eric|last4=Darrell|first4=Trevor|last5=Doll\u00e1r|first5=Piotr|last6=Girshick|first6=Ross|date=2021-06-28|title=Early Convolutions Help Transformers See Better|class=cs.CV|eprint=2106.14881}}</ref>\n\nThe CNN translates from the basic pixel level to a feature map. A tokenizer translates the feature map into a series of tokens that are then fed into the transformer, which applies the attention mechanism to produce a series of output tokens. Finally, a projector reconnects the output tokens to the feature map. The latter allows the analysis to exploit potentially significant pixel-level details. This drastically reduces the number of tokens that need to be analyzed, reducing costs accordingly.<ref name=\":1\">{{Cite web|date=2020-06-12|title=Facebook and UC Berkeley Boost CV Performance and Lower Compute Cost With Visual Transformers|url=https://medium.com/syncedreview/facebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers-c019823f0561|access-date=2021-07-11|website=Medium|language=en}}</ref>\n\nThe differences between CNNs and Vision Transformers are many and lie mainly in their architectural differences.\n\nIn fact, CNNs achieve excellent results even with training based on data volumes that are not as large as those required by Vision Transformers.\n\nThis different behaviour seems to derive from the different [[Inductive bias|inductive biases]] they possess. The filter-oriented architecture of CNNs can be somehow exploited by these networks to grasp more quickly the particularities of the analysed images even if, on the other hand, they end up limiting them making it more complex to grasp global relations.<ref>{{cite arXiv|last1=Raghu|first1=Maithra|last2=Unterthiner|first2=Thomas|last3=Kornblith|first3=Simon|last4=Zhang|first4=Chiyuan|last5=Dosovitskiy|first5=Alexey|date=2021-08-19|title=Do Vision Transformers See Like Convolutional Neural Networks?|class=cs.CV |eprint=2108.08810}}</ref><ref>{{cite web|last=Coccomini|first=Davide|date=2021-07-24|title=Vision Transformers or Convolutional Neural Networks? Both!|url=https://towardsdatascience.com/vision-transformers-or-convolutional-neural-networks-both-de1a2c3c62e4|url-access=subscription|website=Towards Data Science}}</ref>\n\nOn the other hand, the Vision Transformers possess a different kind of bias toward exploring topological relationships between patches, which leads them to be able to capture also global and wider range relations but at the cost of a more onerous training in terms of data.\n\nVision Transformers also proved to be much more robust to input image distortions such as adversarial patches or permutations.<ref>{{cite arXiv|last1=Naseer|first1=Muzammal|last2=Ranasinghe|first2=Kanchana|last3=Khan|first3=Salman|last4=Hayat|first4=Munawar|last5=Khan|first5=Fahad Shahbaz|last6=Yang|first6=Ming-Hsuan|date=2021-05-21|title=Intriguing Properties of Vision Transformers|class=cs.CV |eprint=2105.10497|language=en}}</ref>\n\nHowever, choosing one architecture over another is not always the wisest choice, and excellent results have been obtained in several Computer Vision tasks through hybrid architectures combining convolutional layers with Vision Transformers.<ref>{{cite arXiv|last1=Dai|first1=Zihang|last2=Liu|first2=Hanxiao|last3=Le|first3=Quoc V.|last4=Tan|first4=Mingxing|date=2021-06-09|title=CoAtNet: Marrying Convolution and Attention for All Data Sizes|class=cs.CV |eprint=2106.04803|language=en}}</ref><ref>{{cite arXiv|last1=Wu|first1=Haiping|last2=Xiao|first2=Bin|last3=Codella|first3=Noel|last4=Liu|first4=Mengchen|last5=Dai|first5=Xiyang|last6=Yuan|first6=Lu|last7=Zhang|first7=Lei|date=2021-03-29|title=CvT: Introducing Convolutions to Vision Transformers|class=cs.CV |eprint=2103.15808|language=en}}</ref><ref>{{cite book|last1=Coccomini|first1=Davide|last2=Messina|first2=Nicola|last3=Gennaro|first3=Claudio|last4=Falchi|first4=Fabrizio|title=Image Analysis and Processing \u2013 ICIAP 2022|chapter=Combining Efficient ''Net'' and Vision Transformers for Video Deepfake Detection |series=Lecture Notes in Computer Science |year=2022 |volume=13233 |pages=219\u2013229 |doi=10.1007/978-3-031-06433-3_19 |arxiv=2107.02612|isbn=978-3-031-06432-6 |s2cid=235742764 |language=en}}</ref>\n\n== The Role of Self-Supervised Learning ==\nThe considerable need for data during the training phase has made it essential to find alternative methods to train these models,<ref>{{cite web|last=Coccomini|first=Davide|date=2021-07-24|title=Self-Supervised Learning in Vision Transformers|url=https://towardsdatascience.com/self-supervised-learning-in-vision-transformers-30ff9be928c|url-access=subscription|website=Towards Data Science}}</ref> and a central role is now played by [[Self-supervised learning|self-supervised methods]]. Using these approaches, it is possible to train a neural network in an almost autonomous way, allowing it to deduce the peculiarities of a specific problem without having to build a large dataset or provide it with accurately assigned labels. Being able to train a Vision Transformer without having to have a huge vision dataset at its disposal could be the key to the widespread dissemination of this promising new architecture.\n\n== Applications ==\nVision Transformers have been used in many Computer Vision tasks with excellent results and in some cases even state-of-the-art.\n\nAmong the most relevant areas of application are:\n\n* [[Image classification|Image Classification]]\n*[[Object detection|Object Detection]]\n* [[Deepfake|Video Deepfake Detection]]\n* [[Image segmentation]]\n* [[Anomaly detection]]\n* [[Image Synthesis]]\n* [[Cluster analysis]]\n*[[Autonomous driving|Autonomous Driving]]\nVision Transformer-based algorithms such as DINO (self-'''di'''stillation with '''no''' labels) <ref>{{Cite book |last1=Caron |first1=Mathilde |last2=Touvron |first2=Hugo |last3=Misra |first3=Ishan |last4=Jegou |first4=Herve |last5=Mairal |first5=Julien |last6=Bojanowski |first6=Piotr |last7=Joulin |first7=Armand |chapter=Emerging Properties in Self-Supervised Vision Transformers |date=October 2021 |pages=9630\u20139640 |title=2021 IEEE/CVF International Conference on Computer Vision (ICCV) |chapter-url=http://dx.doi.org/10.1109/iccv48922.2021.00951 |publisher=IEEE |doi=10.1109/iccv48922.2021.00951|arxiv=2104.14294 |isbn=978-1-6654-2812-5 }}</ref> also show promising properties on biological datasets such as images generated with the [[Cell Painting]] assay. DINO has been demonstrated to learn image representations which could be used to cluster images and explore morphological profiles in a feature space.<ref>{{Cite journal |last1=Doron |first1=Michael |last2=Moutakanni |first2=Th\u00e9o |last3=Chen |first3=Zitong S. |last4=Moshkov |first4=Nikita |last5=Caron |first5=Mathilde |last6=Touvron |first6=Hugo |last7=Bojanowski |first7=Piotr |last8=Pernice |first8=Wolfgang M. |last9=Caicedo |first9=Juan C. |date=2023-06-18 |title=Unbiased single-cell morphology with self-supervised vision transformers |url=http://dx.doi.org/10.1101/2023.06.16.545359 |access-date=2024-02-12 |journal=bioRxiv : The Preprint Server for Biology|pages=2023.06.16.545359 |doi=10.1101/2023.06.16.545359 |pmid=37398158 |pmc=10312751 }}</ref>\n\n== Implementations ==\nThere are many implementations of Vision Transformers and its variants available in open source online. The main versions of this architecture have been implemented in [[PyTorch]]<ref>{{GitHub|lucidrains/vit-pytorch}}</ref> but implementations have also been made available for [[TensorFlow]].<ref>{{cite web|last=Salama|first=Khalid|date=2021-01-18|title=Image classification with Vision Transformer|url=https://keras.io/examples/vision/image_classification_with_vision_transformer/|website=keras.io}}</ref>\n\n== See also ==\n\n* [[Transformer (machine learning model)]]\n* [[Attention (machine learning)]]\n* [[Perceiver]]\n* [[Deep learning]]\n* [[PyTorch]]\n* [[TensorFlow]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n\n*{{Cite web|last=Igarashi|first=Yoshiyuki|date=2021-02-04|title=Are You Ready for Vision Transformer (ViT)?|url=https://towardsdatascience.com/are-you-ready-for-vision-transformer-vit-c9e11862c539|url-access=subscription|access-date=2021-07-11|website=Medium|language=en}}\n*{{cite web|last=Coccomini|first=Davide|date=2021-05-03|url=https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382|url-access=subscription|title=On DINO, Self-Distillation with no labels|website=Towards Data Science|access-date=2021-10-03}}\n\n[[Category:Artificial neural networks]]\n[[Category:Image processing]]"}