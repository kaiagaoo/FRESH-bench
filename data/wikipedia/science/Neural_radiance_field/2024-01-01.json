{"title": "Neural radiance field", "page_id": 72900340, "revision_id": 1191284563, "revision_timestamp": "2023-12-22T16:21:55Z", "content": "{{Short description|3D reconstruction technique}}\n{{AI-generated|date=December 2023}}\nA '''neural radiance field''' ('''NeRF''') is a method based on [[deep learning]] for [[3D reconstruction|reconstructing a three-dimensional representation]] of a scene from sparse two-dimensional images. The NeRF model can learn the scene geometry, camera [[Pose (computer vision)|poses]], and the [[reflectance]] properties of objects, allowing it to render photorealistic views of the scene from novel viewpoints. First introduced in 2020,<ref name=\":1\" /> it has since gained significant attention for its potential applications in computer graphics and content creation.<ref name=\":0\" />\n\n== Algorithm ==\nThe NeRF encodes the scene as a [[Volume rendering|volumetric function]] optimized by a fully connected [[Deep learning#Deep neural networks|deep neural network]] (DNN). NeRF can predict a volume density and view-dependent emitted radiance given the spatial location (''x, y, z'') and viewing direction in [[Euler angles]] (''\u03b8, \u03a6'') of the camera. By sampling many points along camera rays, traditional [[volume rendering]] techniques can produce an image.<ref name=\":1\">{{Cite book |last1=Mildenhall |first1=Ben |last2=Srinivasan |first2=Pratul P. |last3=Tancik |first3=Matthew |last4=Barron |first4=Jonathan T. |last5=Ramamoorthi |first5=Ravi |last6=Ng |first6=Ren |title=Computer Vision \u2013 ECCV 2020 |chapter=NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis |date=2020 |editor-last=Vedaldi |editor-first=Andrea |editor2-last=Bischof |editor2-first=Horst |editor3-last=Brox |editor3-first=Thomas |editor4-last=Frahm |editor4-first=Jan-Michael |series=Lecture Notes in Computer Science |volume=12346 |language=en |location=Cham |publisher=Springer International Publishing |pages=405\u2013421 |doi=10.1007/978-3-030-58452-8_24 |arxiv=2003.08934 |isbn=978-3-030-58452-8|s2cid=213175590 }}</ref>\n\n=== Data collection ===\nA NeRF needs to be retrained for each unique scene. The first step is to collect images of the scene from different angles and their respective camera pose. This requires tracking of the camera position and orientation, often through some combination of [[Simultaneous localization and mapping|SLAM]], [[Satellite navigation|GPS]], or [[Inertial navigation system|inertial]] estimation. Researchers often use synthetic data to evaluate NeRF and related techniques. For such data, images ([[Rendering (computer graphics)|rendered through traditional non-learned methods]]) and respective camera poses are reproducible and error-free.<ref name=\":2\">{{Cite book |last1=Tancik |first1=Matthew |last2=Weber |first2=Ethan |last3=Ng |first3=Evonne |last4=Li |first4=Ruilong |last5=Yi |first5=Brent |last6=Kerr |first6=Justin |last7=Wang |first7=Terrance |last8=Kristoffersen |first8=Alexander |last9=Austin |first9=Jake |last10=Salahi |first10=Kamyar |last11=Ahuja |first11=Abhik |last12=McAllister |first12=David |last13=Kanazawa |first13=Angjoo |title=Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings |chapter=Nerfstudio: A Modular Framework for Neural Radiance Field Development |date=2023-07-23 |pages=1\u201312 |doi=10.1145/3588432.3591516|arxiv=2302.04264 |isbn=9798400701597 |s2cid=256662551 }}</ref>       \n\n=== Training ===\nFor each sparse viewpoint (image and camera pose) provided, camera [[Ray marching|rays are marched]] through the scene, generating a set of 3D points with a given radiance direction (into the camera). For these points, volume density and emitted radiance are predicted using the MLP. An image is then generated through classical volume rendering. Because this process is fully differentiable, the error between the predicted image and the original image can be minimized with [[gradient descent]] over multiple viewpoints, encouraging the MLP to develop a coherent model of the scene.<ref name=\":1\" />\n\n== Variations and improvements ==\nEarly versions of NeRF were slow to optimize and required that all input views were taken with the same camera in the same lighting conditions. These performed best when limited to orbiting around individual objects, such as a drum set, plants or small toys.<ref name=\":0\">{{Cite web |title=What is a Neural Radiance Field (NeRF)? {{!}} Definition from TechTarget |url=https://www.techtarget.com/searchenterpriseai/definition/neural-radiance-fields-NeRF |access-date=2023-10-24 |website=Enterprise AI |language=en}}</ref> Since the original paper in 2020, many improvements have been made to the NeRF algorithm, with variations for special use cases. \n\n=== Fourier feature mapping ===\nIn 2020, shortly after the release of NeRF, the addition of Fourier Feature Mapping improved training speed and image accuracy. Deep neural networks struggle to learn high frequency functions in low dimensional domains; a phenomenon known as spectral bias. To overcome this shortcoming, points are mapped to a higher dimensional feature space before being fed into the MLP. \n\n<math>\\gamma(\\mathrm{v}) = \\begin{bmatrix} a_1 \\cos(2{\\pi} {\\Beta}_1^T \\mathrm{v}) \\\\ a_1 \\sin(2\\pi {\\Beta}_1^T \\mathrm{v}) \\\\ \\vdots \\\\ a_m \\cos(2{\\pi} {\\Beta}_m^T \\mathrm{v}) \\\\ a_m \\sin(2{\\pi} {\\Beta}_m^T \\mathrm{v}) \\end{bmatrix}</math>\n\nWhere <math>\\mathrm{v}</math> is the input point, <math>\\Beta_i</math> are the frequency vectors, and <math>a_i</math> are coefficients.   \n\nThis allows for rapid convergence to high frequency functions, such as pixels in a detailed image.<ref>{{Cite arXiv |last1=Tancik |first1=Matthew |last2=Srinivasan |first2=Pratul P. |last3=Mildenhall |first3=Ben |last4=Fridovich-Keil |first4=Sara |last5=Raghavan |first5=Nithin |last6=Singhal |first6=Utkarsh |last7=Ramamoorthi |first7=Ravi |last8=Barron |first8=Jonathan T. |last9=Ng |first9=Ren |date=2020-06-18 |title=Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains |class=cs.CV |language=en |eprint=2006.10739}}</ref>   \n\n=== Bundle-adjusting neural radiance fields ===\nOne limitation of NeRFs is the requirement of knowing accurate camera poses to train the model. Often times, pose estimation methods are not completely accurate, nor is the camera pose even possible to know. These imperfections result in artifacts and suboptimal convergence. So, a method was developed to optimize the camera pose along with the volumetric function itself. Called Bundle-Adjusting Neural Radiance Field (BARF), the technique uses a dynamic [[low-pass filter]] to go from coarse to fine adjustment, minimizing error by finding the geometric transformation to the desired image. This corrects imperfect camera poses and greatly improves the quality of NeRF renders.<ref>{{Cite arXiv |last1=Lin |first1=Chen-Hsuan |last2=Ma |first2=Wei-Chiu |last3=Torralba |first3=Antonio |last4=Lucey |first4=Simon |title=BARF: Bundle-Adjusting Neural Radiance Fields  |date=2021 |class=cs.CV |eprint=2104.06405 }}</ref>\n\n=== Multiscale representation ===\nConventional NeRFs struggle to represent detail at all viewing distances, producing blurry images up close and overly [[Aliasing|aliased]] images from distant views. In 2021, researchers introduced a technique to improve the sharpness of details at different viewing scales known as mip-NeRF (comes from [[mipmap]]). Rather than sampling a single ray per pixel, the technique fits a [[Gaussian function|gaussian]] to the conical [[frustum]] cast by the camera. This improvement effectively anti-aliases across all viewing scales. mip-NeRF also reduces overall image error and is faster to converge at ~half the size of ray-based NeRF.<ref>{{Cite arXiv |last1=Barron |first1=Jonathan T. |last2=Mildenhall |first2=Ben |last3=Tancik |first3=Matthew |last4=Hedman |first4=Peter |last5=Martin-Brualla |first5=Ricardo |last6=Srinivasan |first6=Pratul P. |date=2021-04-07 |title=Mip-NeRF: {A} Multiscale Representation for Anti-Aliasing Neural Radiance Fields |class=cs.CV |eprint=2103.13415}}</ref>   \n\n=== Learned initializations ===\nIn 2021, researchers applied [[Meta-learning (computer science)|meta-learning]] to assign initial weights to the MLP. This rapidly speeds up convergence by effectively giving the network a head start in gradient descent. Meta-learning also allowed the MLP to learn an underlying representation of certain scene types. For example, given a dataset of famous tourist landmarks, an initialized NeRF could partially reconstruct a scene given one image.<ref>{{Cite arXiv |last1=Tancik |first1=Matthew |last2=Mildenhall |first2=Ben |last3=Wang |first3=Terrance |last4=Schmidt |first4=Divi |last5=Srinivasan |first5=Pratul |date=2021 |title=Learned Initializations for Optimizing Coordinate-Based Neural Representations |class=cs.CV |eprint=2012.02189}}</ref>\n\n=== NeRF in the wild ===\nConventional NeRFs are vulnerable to slight variations in input images (objects, lighting) often resulting in [[Ghosting (television)|ghosting]] and artifacts. As a result, NeRFs struggle to represent dynamic scenes, such as bustling city streets with changes in lighting and dynamic objects. In 2021, researchers at Google<ref name=\":0\" /> developed a new method for accounting for these variations, named NeRF in the Wild (NeRF-W). This method splits the neural network (MLP) into three separate models. The main MLP is retained to encode the static volumetric radiance. However, it operates in sequence with a separate MLP for appearance embedding (changes in lighting, camera properties) and an MLP for transient embedding (changes in scene objects). This allows the NeRF to be trained on diverse photo collections, such as those taken by mobile phones at different times of day.<ref>{{Cite arXiv|last1=Martin-Brualla |first1=Ricardo |last2=Radwan |first2=Noha |last3=Sajjadi |first3=Mehdi S. M. |last4=Barron |first4=Jonathan T. |last5=Dosovitskiy |first5=Alexey |last6=Duckworth |first6=Daniel |title=NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections  |date=2020 |class=cs.CV |eprint=2008.02268 }}</ref>\n\n=== Relighting ===\nIn 2021, researchers added more outputs to the MLP at the heart of NeRFs. The output now included: volume density, surface normal, material parameters, distance to the first surface intersection (in any direction), and visibility of the external environment in any direction. The inclusion of these new parameters lets the MLP learn material properties, rather than pure radiance values. This facilitates a more complex rendering pipeline, calculating direct and [[global illumination]], specular highlights, and shadows. As a result, the NeRF can render the scene under any lighting conditions with no re-training.<ref>{{Cite arXiv |last1=Srinivasan |first1=Pratul P. |last2=Deng |first2=Boyang |last3=Zhang |first3=Xiuming |last4=Tancik |first4=Matthew |last5=Mildenhall |first5=Ben |last6=Barron |first6=Jonathan T. |title=NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis  |date=2020 |class=cs.CV |eprint=2012.03927}}</ref>\n\n=== Plenoctrees ===\nAlthough NeRFs had reached high levels of fidelity, their costly compute time made them useless for many applications requiring real-time rendering, such as [[Virtual reality|VR]]/[[Augmented reality|AR]] and interactive content. Introduced in 2021, Plenoctrees (plenoptic [[Octree|octrees]]) enabled real-time rendering of pre-trained NeRFs through division of the volumetric radiance function into an octree. Rather than assigning a radiance direction into the camera, viewing direction is taken out of the network input and spherical radiance is predicted for each region. This makes rendering over 3000x faster than conventional NeRFs.<ref>{{Cite arXiv |last1=Yu |first1=Alex |last2=Li |first2=Ruilong |last3=Tancik |first3=Matthew |last4=Li |first4=Hao |last5=Ng |first5=Ren |last6=Kanazawa |first6=Angjoo |title=PlenOctrees for Real-time Rendering of Neural Radiance Fields |date=2021 |class=cs.CV |eprint=2103.14024}}</ref>\n\n=== Sparse Neural Radiance Grid ===\nSimilar to Plenoctrees, this method enabled real-time rendering of pretrained NeRFs. To avoid querying the large MLP for each point, this method bakes NeRFs into Sparse Neural Radiance Grids (SNeRG). A SNeRG is a sparse [[voxel]] grid containing opacity and color, with learned feature vectors to encode view-dependent information. A lightweight, more efficient MLP is then used to produce view-dependent residuals to modify the color and opacity. To enable this compressive baking, small changes to the NeRF architecture were made, such as running the MLP once per pixel rather than for each point along the ray. These improvements make SNeRG extremely efficient, outperforming Plenoctrees.<ref>{{Cite arXiv |last1=Hedman |first1=Peter |last2=Srinivasan |first2=Pratul P. |last3=Mildenhall |first3=Ben |last4=Barron |first4=Jonathan T. |last5=Debevec |first5=Paul |title=Baking Neural Radiance Fields for Real-Time View Synthesis |date=2021 |class=cs.CV |eprint=2103.14645}}</ref> \n\n=== Instant NeRFs ===\nIn 2022, researchers at Nvidia enabled real-time training of NeRFs through a technique known as Instant Neural Graphics Primitives. An innovative input encoding reduces computation, enabling real-time training of a NeRF, an improvement orders of magnitude above previous methods. The speedup stems from the use of spatial [[Hash function|hash functions]], which have <math>O(1)</math> access times, and parallelized architectures which run fast on modern [[Graphics processing unit|GPUs]].<ref>{{Cite journal |last1=M\u00fcller |first1=Thomas |last2=Evans |first2=Alex |last3=Schied |first3=Christoph |last4=Keller |first4=Alexander |date=2022-07-04 |title=Instant Neural Graphics Primitives with a Multiresolution Hash Encoding |journal=ACM Transactions on Graphics |volume=41 |issue=4 |pages=1\u201315 |doi=10.1145/3528223.3530127 |arxiv=2201.05989 |s2cid=246016186 |issn=0730-0301}}</ref> \n\n== Related techniques ==\n\n=== Plenoxels ===\nPlenoxel (plenoptic volume element) uses a sparse [[voxel]] representation instead of a volumetric approach as seen in NeRFs. Plenoxel also completely removes the MLP, instead directly performing gradient descent on the voxel coefficients. Plenoxel can match the fidelity of a conventional NeRF in orders of magnitude less training time. Published in 2022, this method disproved the importance of the MLP, showing that the differentiable rendering pipeline is the critical component.<ref>{{Cite arXiv |last1=Fridovich-Keil |first1=Sara |last2=Yu |first2=Alex |last3=Tancik |first3=Matthew |last4=Chen |first4=Qinhong |last5=Recht |first5=Benjamin |last6=Kanazawa |first6=Angjoo |title=Plenoxels: Radiance Fields without Neural Networks |date=2021 |class=cs.CV |eprint=2112.05131}}</ref>\n\n=== Gaussian splatting ===\n[[Gaussian splatting]] is a newer method that can outperform NeRF in render time and fidelity. Rather than representing the scene as a volumetric function, it uses a sparse cloud of 3D [[Gaussian function|gaussians]]. First, a point cloud is generated (through [[structure from motion]]) and converted to gaussians of initial covariance, color, and opacity. The gaussians are directly optimized through stochastic gradient descent to match the input image. This saves computation by removing empty space and foregoing the need to query a neural network for each point. Instead, simply \"splat\" all the gaussians onto the screen and they overlap to produce the desired image.<ref>{{Cite journal |last1=Kerbl |first1=Bernhard |last2=Kopanas |first2=Georgios |last3=Leimkuehler |first3=Thomas |last4=Drettakis |first4=George |date=2023-07-26 |title=3D Gaussian Splatting for Real-Time Radiance Field Rendering |url=http://dx.doi.org/10.1145/3592433 |journal=ACM Transactions on Graphics |volume=42 |issue=4 |pages=1\u201314 |doi=10.1145/3592433 |s2cid=259267917 |issn=0730-0301|doi-access=free }}</ref>\n\n=== Photogrammetry ===\nTraditional [[photogrammetry]] is not neural, instead using robust geometric equations to obtain 3D measurements. NeRFs, unlike photogrammetric methods, do not inherently produce dimensionally accurate 3D geometry. While their results are often sufficient for extracting accurate geometry (ex: via cube marching<ref name=\":1\" />), the process is [[Fuzzy logic|fuzzy]], as with most neural methods. This limits NeRF to cases where the output image is valued, rather than raw scene geometry. However, NeRFs excel in situations with unfavorable lighting. For example, photogrammetric methods completely break down when trying to reconstruct reflective or transparent objects in a scene, while a NeRF is able to infer the geometry.<ref>{{Cite web|url=https://www.youtube.com/watch?v=YX5AoaWrowY|title=Why THIS is the Future of Imagery (and Nobody Knows it Yet)|via=www.youtube.com}}</ref>\n\n== Applications ==\nNeRFs have a wide range of applications, and are starting to grow in popularity as they become integrated into user-friendly applications.<ref name=\":2\" /><ref>{{Cite web |title=Luma AI |url=https://lumalabs.ai/ |access-date=2023-11-09 |website=Luma AI}}</ref> \n\n=== Content creation ===\nNeRFs have huge potential in content creation, where on-demand photorealistic views are extremely valuable.<ref>{{Cite web |date=2023-10-20 |title=Shutterstock Speaks About NeRFs At Ad Week {{!}} Neural Radiance Fields |url=https://neuralradiancefields.io/shutterstock-speaks-about-nerfs-at-ad-week/ |access-date=2023-10-24 |website=neuralradiancefields.io |language=en-US}}</ref> The technology democratizes a space previously only accessible by teams of VFX artists with expensive assets. Neural radiance fields now allow anyone with a camera to create compelling 3D environments.<ref name=\":2\" /> NeRF has been combined with [[Generative artificial intelligence|generative AI]], allowing users with no modelling experience to instruct changes in photorealistic 3D scenes.<ref>{{Cite book |last1=Haque |first1=Ayaan |last2=Tancik |first2=Matthew |last3=Efros |first3=Alexei |last4=Holynski |first4=Aleksander |last5=Kanazawa |first5=Angjoo |title=2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) |chapter=InstructPix2Pix: Learning to Follow Image Editing Instructions |date=2023-06-01 |chapter-url=http://dx.doi.org/10.1109/cvpr52729.2023.01764 |pages=18392\u201318402 |publisher=IEEE |doi=10.1109/cvpr52729.2023.01764|arxiv=2211.09800 |isbn=979-8-3503-0129-8 |s2cid=253581213 }}</ref> NeRFs have potential uses in video production, computer graphics, and product design.  \n\n==== Interactive content ====\nThe photorealism of NeRFs make them appealing for applications where immersion is important, such as virtual reality or videogames. NeRFs can be combined with classical rendering techniques to insert synthetic objects and create believable virtual experiences.<ref>{{Cite web |date=2023-11-08 |title=Venturing Beyond Reality: VR-NeRF {{!}} Neural Radiance Fields |url=https://neuralradiancefields.io/venturing-beyond-reality-vr-nerf/ |access-date=2023-11-09 |website=neuralradiancefields.io |language=en-US}}</ref>\n\n=== Medical imaging ===\nNeRFs have been used to reconstruct 3D CT scans from sparse or even single X-ray views. The model demonstrated high fidelity renderings of chest and knee data. If adopted, this method can save patients from excess doses of ionizing radiation, allowing for safer diagnosis.<ref>{{Cite book |last1=Corona-Figueroa |first1=Abril |last2=Frawley |first2=Jonathan |last3=Taylor |first3=Sam Bond- |last4=Bethapudi |first4=Sarath |last5=Shum |first5=Hubert P. H. |last6=Willcocks |first6=Chris G. |title=2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) |chapter=MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray |date=2022-07-11 |pages=3843\u20133848 |chapter-url=http://dx.doi.org/10.1109/embc48229.2022.9871757 |volume=2022 |publisher=IEEE |doi=10.1109/embc48229.2022.9871757|pmid=36085823 |isbn=978-1-7281-2782-8 |s2cid=246473192 |url=https://dro.dur.ac.uk/37238/1/37238.pdf }}</ref>\n\n=== Robotics and autonomy ===\nThe unique ability of NeRFs to understand transparent and reflective objects makes them useful for robots interacting in such environments. The use of NeRF allowed a robot arm to precisely manipulate a transparent wine glass; a task where traditional [[computer vision]] would struggle.<ref>{{Cite conference |last1=Kerr |first1=Justin |last2=Fu |first2=Letian |last3=Huang |first3=Huang |last4=Avigal |first4=Yahav |last5=Tancik |first5=Matthew |last6=Ichnowski |first6=Jeffrey |last7=Kanazawa |first7=Angjoo |last8=Goldberg |first8=Ken |date=2022-08-15 |title=Evo-NeRF: Evolving NeRF for Sequential Robot Grasping of Transparent Objects |url=https://openreview.net/forum?id=Bxr45keYrf|conference=CoRL 2022 Conference |language=en}}</ref>\n\nNeRFs can also generate photorealistic human faces, making them valuable tools for human-computer interaction. Traditionally rendered faces can be [[Uncanny valley|uncanny]], while [[Deepfake|other neural methods]] are too slow to run in real-time.<ref>{{Cite web |last=Aurora |date=2023-06-04 |title=Generating highly detailed human faces using Neural Radiance Fields |url=https://medium.com/illumination/generating-highly-detailed-human-faces-using-neural-radiance-fields-3b149cbd8af0 |access-date=2023-11-09 |website=ILLUMINATION |language=en}}</ref>\n== References ==\n<!-- See https://en.wikipedia.org/wiki/Help:Referencing_for_beginners on how to create references. -->\n{{Reflist}}\n\n[[Category:Machine learning algorithms]]\n\n{{Computer vision footer}}"}