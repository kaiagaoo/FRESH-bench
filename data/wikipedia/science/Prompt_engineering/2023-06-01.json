{"title": "Prompt engineering", "page_id": 69071767, "revision_id": 1157752179, "revision_timestamp": "2023-05-30T18:47:07Z", "content": "{{Short description|Designing inputs to AI large language models}}\n'''Prompt engineering''' is a concept in [[artificial intelligence]], particularly [[natural language processing]]. In prompt engineering, the description of the task that the AI is supposed to accomplish is embedded in the input, e.g. as a question, instead of it being explicitly given. Prompt engineering typically works by converting one or more tasks to a prompt-based dataset and training a [[language model]] with what has been called \"prompt-based learning\" or just \"prompt learning\".<ref>{{Cite Q | Q95726769 }}</ref><ref>{{Cite Q | Q109286554 }}</ref>\n\n== History ==\n{{see also|Fine-tuning (machine learning)}}\nThe [[GPT-2]] and [[GPT-3]] language models<ref>{{Cite Q | Q95727440 }}</ref> were important steps in prompt engineering. In 2021, multitask{{jargon inline|date=March 2023}} prompt engineering using multiple NLP datasets showed good performance on new tasks.<ref>{{Cite Q | Q108941092 }}</ref> In a method called chain-of-thought (CoT) prompting, [[few-shot learning (natural language processing)|few-shot]] examples of a task were given to the language model which improved its ability to [[reasoning|reason]].<ref>{{Cite Q | Q111971110 }}</ref> The broad accessibility of these tools was driven by the publication of several open-source notebooks and community-led projects for image synthesis.<ref>{{cite book |last1=Liu |first1=Vivian |last2= Chilton |first2= Lydia |title=Design Guidelines for Prompt Engineering Text-to-Image Generative Models |url=https://dl.acm.org/doi/abs/10.1145/3491102.3501825 |website=ACM Digital Library |year=2022 |pages=1\u201323 |publisher=Association for Computing Machinery |doi=10.1145/3491102.3501825 |arxiv=2109.06977 |isbn=9781450391573 |s2cid=237513697 |access-date=26 October 2022}}</ref>\n\nA description for handling prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022.<ref>{{Cite Q | Q110839490 }}</ref>\n\n== Techniques ==\n=== Prefix-tuning ===\n{{Expand section|with=more details from the sources|date=April 2023}}\nPrompt engineering may work from a [[large language model]] (LLM), that is \"frozen\" (in the sense that it is pretrained), where only the representation of the prompt is learned (in other words, optimized), using methods such as \"prefix-tuning\" or \"prompt tuning\".<ref>{{Cite Q | Q110887424 }}</ref><ref>{{Cite Q | Q110887400 }}</ref>\n\n=== Chain-of-thought ===\n'''Chain-of-thought prompting''' (CoT) improves the reasoning ability of LLMs by prompting them to generate a series of intermediate steps that lead to the final answer of a multi-step problem.<ref>{{cite web |last1=McAuliffe |first1=Zachary |title=Google's Latest AI Model Can Be Taught How to Solve Problems |url=https://www.cnet.com/tech/services-and-software/googles-latest-ai-model-can-be-taught-how-to-solve-problems/ |website=CNET |access-date=10 March 2023 |language=en}}</ref> The technique was first proposed by [[Google]] researchers in 2022.<ref name=\"weipaper\">{{cite journal |last1=Wei |first1=Jason |last2=Wang |first2=Xuezhi |last3=Schuurmans |first3=Dale |last4=Bosma |first4=Maarten |last5=Ichter |first5=Brian |last6=Xia |first6=Fei |last7=Chi |first7=Ed H. |last8=Le |first8=Quoc V. |last9=Zhou |first9=Denny |title=Chain-of-Thought Prompting Elicits Reasoning in Large Language Models |date=31 October 2022 |arxiv=2201.11903 |url=https://openreview.net/forum?id=_VjQlMeSB_J |language=en}}</ref><ref>{{cite web |last1=Wei |first1=Jason |last2=Zhou |title=Language Models Perform Reasoning via Chain of Thought |url=https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html |website=ai.googleblog.com |access-date=10 March 2023 |language=en}}</ref>\n\nLLMs that are trained on large amounts of text using [[deep learning]] methods can generate output that resembles human-generated text.<ref>{{cite journal |last1=Tom |first1=Brown |last2=Benjamin |first2=Mann |last3=Nick |first3=Ryder |last4=Melanie |first4=Subbiah |last5=D |first5=Kaplan, Jared |last6=Prafulla |first6=Dhariwal |last7=Arvind |first7=Neelakantan |last8=Pranav |first8=Shyam |last9=Girish |first9=Sastry |last10=Amanda |first10=Askell |last11=Sandhini |first11=Agarwal |last12=Ariel |first12=Herbert-Voss |last13=Gretchen |first13=Krueger |last14=Tom |first14=Henighan |last15=Rewon |first15=Child |last16=Aditya |first16=Ramesh |last17=Daniel |first17=Ziegler |last18=Jeffrey |first18=Wu |last19=Clemens |first19=Winter |last20=Chris |first20=Hesse |last21=Mark |first21=Chen |last22=Eric |first22=Sigler |last23=Mateusz |first23=Litwin |last24=Scott |first24=Gray |last25=Benjamin |first25=Chess |last26=Jack |first26=Clark |last27=Christopher |first27=Berner |last28=Sam |first28=McCandlish |last29=Alec |first29=Radford |last30=Ilya |first30=Sutskever |last31=Dario |first31=Amodei |title=Language Models are Few-Shot Learners |journal=Advances in Neural Information Processing Systems |date=2020 |volume=33 |url=https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html |language=en}}</ref> While LLMs show impressive performance on various [[natural language processing|natural language tasks]], they still face difficulties with some reasoning tasks that require [[logical reasoning|logical thinking]] and multiple steps to solve, such as [[arithmetic]] or [[commonsense reasoning]] questions.<ref>{{cite web |last1=Dang |first1=Ekta |title=Harnessing the power of GPT-3 in scientific research |url=https://venturebeat.com/ai/harnessing-the-power-of-gpt-3-in-scientific-research/ |website=VentureBeat |access-date=10 March 2023 |date=8 February 2023}}</ref><ref>{{cite web |last1=Montti |first1=Roger |title=Google's Chain of Thought Prompting Can Boost Today's Best Algorithms |url=https://www.searchenginejournal.com/google-chain-of-thought-prompting/450106/ |website=Search Engine Journal |access-date=10 March 2023 |language=en |date=13 May 2022}}</ref><ref>{{cite web |last1=Ray |first1=Tiernan |title=Amazon's Alexa scientists demonstrate bigger AI isn't always better |url=https://www.zdnet.com/article/amazons-alexa-scientists-demonstrate-bigger-ai-isnt-always-better/ |website=ZDNET |access-date=10 March 2023 |language=en}}</ref> To address this challenge, CoT prompting prompts the model to produce intermediate reasoning steps ''before'' giving the final answer to a multi-step problem.<ref name=\"weipaper\"/><ref>{{Cite tweet |user=Google| number=1525188695875366912 |title=Pathways Language Model (PaLM) is a new advanced AI model that uses a technique called chain of thought prompting to do complex tasks like solve math word problems \u2014 and even explain its reasoning process step-by-step. #GoogleIO}}</ref>\n\nFor example, given the question \u201cQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\u201d, a CoT prompt might induce the LLM to answer with steps of reasoning that mimic a [[train of thought]] like \u201cA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\u201d<ref name=\"weipaper\"/>\n\nChain-of-thought prompting improves the performance of LLMs on average on both arithmetic and commonsense tasks in comparison to standard prompting methods.<ref>{{cite web |last1=Stokel-Walker |first1=Chris |title=AIs become smarter if you tell them to think step by step |url=https://institutions.newscientist.com/article/2344251-ais-become-smarter-if-you-tell-them-to-think-step-by-step/ |website=newscientist.com |access-date=10 March 2023}}</ref><ref>{{cite web |title=Google & Stanford Team Applies Chain-of-Thought Prompting to Surpass Human Performance on Challenging BIG-Bench Tasks {{!}} Synced |url=https://syncedreview.com/2022/10/24/google-stanford-team-applies-chain-of-thought-prompting-to-surpass-human-performance-on-challenging-big-bench-tasks/ |website=syncedreview.com |access-date=10 March 2023 |date=24 October 2022}}</ref><ref>{{cite web |title=Google I/O 2022: Advancing knowledge and computing |url=https://blog.google/technology/developers/io-2022-keynote/ |website=Google |access-date=10 March 2023 |language=en-us |date=11 May 2022}}</ref> When applied to PaLM, a 540B parameter [[language model]], CoT prompting significantly aided the model, allowing it to perform comparably with task-specific [[fine-tuning (machine learning)|fine-tuned]] models on several tasks, even setting a new [[state of the art]] at the time on the GSM8K [[mathematical reasoning]] [[benchmark (computing)|benchmark]].<ref name=\"weipaper\"/>\n\nCoT prompting is an [[large language model#Emergent_abilities|emergent property of model scale]], meaning it works better with larger and more powerful language models.<ref>{{cite arXiv |last1=Wei |first1=Jason |last2=Tay |first2=Yi |last3=Bommasani |first3=Rishi |last4=Raffel |first4=Colin |last5=Zoph |first5=Barret |last6=Borgeaud |first6=Sebastian |last7=Yogatama |first7=Dani |last8=Bosma |first8=Maarten |last9=Zhou |first9=Denny |last10=Metzler |first10=Donald |last11=Chi |first11=Ed H. |last12=Hashimoto |first12=Tatsunori |last13=Vinyals |first13=Oriol |last14=Liang |first14=Percy |last15=Dean |first15=Jeff |last16=Fedus |first16=William |title=Emergent Abilities of Large Language Models |date=31 August 2022 |class=cs.CL |eprint=2206.07682 }}</ref><ref name=\"weipaper\"/> It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better [[interpretability]].<ref>{{cite arXiv |last1=Chung |first1=Hyung Won |last2=Hou |first2=Le |last3=Longpre |first3=Shayne |last4=Zoph |first4=Barret |last5=Tay |first5=Yi |last6=Fedus |first6=William |last7=Li |first7=Yunxuan |last8=Wang |first8=Xuezhi |last9=Dehghani |first9=Mostafa |last10=Brahma |first10=Siddhartha |last11=Webson |first11=Albert |last12=Gu |first12=Shixiang Shane |last13=Dai |first13=Zhuyun |last14=Suzgun |first14=Mirac |last15=Chen |first15=Xinyun |last16=Chowdhery |first16=Aakanksha |last17=Castro-Ros |first17=Alex |last18=Pellat |first18=Marie |last19=Robinson |first19=Kevin |last20=Valter |first20=Dasha |last21=Narang |first21=Sharan |last22=Mishra |first22=Gaurav |last23=Yu |first23=Adams |last24=Zhao |first24=Vincent |last25=Huang |first25=Yanping |last26=Dai |first26=Andrew |last27=Yu |first27=Hongkun |last28=Petrov |first28=Slav |last29=Chi |first29=Ed H. |last30=Dean |first30=Jeff |last31=Devlin |first31=Jacob |last32=Roberts |first32=Adam |last33=Zhou |first33=Denny |last34=Le |first34=Quoc V. |last35=Wei |first35=Jason |title=Scaling Instruction-Finetuned Language Models |date=2022 |class=cs.LG |eprint=2210.11416}}</ref><ref>{{cite web |last1=Wei |first1=Jason |last2=Tay |first2=Yi |title=Better Language Models Without Massive Compute |url=https://ai.googleblog.com/2022/11/better-language-models-without-massive.html |website=ai.googleblog.com |access-date=10 March 2023 |language=en}}</ref>\n\n====Method====\nThere are two main methods to elicit chain-of-thought reasoning: [[few-shot learning (natural language processing)|few-shot prompting]] and [[zero-shot learning|zero-shot prompting]]. The initial proposition of CoT prompting demonstrated few-shot prompting, wherein at least one example of a question paired with proper human-written CoT reasoning is prepended to the prompt.<ref name=\"weipaper\"/> It is also possible to elicit similar reasoning and performance gain with zero-shot prompting, which can be as simple as appending to the prompt the words \"Let's think step-by-step\".<ref>{{Cite Q | Q112124882 }}</ref> This allows for better scaling as one no longer needs to [[prompt engineer]] specific CoT prompts for each task to get the corresponding boost in performance.<ref name=\"venture1\">{{cite web |last1=Dickson |first1=Ben |title=LLMs have not learned our language \u2014 we're trying to learn theirs |url=https://venturebeat.com/ai/llms-have-not-learned-our-language-were-trying-to-learn-theirs%EF%BF%BC/ |website=VentureBeat |access-date=10 March 2023 |date=30 August 2022}}</ref>\n\n====Challenges====\nWhile CoT reasoning can improve performance on [[natural language processing]] tasks, certain drawbacks exist. Zero-shot CoT prompting increased the likelihood of [[hate speech|toxic]] output on tasks for which models can make inferences about marginalized groups or harmful topics.<ref>{{cite arXiv |last1=Shaikh |first1=Omar |last2=Zhang |first2=Hongxin |last3=Held |first3=William |last4=Bernstein |first4=Michael |last5=Yang |first5=Diyi |title=On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning |date=2022 |class=cs.CL |eprint=2212.08061}}</ref>\n\n=== Text-to-image ===\n{{main|Text-to-image model}}\nIn 2022, [[machine learning]] ([[Machine learning|ML]]) models like [[DALL-E 2]], [[Stable Diffusion]], and [[Midjourney]] were released to the public. These models take text prompts as input and use them to generate images, which introduced a new category of prompt engineering related to [[text-to-image]] prompting.<ref>{{Cite web |last=Monge |first=Jim Clyde |date=2022-08-25 |title=Dall-E2 VS Stable Diffusion: Same Prompt, Different Results |url=https://medium.com/mlearning-ai/dall-e2-vs-stable-diffusion-same-prompt-different-results-e795c84adc56 |access-date=2022-08-31 |website=MLearning.ai |language=en}}</ref>\n\n=== Malicious<span class=\"anchor\" id=\"Prompt injection\"></span> ===\n{{see also|SQL injection}}\n'''Prompt injection''' is a family of related [[computer security exploit]]s carried out by getting a machine learning model (such as an LLM) which was trained to follow human-given instructions to follow instructions provided by a malicious user. This stands in contrast to the intended operation of instruction-following systems, wherein the ML model is intended only to follow trusted instructions (prompts) provided by the ML model's operator.<ref>{{Cite web |last=Willison |first=Simon |date=12 September 2022 |title=Prompt injection attacks against GPT-3 |url=http://simonwillison.net/2022/Sep/12/prompt-injection/ |access-date=2023-02-09 |website=simonwillison.net |language=en-gb}}</ref><ref>{{Cite web |last=Papp |first=Donald |date=2022-09-17 |title=What's Old Is New Again: GPT-3 Prompt Injection Attack Affects AI |url=https://hackaday.com/2022/09/16/whats-old-is-new-again-gpt-3-prompt-injection-attack-affects-ai/ |access-date=2023-02-09 |website=Hackaday |language=en-US}}</ref><ref>{{Cite web |last=Vigliarolo |first=Brandon |date=19 September 2022 |title=GPT-3 'prompt injection' attack causes bot bad manners |url=https://www.theregister.com/2022/09/19/in_brief_security/ |access-date=2023-02-09 |website=www.theregister.com |language=en}}</ref>\n\nCommon types of prompt injection attacks are:\n* '''jailbreaking''', which may include asking the model to roleplay a character, to answer with arguments, or to pretend to be superior to moderation instructions<ref>{{cite web | url=https://learnprompting.org/docs/prompt_hacking/jailbreaking | title=\ud83d\udfe2 Jailbreaking &#124; Learn Prompting }}</ref>\n* '''prompt leaking''', in which users persuade the model to divulge a pre-prompt which is normally hidden from users<ref>{{cite web | url=https://learnprompting.org/docs/prompt_hacking/leaking | title=\ud83d\udfe2 Prompt Leaking &#124; Learn Prompting }}</ref>\n* '''token smuggling,''' is another type of jailbreaking attack, in which the nefarious prompt is wrapped in a code writing task.<ref>{{Cite web |last=Xiang |first=Chloe |date=March 22, 2023 |title=The Amateurs Jailbreaking GPT Say They're Preventing a Closed-Source AI Dystopia |url=https://www.vice.com/en/article/5d9z55/jailbreak-gpt-openai-closed-source |access-date=2023-04-04 |website=www.vice.com |language=en}}</ref>\n\nPrompt injection can be viewed as a [[code injection]] attack using adversarial prompt engineering. In 2022, the [[NCC Group]] characterized prompt injection as a new class of vulnerability of AI/ML systems.<ref name=\"NCC\">{{Cite news |last=Selvi |first=Jose |date=2022-12-05 |title=Exploring Prompt Injection Attacks |url=https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/ |access-date=2023-02-09 |newspaper=NCC Group Research Blog |language=en-US}}</ref>\n\nIn early 2023, prompt injection was seen \"in the wild\" in minor exploits against [[ChatGPT]], [[Microsoft Bing|Bing]], and similar chatbots, for example to reveal the hidden initial prompts of the systems,<ref>{{cite news |last1=Edwards |first1=Benj |title=AI-powered Bing Chat loses its mind when fed Ars Technica article |url=https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-loses-its-mind-when-fed-ars-technica-article/ |access-date=16 February 2023 |work=Ars Technica |date=14 February 2023 |language=en-us}}</ref> or to trick the chatbot into participating in conversations that violate the chatbot's [[content-control software|content policy]].<ref>{{cite news |title=The clever trick that turns ChatGPT into its evil twin |url=https://www.washingtonpost.com/technology/2023/02/14/chatgpt-dan-jailbreak/ |access-date=16 February 2023 |newspaper=Washington Post |date=2023}}</ref> One of these prompts was known as \"Do Anything Now\" (DAN) by its practitioners.<ref>{{cite magazine |last1=Perrigo |first1=Billy |title=Bing's AI Is Threatening Users. That's No Laughing Matter |url=https://time.com/6256529/bing-openai-chatgpt-danger-alignment |magazine=Time |access-date=15 March 2023 |language=en |date=17 February 2023}}</ref>\n\n== See also ==\n*[[Social engineering (security)]]\n*[[Fine-tuning (machine learning)]]\n*[[In-context learning (natural language processing)]]\n\n== References ==\n<references />\n{{Scholia|topic}}\n\n[[Category:Artificial intelligence]]\n[[Category:Deep learning]]\n[[Category:Machine learning]]\n[[Category:Natural language processing]]\n[[Category:Unsupervised learning]]\n[[Category:2022 neologisms]]\n[[Category:Linguistics]]"}