{"title": "Prompt engineering", "page_id": 69071767, "revision_id": 1130578183, "revision_timestamp": "2022-12-30T22:08:32Z", "content": "{{Short description|Concept in artificial intelligence}}\n'''Prompt engineering''' is a concept in [[artificial intelligence]], particularly [[natural language processing]] (NLP).\nIn prompt engineering, the description of the task is embedded in the input, e.g., as a question instead of it being implicitly given.\nPrompt engineering typically works by converting one or more tasks to a prompt-based dataset and training a [[language model]] with what has been called \"prompt-based learning\" or just \"prompt learning\".<ref>{{Cite Q | Q95726769 }}</ref><ref>{{Cite Q | Q109286554 }}</ref>\nPrompt engineering may work from a large \"frozen\" pretrained language model and where only the representation of the prompt is learned, with what has been called \"prefix-tuning\" or \"prompt tuning\".<ref>{{Cite Q | Q110887424 }}</ref><ref>{{Cite Q | Q110887400 }}</ref>\n\nThe [[GPT-2]] and [[GPT-3]] language models<ref>{{Cite Q | Q95727440 }}</ref> were important steps in prompt engineering.\nIn 2021, multitask prompt engineering using multiple NLP datasets showed good performance on new tasks.<ref>{{Cite Q | Q108941092 }}</ref>\nPrompts that include a [[chain of thought|train of thought]] in [[few-shot learning]] examples show better indication of [[reasoning]] in language models.<ref>{{Cite Q | Q111971110 }}</ref> In [[zero-shot learning]] prepending text to the prompt that encourages a chain of thought (e.g. \"Let's think step by step\") may improve the performance of a language model in multi-step reasoning problems.<ref>{{Cite Q | Q112124882 }}</ref> The broad accessibility of these tools were driven by the publication of several open-source notebooks and community-led projects for image synthesis.<ref>{{cite web |last1=Liu |first1=Vivian |last2= Chilton |first2= Lydia |title=Design Guidelines for Prompt Engineering Text-to-Image Generative Models |url=https://dl.acm.org/doi/abs/10.1145/3491102.3501825 |website=ACM Digital Library |publisher=Association for Computing Machinery |access-date=26 October 2022}}</ref>\n\nA description for handling prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022.<ref>{{Cite Q | Q110839490 }}</ref>\n\nIn 2022, [[Machine learning|machine learning models]] like [[DALL-E]], [[Stable Diffusion]], and [[Midjourney]] were released to the public. These models take text prompts as input and use them to generate images, which effected a new category of prompt engineering related to [[text-to-image]] prompting.<ref>{{Cite web |last=Monge |first=Jim Clyde |date=2022-08-25 |title=Dall-E2 VS Stable Diffusion: Same Prompt, Different Results |url=https://medium.com/mlearning-ai/dall-e2-vs-stable-diffusion-same-prompt-different-results-e795c84adc56 |access-date=2022-08-31 |website=MLearning.ai |language=en}}</ref>\n\n== References ==\n{{Scholia|topic}}\n<references/>\n\n[[Category:Natural language processing]]"}