{"title": "Retrieval-augmented generation", "page_id": 75229858, "revision_id": 1262734659, "revision_timestamp": "2024-12-12T22:20:49Z", "content": "{{Short description|Type of information retrieval using LLMs}}\n\n'''Retrieval Augmented Generation''' ('''RAG''') is a technique that grants [[generative artificial intelligence]] models [[information retrieval]] capabilities. It modifies interactions with a [[large language model]] (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static [[training data]]. This allows LLMs to use domain-specific and/or updated information.<ref name=\"Survey\"/>  \nUse cases include providing chatbot access to internal company data or giving factual information only from an authoritative source.<ref name=\"AWS\"/>\n\n==Process==\nThe RAG process is made up of four key stages. First, all the data must be prepared and indexed for use by the LLM. Thereafter, each query consists of a retrieval<ref>{{Cite web |title=Evolving Interactions {{!}} Looking Glass 2024 |url=https://www.thoughtworks.com/insights/looking-glass/evolving-interactions |access-date=2024-12-12 |website=Thoughtworks |language=en}}</ref>, augmentation, and generation phase.<ref name=\"Survey\"/>\n\n===Indexing===\nTypically, the data to be referenced is converted into LLM [[word embeddings|embeddings]], numerical representations in the form of large vectors. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example [[knowledge graphs]]).<ref name=\"Survey\"/> These embeddings are then stored in a [[vector database]] to allow for [[document retrieval]].\n\n[[File:RAG diagram.svg|thumb|Overview of RAG process, combining external documents and user input into an LLM prompt to get tailored output]]\n===Retrieval===\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query.<ref name=\"FCC\"/> This comparison can be done using a variety of methods, which depend in part on the type of indexing used.<ref name=\"Survey\"/>\n\n===Augmentation===\nThe model feeds this relevant retrieved information into the LLM via [[prompt engineering]] of the user's original query.<ref name=\"AWS\"/> Newer implementations ({{as of|2023|lc=y}}) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.<ref name=\"Survey\"/>\n\n===Generation===\nFinally, the LLM can generate output based on both the query and the retrieved documents.<ref name=\"BUZBP\"/> Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.<ref name=\"Survey\"/>\n\n== Improvements ==\nImprovements to the basic process above can be applied at different stages in the RAG flow. \n\n=== Encoder ===\nThese methods center around the encoding of text as either dense or sparse vectors. Sparse vectors, used to encode the identity of a word, are typically [[Large language model#Tokenization|dictionary]] length and contain almost all zeros. Dense vectors, used to encode meaning, are much smaller and contain far fewer zeros. Several enhancements can be made to the way similarities are calculated in the vector stores (databases).  \n\n* Performance can be improved with faster dot products, approximate nearest neighors, or centroid searches.<ref name=\"faiss\"/>  \n* Accuracy can be improved with Late Interactions.{{clarify|date=August 2024}}<ref name=\"colbert\"/>\n* Hybrid vectors: dense vector representations can be combined with sparse [[one-hot]] vectors in order to use the faster sparse dot products rather than the slower dense ones.<ref name=\"splade\"/>  Other{{clarify|date=August 2024}} methods can combine sparse methods (BM25, SPLADE) with dense ones like DRAGON.\n\n=== Retriever-centric methods ===\nThese methods focus on improving the quality of hits from the vector database:\n\n* pre-train the retriever using the Inverse Cloze Task.<ref name=orqa /> \n* progressive data augmentation.  The method of Dragon samples difficult negatives to train a dense vector  retriever.<ref name=\"dragon\"/> \n*Under supervision, train the retriever for a given generator.  Given a prompt and the desired answer, retrieve the top-k vectors, and feed those vectors into the generator to achieve a perplexity score for the correct answer.  Then minimize the KL-divergence between the observed retrieved vectors probability and LM likelihoods to adjust the retriever.<ref name=\"replug\"/> \n* use reranking to train the retriever.<ref name=\"ralm\"/>\n\n=== Language model ===\n\n{{Image frame | width=300 | align=center | content=[[File:Language model in Deepmind's 2021 Retro for RAG.svg|300px]] | caption=Retro language model for RAG.  Each Retro block consists of Attention, Chunked Cross Attention, and Feed Forward layers.  Black-lettered boxes show data being changed, and blue lettering shows the algorithm performing the changes. }}\n\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts.<ref name=\"borgeaud\"/>  Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided.  The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics.  The redesigned language model is shown here.  \n\nIt has been reported that Retro is not reproducible,<!--ref name=\"cs25v3a\"/--> so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.<ref name=\"wang2023a\"/>\n\n=== Chunking ===\n{{norefs|section|date=October 2024}}\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\n\n{{Image frame | align=center | width=500 | content=[[File:rag-doc-styles.png | 500px]] | caption= Different data styles have patterns that correct chunking can take advantage of. }}\nThree types of chunking strategies are:\n\n*Fixed length with overlap. This is fast and easy.  Overlapping consecutive chunks helps to maintain semantic context across chunks.\n*Syntax-based chunks can break the document up into sentences.  Libraries such as spaCy or NLTK can also help.\n*File format-based chunking. Certain file types have natural chunks built in, and it's best to respect them.  For example, code files are best chunked and vectorized as whole functions or classes.  HTML files should leave <nowiki><table> or base64 encoded <img></nowiki> elements intact.  Similar considerations should be taken for pdf files.  Libraries such as Unstructured or Langchain can assist with this method.\n\n==Challenges==\nIf the external data source is large, retrieval can be slow. The use of RAG does not completely eliminate the general challenges faced by LLMs, including [[Hallucination (artificial intelligence)|hallucination]].<ref name=\"FCC\"/>\n\n==References==\n{{Reflist|refs=\n\n<ref name=\"Survey\">{{cite arXiv |last1=Gao |first1=Yunfan |last2=Xiong |first2=Yun |last3=Gao |first3=Xinyu |last4=Jia |first4=Kangxiang |last5=Pan |first5=Jinliu |last6=Bi |first6=Yuxi |last7=Dai |first7=Yi |last8=Sun |first8=Jiawei |last9=Wang |first9=Meng |last10=Wang |first10=Haofen |title=Retrieval-Augmented Generation for Large Language Models: A Survey |date=2023 |class=cs.CL |eprint=2312.10997}}</ref>\n<ref name=\"AWS\">{{cite web |title=What is RAG? - Retrieval-Augmented Generation AI Explained - AWS |url=https://aws.amazon.com/what-is/retrieval-augmented-generation/ |website=Amazon Web Services, Inc. |access-date=16 July 2024}}</ref>\n<ref name=\"FCC\">{{cite web |title=Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook |url=https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/ |website=freeCodeCamp.org |access-date=16 July 2024 |language=en |date=11 June 2024}}</ref>\n<ref name=\"BUZBP\">{{Cite journal |last1=Lewis |first1=Patrick |last2=Perez |first2=Ethan |last3=Piktus |first3=Aleksandra |last4=Petroni |first4=Fabio |last5=Karpukhin |first5=Vladimir |last6=Goyal |first6=Naman |last7=K\u00fcttler |first7=Heinrich |last8=Lewis |first8=Mike |last9=Yih |first9=Wen-tau |last10=Rockt\u00e4schel |first10=Tim |last11=Riedel |first11=Sebastian |last12=Kiela |first12=Douwe |date=2020 |title=Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks |url=https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Curran Associates, Inc. |volume=33 |pages=9459\u20139474 |arxiv=2005.11401}}</ref>\n\n<ref name=\"orqa\">   {{ Cite web | url=https://aclanthology.org/P19-1612.pdf | title=\"Latent Retrieval for Weakly Supervised Open Domain Question Answering\" | last1=Lee | first1=Kenton | last2=Chang | first2=Ming-Wei | last3=Toutanova | first3=Kristina | date=2019 }} </ref>\n<ref name=\"faiss\">  {{ cite web | url=https://github.com/facebookresearch/faiss | title=faiss | website=[[GitHub]] }} </ref>\n<ref name=\"colbert\">{{ Cite web | url=https://dl.acm.org/doi/pdf/10.1145/3397271.3401075 | title=\"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\" | last1=Khattab | first1=Omar | last2=Zaharia | first2=Matei | date=2020 | doi=10.1145/3397271.3401075 }} </ref>\n<ref name=\"splade\" >{{ Cite journal | title=\"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval\"  | last1=Formal | first1=Thibault | last2=Lassance | first2=Carlos | last3=Piwowarski | first3=Benjamin | last4=Clinchant | first4=St\u00e9phane | journal=Arxiv | date=2021 | s2cid=237581550 }} </ref>\n<ref name=\"dragon\" >{{ Cite web | url=https://aclanthology.org/2023.findings-emnlp.423.pdf | title=\"How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval\" | last1=Lin | first1=Sheng-Chieh | last2=Asai | first2=Akari | date=2023 }} </ref>\n<ref name=\"replug\" >{{ Cite book | chapter-url=https://aclanthology.org/2024.naacl-long.463 | title=\"REPLUG: Retrieval-Augmented Black-Box Language Models\" | last1=Shi | first1=Weijia | last2=Min | first2=Sewon | chapter=REPLUG: Retrieval-Augmented Black-Box Language Models | date=2024 | pages=8371\u20138384 | doi=10.18653/v1/2024.naacl-long.463 | arxiv=2301.12652 }} </ref>\n<ref name=\"ralm\"   >{{ Cite journal | url=https://aclanthology.org/2023.tacl-1.75 | title=\"In-Context Retrieval-Augmented Language Models\" | last1=Ram | first1=Ori | last2=Levine | first2=Yoav | last3=Dalmedigos | first3=Itay | last4=Muhlgay | first4=Dor | last5=Shashua | first5=Amnon | last6=Leyton-Brown | first6=Kevin | last7=Shoham | first7=Yoav | journal=Transactions of the Association for Computational Linguistics | date=2023 | volume=11 | pages=1316\u20131331 | doi=10.1162/tacl_a_00605 | arxiv=2302.00083 }} </ref>\n<ref name=\"borgeaud\">{{cite web | url=https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf | title=\"Improving language models by retrieving from trillions of tokens\" | last1=Borgeaud | first1=Sebastian | last2=Mensch | first2=Arthur | date=2021 }} </ref>\n<ref name=\"wang2023a\">{{ Cite web | url=https://aclanthology.org/2023.emnlp-main.482.pdf | title=\"Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\" | last1=Wang | first1=Boxin | last2=Ping | first2=Wei | date=2023 }} </ref>\n}}\n\n{{Generative AI}}\n{{Artificial intelligence navbox}}\n\n[[Category:Large language models]]\n[[Category:Natural language processing]]\n[[Category:Information retrieval systems]]\n[[Category:Generative artificial intelligence]]"}