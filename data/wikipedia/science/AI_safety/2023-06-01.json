{"title": "AI safety", "page_id": 72360809, "revision_id": 1157913107, "revision_timestamp": "2023-05-31T19:27:56Z", "content": "{{Short description|Research area on making AI safe and beneficial}}\n\n'''AI safety''' is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from [[artificial intelligence]] (AI) systems. It encompasses [[machine ethics]] and [[AI alignment]], which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\n\n== Motivations ==\n\nAI researchers have widely different opinions about the severity and primary sources of risk posed by AI technology<ref name=\":1\">{{Cite journal |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |date=2018-07-31 |title=Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts |url=http://jair.org/index.php/jair/article/view/11222 |journal=Journal of Artificial Intelligence Research |volume=62 |pages=729\u2013754 |doi=10.1613/jair.1.11222 |s2cid=8746462 |issn=1076-9757 |access-date=2022-11-28 |archive-date=2023-02-10 |archive-url=https://web.archive.org/web/20230210114220/https://jair.org/index.php/jair/article/view/11222 |url-status=live }}</ref><ref>{{Cite journal |last1=Zhang |first1=Baobao |last2=Anderljung |first2=Markus |last3=Kahn |first3=Lauren |last4=Dreksler |first4=Noemi |last5=Horowitz |first5=Michael C. |last6=Dafoe |first6=Allan |date=2021-05-05 |title=Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers |arxiv=2105.02117 }}</ref><ref>{{Cite web| last1 = Stein-Perlman| first1 = Zach| last2 = Weinstein-Raun| first2 = Benjamin| last3 = Grace| title = 2022 Expert Survey on Progress in AI| work = AI Impacts| accessdate = 2022-11-23| date = 2022-08-04| url = https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123052335/https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/| url-status = live}}</ref> \u2013 though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \u201cextremely bad (e.g. [[human extinction]])\u201d outcome of advanced AI.<ref name=\":1\" /> In a 2022 survey of the Natural language processing (NLP) community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \u201cat least as bad as an all-out nuclear war.\u201d<ref>{{Cite journal |last1=Michael |first1=Julian |last2=Holtzman |first2=Ari |last3=Parrish |first3=Alicia |last4=Mueller |first4=Aaron |last5=Wang |first5=Alex |last6=Chen |first6=Angelica |last7=Madaan |first7=Divyam |last8=Nangia |first8=Nikita |last9=Pang |first9=Richard Yuanzhe |last10=Phang |first10=Jason |last11=Bowman |first11=Samuel R. |date=2022-08-26 |title=What Do NLP Researchers Believe? Results of the NLP Community Metasurvey |arxiv=2208.12852 }}</ref> Scholars discuss current risks from critical systems failures,<ref>{{cite thesis |type=PhD |last = De-Arteaga| first = Maria| title = Machine Learning in High-Stakes Settings: Risks and Opportunities| date = 2020-05-13 |publisher=Carnegie Mellon University}}</ref> bias,<ref name=\":3\">{{Cite journal |last1=Mehrabi |first1=Ninareh |last2=Morstatter |first2=Fred |last3=Saxena |first3=Nripsuta |last4=Lerman |first4=Kristina |last5=Galstyan |first5=Aram |date=2021 |title=A Survey on Bias and Fairness in Machine Learning |url=https://dl.acm.org/doi/10.1145/3457607 |journal=ACM Computing Surveys |language=en |volume=54 |issue=6 |pages=1\u201335 |doi=10.1145/3457607 |arxiv=1908.09635 |s2cid=201666566 |issn=0360-0300 |access-date=2022-11-28 |archive-date=2022-11-23 |archive-url=https://web.archive.org/web/20221123054208/https://dl.acm.org/doi/10.1145/3457607 |url-status=live }}</ref> and AI enabled surveillance;<ref>{{Cite report| publisher = Carnegie Endowment for International Peace|last = Feldstein| first = Steven| title = The Global Expansion of AI Surveillance| date = 2019}}</ref> emerging risks from technological unemployment, digital manipulation,<ref>{{Cite journal| last = Barnes| first = Beth| title = Risks from AI persuasion| journal = Lesswrong| accessdate = 2022-11-23| date = 2021| url = https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123055429/https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion| url-status = live}}</ref> and weaponization;<ref name=\":13\">{{Cite journal |last1=Brundage |first1=Miles |last2=Avin |first2=Shahar |last3=Clark |first3=Jack |last4=Toner |first4=Helen |last5=Eckersley |first5=Peter |last6=Garfinkel |first6=Ben |last7=Dafoe |first7=Allan |last8=Scharre |first8=Paul |last9=Zeitzoff |first9=Thomas |last10=Filar |first10=Bobby |last11=Anderson |first11=Hyrum |last12=Roff |first12=Heather |last13=Allen |first13=Gregory C |last14=Steinhardt |first14=Jacob |last15=Flynn |first15=Carrick |date=2018-04-30 |others=Apollo-University Of Cambridge Repository, Apollo-University Of Cambridge Repository |title=The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation |publisher=Apollo - University of Cambridge Repository |url=https://www.repository.cam.ac.uk/handle/1810/275332 |doi=10.17863/cam.22520 |s2cid=3385567 |access-date=2022-11-28 |archive-date=2022-11-23 |archive-url=https://web.archive.org/web/20221123055429/https://www.repository.cam.ac.uk/handle/1810/275332 |url-status=live }}</ref> and speculative risks from losing control of future [[artificial general intelligence]] (AGI) agents.<ref>{{Cite journal |last=Carlsmith |first=Joseph |date=2022-06-16 |title=Is Power-Seeking AI an Existential Risk? |arxiv=2206.13353 }}</ref>\n\nSome have criticized concerns about AGI, such as [[Stanford University]] adjunct professor [[Andrew Ng]] who compared them to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref>{{Cite web| last = Shermer| first = Michael| title = Artificial Intelligence Is Not a Threat---Yet| work = Scientific American| accessdate = 2022-11-23| date = 2017| url = https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/| archive-date = 2017-12-01| archive-url = https://web.archive.org/web/20171201051401/https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/| url-status = live}}</ref> Others, such as [[University of California, Berkeley]] professor [[Stuart J. Russell]] urge caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it.\"<ref>{{Cite web| last = Dafoe| first = Allan| title = Yes, We Are Worried About the Existential Risk of Artificial Intelligence| work = MIT Technology Review| accessdate = 2022-11-28| date = 2016| url = https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/| archive-date = 2022-11-28| archive-url = https://web.archive.org/web/20221128223713/https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/| url-status = live}}</ref>\n\n== Background ==\n\nRisks from AI began to be seriously discussed at the start of the [[Information Age|computer age]]:\n\n{{Blockquote\n|text=Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\n|author=Norbert Wiener (1949)<ref>{{Cite news| issn = 0362-4331| last = Markoff| first = John| title = In 1949, He Imagined an Age of Robots| work = The New York Times| accessdate = 2022-11-23| date = 2013-05-20| url = https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123061554/https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html| url-status = live}}</ref>\n}}\n\nFrom 2008 to 2009, the [[Association for the Advancement of Artificial Intelligence|AAAI]] commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.\"<ref>{{Cite web| last = AAAI| title = AAAI Presidential Panel on Long-Term AI Futures| accessdate = 2022-11-23| url = https://www.aaai.org/Organization/presidential-panel.php| archive-date = 2022-09-01| archive-url = https://web.archive.org/web/20220901033354/https://www.aaai.org/Organization/presidential-panel.php| url-status = live}}</ref>\n\nIn 2011, [[Roman Yampolskiy]] introduced the term \"AI safety engineering\"<ref>{{Cite journal |last1=Yampolskiy |first1=Roman V. |last2=Spellchecker |first2=M. S. |date=2016-10-25 |title=Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures |arxiv=1610.07997 }}</ref> at the Philosophy and Theory of Artificial Intelligence conference,<ref>{{Cite web| title = PT-AI 2011 - Philosophy and Theory of Artificial Intelligence (PT-AI 2011)| accessdate = 2022-11-23| url = https://conference.researchbib.com/view/event/13986| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123062236/https://conference.researchbib.com/view/event/13986| url-status = live}}</ref> listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable.\"<ref>{{Citation |last=Yampolskiy |first=Roman V. |title=Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach |date=2013 |url=http://link.springer.com/10.1007/978-3-642-31674-6_29 |work=Philosophy and Theory of Artificial Intelligence |series=Studies in Applied Philosophy, Epistemology and Rational Ethics |volume=5 |pages=389\u2013396 |editor-last=M\u00fcller |editor-first=Vincent C. |place=Berlin, Heidelberg |publisher=Springer Berlin Heidelberg |doi=10.1007/978-3-642-31674-6_29 |isbn=978-3-642-31673-9 |access-date=2022-11-23 |archive-date=2023-03-15 |archive-url=https://web.archive.org/web/20230315184334/https://link.springer.com/chapter/10.1007/978-3-642-31674-6_29 |url-status=live }}</ref>\n\nIn 2014, philosopher [[Nick Bostrom]] published the book ''[[Superintelligence: Paths, Dangers, Strategies]]''. His argument that future advanced systems may pose a threat to human existence prompted [[Elon Musk]],<ref>{{Cite tweet |author=Elon Musk|user=elonmusk |number=495759307346952192 |title=Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.}}</ref> [[Bill Gates]],<ref>{{Cite AV media| people = Kaiser Kuo| title = Baidu CEO Robin Li interviews Bill Gates and Elon Musk at the Boao Forum, March 29 2015| accessdate = 2022-11-23| date = 2015-03-31| time = 55:49| url = https://www.youtube.com/watch?v=NG0ZjUfOBUs&t=1055s&ab_channel=KaiserKuo| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123072346/https://www.youtube.com/watch?v=NG0ZjUfOBUs&t=1055s&ab_channel=KaiserKuo| url-status = live}}</ref> and [[Stephen Hawking]]<ref>{{Cite news| last = Cellan-Jones| first = Rory| title = Stephen Hawking warns artificial intelligence could end mankind| work = BBC News| accessdate = 2022-11-23| date = 2014-12-02| url = https://www.bbc.com/news/technology-30290540| archive-date = 2015-10-30| archive-url = https://web.archive.org/web/20151030054329/http://www.bbc.com/news/technology-30290540| url-status = live}}</ref> to voice similar concerns.\n\nIn 2015, dozens of artificial intelligence experts signed an [[Open Letter on Artificial Intelligence|open letter on artificial intelligence]] calling for research on the societal impacts of AI and outlining concrete directions.<ref>{{Cite web| last = Future of Life Institute| title = Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter| work = Future of Life Institute| accessdate = 2022-11-23| url = https://futureoflife.org/open-letter/ai-open-letter/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123072924/https://futureoflife.org/open-letter/ai-open-letter/| url-status = live}}</ref> To date, the letter has been signed by over 8000 people including [[Yann LeCun]], [[Shane Legg]], [[Yoshua Bengio]], and [[Stuart J. Russell|Stuart Russell]].\n\nIn the same year, a group of academics led by professor [[Stuart J. Russell|Stuart Russell]] founded the [[Center for Human-Compatible Artificial Intelligence|Center for Human-Compatible AI]] at UC Berkeley and the [[Future of Life Institute]] awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial.\"<ref>{{Cite web| last = Future of Life Institute| title = AI Research Grants Program| work = Future of Life Institute| accessdate = 2022-11-23| url = https://futureoflife.org/ai-research/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123074311/https://futureoflife.org/ai-research/| url-status = live}}</ref>\n\nIn 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,<ref>{{Cite web| title = SafArtInt 2016| accessdate = 2022-11-23| url = https://www.cmu.edu/safartint/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123074311/https://www.cmu.edu/safartint/| url-status = live}}</ref> which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.<ref>{{Cite web| last = Bach| first = Deborah| title = UW to host first of four White House public workshops on artificial intelligence| work = UW News| accessdate = 2022-11-23| date = 2016| url = https://www.washington.edu/news/2016/05/19/uw-to-host-first-of-four-white-house-public-workshops-on-artificial-intelligence/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123074321/https://www.washington.edu/news/2016/05/19/uw-to-host-first-of-four-white-house-public-workshops-on-artificial-intelligence/| url-status = live}}</ref> In the same year, Concrete Problems in AI Safety \u2013 one of the first and most influential technical AI Safety agendas \u2013  was published.<ref>{{Cite journal |last1=Amodei |first1=Dario |last2=Olah |first2=Chris |last3=Steinhardt |first3=Jacob |last4=Christiano |first4=Paul |last5=Schulman |first5=John |last6=Man\u00e9 |first6=Dan |date=2016-07-25 |title=Concrete Problems in AI Safety |arxiv=1606.06565 }}</ref>\n\nIn 2017, the Future of Life Institute sponsored the [[Asilomar Conference on Beneficial AI]], where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.\"<ref name=\":21\">{{Cite web| last = Future of Life Institute| title = AI Principles| work = Future of Life Institute| accessdate = 2022-11-23| url = https://futureoflife.org/open-letter/ai-principles/| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123074312/https://futureoflife.org/open-letter/ai-principles/| url-status = live}}</ref>\n\nIn 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance.<ref name=\":8\">{{Cite web| last = Research| first = DeepMind Safety| title = Building safe artificial intelligence: specification, robustness, and assurance| work = Medium| accessdate = 2022-11-23| date = 2018-09-27| url = https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1| archive-date = 2023-02-10| archive-url = https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1| url-status = live}}</ref> The following year, researchers organized a workshop at ICLR that focused on these problem areas.<ref>{{Cite web| title = SafeML ICLR 2019 Workshop| accessdate = 2022-11-23| url = https://sites.google.com/view/safeml-iclr2019| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123074310/https://sites.google.com/view/safeml-iclr2019| url-status = live}}</ref>\n\n== Research foci ==\nAI safety research areas include robustness, monitoring, and alignment.<ref name=\":8\" /><ref name=\":7\" /> Robustness is concerned with making systems highly reliable, monitoring is about anticipating failures or detecting misuse, and alignment is focused on ensuring they have beneficial objectives.\n\n=== Robustness ===\nRobustness research focuses on ensuring that AI systems behave as intended in a wide range of different situations, which includes the following subproblems:\n* [[Black swan theory|Black swan]] robustness: building systems that behave as intended in rare situations.\n* [[Adversarial machine learning|Adversarial robustness]]: designing systems to be resilient to inputs that are intentionally selected to make them fail.\n\n==== Black swan robustness ====\nRare inputs can cause AI systems to fail catastrophically. For example, in the [[2010 flash crash]], automated trading systems unexpectedly overreacted to market aberrations, destroying one trillion dollars of stock value in minutes.<ref>{{Cite journal |last1=Kirilenko |first1=Andrei |last2=Kyle |first2=Albert S. |last3=Samadi |first3=Mehrdad |last4=Tuzun |first4=Tugkan |date=2017 |title=The Flash Crash: High-Frequency Trading in an Electronic Market: The Flash Crash |url=https://onlinelibrary.wiley.com/doi/10.1111/jofi.12498 |journal=The Journal of Finance |language=en |volume=72 |issue=3 |pages=967\u2013998 |doi=10.1111/jofi.12498 |hdl=10044/1/49798 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124070043/https://onlinelibrary.wiley.com/doi/10.1111/jofi.12498 |url-status=live }}</ref> Note that no distribution shift needs to occur for this to happen. Black swan failures can occur as a consequence of the input data being [[Long tail|long-tailed]], which is often the case in real-world environments.<ref>{{Cite journal |last=Newman |first=Mej |date=2005 |title=Power laws, Pareto distributions and Zipf's law |url=http://www.tandfonline.com/doi/abs/10.1080/00107510500052444 |journal=Contemporary Physics |language=en |volume=46 |issue=5 |pages=323\u2013351 |doi=10.1080/00107510500052444 |arxiv=cond-mat/0412004 |bibcode=2005ConPh..46..323N |s2cid=2871747 |issn=0010-7514 |access-date=2022-11-28 |archive-date=2022-11-16 |archive-url=https://web.archive.org/web/20221116220804/https://www.tandfonline.com/doi/abs/10.1080/00107510500052444 |url-status=live }}</ref> Autonomous vehicles continue to struggle with \u2018corner cases\u2019 that might not have come up during training; for example, a vehicle might ignore a stop sign that is lit up as an LED grid.<ref>{{Cite web| last = Eliot| first = Lance| title = Whether Those Endless Edge Or Corner Cases Are The Long-Tail Doom For AI Self-Driving Cars| work = Forbes| accessdate = 2022-11-24| url = https://www.forbes.com/sites/lanceeliot/2021/07/13/whether-those-endless-edge-or-corner-cases-are-the-long-tail-doom-for-ai-self-driving-cars/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124070049/https://www.forbes.com/sites/lanceeliot/2021/07/13/whether-those-endless-edge-or-corner-cases-are-the-long-tail-doom-for-ai-self-driving-cars/| url-status = live}}</ref> Though problems like these may be solved as machine learning systems develop a better understanding of the world, some researchers point out that even humans often fail to adequately respond to unprecedented events like the COVID-19 pandemic, arguing that black swan robustness will be a persistent safety problem.<ref name=\":7\">{{Cite journal |last1=Hendrycks |first1=Dan |last2=Carlini |first2=Nicholas |last3=Schulman |first3=John |last4=Steinhardt |first4=Jacob |date=2022-06-16 |title=Unsolved Problems in ML Safety |arxiv=2109.13916 }}</ref>\n\n==== Adversarial robustness ====\nAI systems are often vulnerable to [[Adversarial machine learning#Adversarial examples|adversarial examples]] or \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake\u201d.<ref>{{Cite web| last1 = Goodfellow| first1 = Ian| last2 = Papernot| first2 = Nicolas| last3 = Huang| first3 = Sandy| last4 = Duan| first4 = Rocky| last5 = Abbeel| first5 = Pieter| last6 = Clark| first6 = Jack| title = Attacking Machine Learning with Adversarial Examples| work = OpenAI| accessdate = 2022-11-24| date = 2017-02-24| url = https://openai.com/blog/adversarial-example-research/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124070536/https://openai.com/blog/adversarial-example-research/| url-status = live}}</ref> For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.<ref name=\":4\">{{Cite journal |last1=Szegedy |first1=Christian |last2=Zaremba |first2=Wojciech |last3=Sutskever |first3=Ilya |last4=Bruna |first4=Joan |last5=Erhan |first5=Dumitru |last6=Goodfellow |first6=Ian |last7=Fergus |first7=Rob |date=2014-02-19 |title=Intriguing properties of neural networks |arxiv=1312.6199 }}</ref> This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.<ref>{{Cite journal |last1=Kurakin |first1=Alexey |last2=Goodfellow |first2=Ian |last3=Bengio |first3=Samy |date=2017-02-10 |title=Adversarial examples in the physical world |arxiv=1607.02533 }}</ref><ref>{{Cite journal |last1=Madry |first1=Aleksander |last2=Makelov |first2=Aleksandar |last3=Schmidt |first3=Ludwig |last4=Tsipras |first4=Dimitris |last5=Vladu |first5=Adrian |date=2019-09-04 |title=Towards Deep Learning Models Resistant to Adversarial Attacks |arxiv=1706.06083 }}</ref><ref>{{Cite journal |last1=Kannan |first1=Harini |last2=Kurakin |first2=Alexey |last3=Goodfellow |first3=Ian |date=2018-03-16 |title=Adversarial Logit Pairing |arxiv=1803.06373 }}</ref>\n[[File:Illustration_of_imperceptible_adversarial_pertubation.png|400px|right|thumb|Carefully crafted noise can be added to an image to cause it to be misclassifed with high confidence.]]\nAll of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.<ref name=\":4\" />\n\nAdversarial robustness is often associated with security.<ref>{{Cite journal |last1=Gilmer |first1=Justin |last2=Adams |first2=Ryan P. |last3=Goodfellow |first3=Ian |last4=Andersen |first4=David |last5=Dahl |first5=George E. |date=2018-07-19 |title=Motivating the Rules of the Game for Adversarial Example Research |arxiv=1807.06732 }}</ref> Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.<ref>{{Cite journal |last1=Carlini |first1=Nicholas |last2=Wagner |first2=David |date=2018-03-29 |title=Audio Adversarial Examples: Targeted Attacks on Speech-to-Text |arxiv=1801.01944 }}</ref> Network intrusion<ref>{{Cite journal |last1=Sheatsley |first1=Ryan |last2=Papernot |first2=Nicolas |last3=Weisman |first3=Michael |last4=Verma |first4=Gunjan |last5=McDaniel |first5=Patrick |date=2022-09-09 |title=Adversarial Examples in Constrained Domains |arxiv=2011.01183 }}</ref> and malware<ref>{{Cite journal |last1=Suciu |first1=Octavian |last2=Coull |first2=Scott E. |last3=Johns |first3=Jeffrey |date=2019-04-13 |title=Exploring Adversarial Examples in Malware Detection |arxiv=1810.08280 }}</ref> detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\n\nModels that represent objectives (reward models) must also be adversarially robust.\u00a0 For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.<ref>{{Cite journal |last1=Ouyang |first1=Long |last2=Wu |first2=Jeff |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |date=2022-03-04 |title=Training language models to follow instructions with human feedback |arxiv=2203.02155 }}</ref> Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.<ref name=\":0\">{{Cite journal |last1=Gao |first1=Leo |last2=Schulman |first2=John |last3=Hilton |first3=Jacob |date=2022-10-19 |title=Scaling Laws for Reward Model Overoptimization |arxiv=2210.10760 }}</ref> This issue can be addressed by improving the adversarial robustness of the reward model.<ref>{{Cite journal |last1=Yu |first1=Sihyun |last2=Ahn |first2=Sungsoo |last3=Song |first3=Le |last4=Shin |first4=Jinwoo |date=2021-10-27 |title=RoMA: Robust Model Adaptation for Offline Model-based Optimization |arxiv=2110.14188 }}</ref> More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.<ref name=\"X-Risk Analysis for AI Research\">{{Cite journal |last1=Hendrycks |first1=Dan |last2=Mazeika |first2=Mantas |date=2022-09-20 |title=X-Risk Analysis for AI Research |arxiv=2206.05862 }}</ref>\n\n=== Monitoring ===\nMonitoring focuses on anticipating AI system failures so that they can be prevented or managed. Subproblems of monitoring include flagging when systems are uncertain, detecting malicious use, understanding the inner workings of [[Black box|black-box]] AI systems, and identifying hidden functionality planted by a malicious actor.{{citation needed|date=February 2023}}\n\n==== Estimating uncertainty ====\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.<ref>{{Cite journal |last1=Tran |first1=Khoa A. |last2=Kondrashova |first2=Olga |last3=Bradley |first3=Andrew |last4=Williams |first4=Elizabeth D. |last5=Pearson |first5=John V. |last6=Waddell |first6=Nicola |date=2021 |title=Deep learning in cancer diagnosis, prognosis and treatment selection |journal=Genome Medicine |language=en |volume=13 |issue=1 |pages=152 |doi=10.1186/s13073-021-00968-x |issn=1756-994X |pmc=8477474 |pmid=34579788}}</ref> ML models generally express confidence by outputting probabilities; however, they are often overconfident,<ref>{{Cite conference| publisher = PMLR| volume = 70| pages = 1321\u20131330| last1 = Guo| first1 = Chuan| last2 = Pleiss| first2 = Geoff| last3 = Sun| first3 = Yu| last4 = Weinberger| first4 = Kilian Q.| title = On calibration of modern neural networks| book-title = Proceedings of the 34th international conference on machine learning| series = Proceedings of machine learning research| date = 2017-08-06}}</ref> especially in situations that differ from those that they were trained to handle.<ref>{{Cite journal |last1=Ovadia |first1=Yaniv |last2=Fertig |first2=Emily |last3=Ren |first3=Jie |last4=Nado |first4=Zachary |last5=Sculley |first5=D. |last6=Nowozin |first6=Sebastian |last7=Dillon |first7=Joshua V. |last8=Lakshminarayanan |first8=Balaji |last9=Snoek |first9=Jasper |date=2019-12-17 |title=Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift |arxiv=1906.02530 }}</ref> Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\n\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.<ref>{{Cite journal |last1=Bogdoll |first1=Daniel |last2=Breitenstein |first2=Jasmin |last3=Heidecker |first3=Florian |last4=Bieshaar |first4=Maarten |last5=Sick |first5=Bernhard |last6=Fingscheidt |first6=Tim |last7=Z\u00f6llner |first7=J. Marius |date=2021 |title=Description of Corner Cases in Automated Driving: Goals and Challenges |journal=2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW) |pages=1023\u20131028 |doi=10.1109/ICCVW54120.2021.00119|arxiv=2109.09607 |isbn=978-1-6654-0191-3 |s2cid=237572375 }}</ref> Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,<ref>{{Cite journal |last1=Hendrycks |first1=Dan |last2=Mazeika |first2=Mantas |last3=Dietterich |first3=Thomas |date=2019-01-28 |title=Deep Anomaly Detection with Outlier Exposure |arxiv=1812.04606 }}</ref> though several other techniques are in use.<ref>{{Cite journal |last1=Wang |first1=Haoqi |last2=Li |first2=Zhizhong |last3=Feng |first3=Litong |last4=Zhang |first4=Wayne |date=2022-03-21 |title=ViM: Out-Of-Distribution with Virtual-logit Matching |arxiv=2203.10807 }}</ref><ref>{{Cite journal |last1=Hendrycks |first1=Dan |last2=Gimpel |first2=Kevin |date=2018-10-03 |title=A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks |arxiv=1610.02136 }}</ref>\n\n==== Detecting malicious use ====\nScholars<ref name=\":13\" /> and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,<ref>{{Cite journal |last1=Urbina |first1=Fabio |last2=Lentzos |first2=Filippa |last3=Invernizzi |first3=C\u00e9dric |last4=Ekins |first4=Sean |date=2022 |title=Dual use of artificial-intelligence-powered drug discovery |journal=Nature Machine Intelligence |language=en |volume=4 |issue=3 |pages=189\u2013191 |doi=10.1038/s42256-022-00465-9 |issn=2522-5839 |pmc=9544280 |pmid=36211133}}</ref> manipulate public opinion,<ref>{{Cite journal |last1=Center for Security and Emerging Technology |last2=Buchanan |first2=Ben |last3=Lohn |first3=Andrew |last4=Musser |first4=Micah |last5=Sedova |first5=Katerina |date=2021 |title=Truth, Lies, and Automation: How Language Models Could Change Disinformation |url=https://cset.georgetown.edu/publication/truth-lies-and-automation/ |journal= |doi=10.51593/2021ca003 |s2cid=240522878 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124073719/https://cset.georgetown.edu/publication/truth-lies-and-automation/ |url-status=live }}</ref><ref>{{Cite web| title = Propaganda-as-a-service may be on the horizon if large language models are abused| work = VentureBeat| accessdate = 2022-11-24| date = 2021-12-14| url = https://venturebeat.com/ai/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124073718/https://venturebeat.com/ai/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/| url-status = live}}</ref> or automate cyber attacks.<ref>{{Cite journal |last1=Center for Security and Emerging Technology |last2=Buchanan |first2=Ben |last3=Bansemer |first3=John |last4=Cary |first4=Dakota |last5=Lucas |first5=Jack |last6=Musser |first6=Micah |date=2020 |title=Automating Cyber Attacks: Hype and Reality |newspaper=Center for Security and Emerging Technology |url=https://cset.georgetown.edu/publication/automating-cyber-attacks/ |doi=10.51593/2020ca002 |s2cid=234623943 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124074301/https://cset.georgetown.edu/publication/automating-cyber-attacks/ |url-status=live }}</ref> These worries are a practical concern for companies like OpenAI which host powerful AI tools online.<ref>{{Cite web| title = Lessons Learned on Language Model Safety and Misuse| work = OpenAI| accessdate = 2022-11-24| date = 2022-03-03| url = https://openai.com/blog/language-model-safety-and-misuse/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124074259/https://openai.com/blog/language-model-safety-and-misuse/| url-status = live}}</ref> In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.<ref>{{Cite web| last1 = Markov| first1 = Todor| last2 = Zhang| first2 = Chong| last3 = Agarwal| first3 = Sandhini| last4 = Eloundou| first4 = Tyna| last5 = Lee| first5 = Teddy| last6 = Adler| first6 = Steven| last7 = Jiang| first7 = Angela| last8 = Weng| first8 = Lilian| title = New-and-Improved Content Moderation Tooling| work = OpenAI| accessdate = 2022-11-24| date = 2022-08-10| url = https://openai.com/blog/new-and-improved-content-moderation-tooling/| archive-date = 2023-01-11| archive-url = https://web.archive.org/web/20230111020935/https://openai.com/blog/new-and-improved-content-moderation-tooling/| url-status = live}}</ref>\n\n==== Transparency ====\nNeural networks have often been described as [[black box]]es,<ref name=\":5\">{{Cite journal| doi = 10.1038/d41586-022-00858-1| last = Savage| first = Neil| title = Breaking into the black box of artificial intelligence| journal = Nature| accessdate = 2022-11-24| date = 2022-03-29| pmid = 35352042| s2cid = 247792459| url = https://www.nature.com/articles/d41586-022-00858-1| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124074724/https://www.nature.com/articles/d41586-022-00858-1| url-status = live}}</ref> meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.<ref>{{Cite journal |last1=Center for Security and Emerging Technology |last2=Rudner |first2=Tim |last3=Toner |first3=Helen |date=2021 |title=Key Concepts in AI Safety: Interpretability in Machine Learning |url=https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/ |journal= |doi=10.51593/20190042 |s2cid=233775541 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124075212/https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/ |url-status=live }}</ref> This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.<ref>{{Cite web| last = McFarland| first = Matt| title = Uber pulls self-driving cars after first fatal crash of autonomous vehicle| work = CNNMoney| accessdate = 2022-11-24| date = 2018-03-19| url = https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124075209/https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html| url-status = live}}</ref>\n\nOne benefit of transparency is explainability.<ref name=\":6\">{{Cite journal |last1=Doshi-Velez |first1=Finale |last2=Kortz |first2=Mason |last3=Budish |first3=Ryan |last4=Bavitz |first4=Chris |last5=Gershman |first5=Sam |last6=O'Brien |first6=David |last7=Scott |first7=Kate |last8=Schieber |first8=Stuart |last9=Waldo |first9=James |last10=Weinberger |first10=David |last11=Weller |first11=Adrian |last12=Wood |first12=Alexandra |date=2019-12-20 |title=Accountability of AI Under the Law: The Role of Explanation |arxiv=1711.01134 }}</ref> It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.<ref name=\":6\" />\n\nAnother benefit is to reveal the cause of failures.<ref name=\":5\" /> At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were \u2018paying attention\u2019 to irrelevant hospital labels.<ref>{{Cite journal |last1=Fong |first1=Ruth |last2=Vedaldi |first2=Andrea |date=2017 |title=Interpretable Explanations of Black Boxes by Meaningful Perturbation |journal=2017 IEEE International Conference on Computer Vision (ICCV) |pages=3449\u20133457 |doi=10.1109/ICCV.2017.371|arxiv=1704.03296 |isbn=978-1-5386-1032-9 |s2cid=1633753 }}</ref>\n\nTransparency techniques can also be used to correct errors. For example, in the paper \u201cLocating and Editing Factual Associations in GPT,\u201d the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to \u2018edit\u2019 this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.<ref>{{Cite journal| volume = 35| last1 = Meng| first1 = Kevin| last2 = Bau| first2 = David| last3 = Andonian| first3 = Alex| last4 = Belinkov| first4 = Yonatan| title = Locating and editing factual associations in GPT| journal = Advances in Neural Information Processing Systems| date = 2022| arxiv = 2202.05262}}</ref> Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.<ref>{{Cite journal |last1=Bau |first1=David |last2=Liu |first2=Steven |last3=Wang |first3=Tongzhou |last4=Zhu |first4=Jun-Yan |last5=Torralba |first5=Antonio |date=2020-07-30 |title=Rewriting a Deep Generative Model |arxiv=2007.15646 }}</ref>\n\nFinally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.<ref>{{Cite journal |last1=R\u00e4uker |first1=Tilman |last2=Ho |first2=Anson |last3=Casper |first3=Stephen |last4=Hadfield-Menell |first4=Dylan |date=2022-09-05 |title=Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks |arxiv=2207.13243 }}</ref> \u201cInner\u201d interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.<ref>{{Cite journal |last1=Bau |first1=David |last2=Zhou |first2=Bolei |last3=Khosla |first3=Aditya |last4=Oliva |first4=Aude |last5=Torralba |first5=Antonio |date=2017-04-19 |title=Network Dissection: Quantifying Interpretability of Deep Visual Representations |arxiv=1704.05796 }}</ref><ref>{{Cite journal |last1=McGrath |first1=Thomas |last2=Kapishnikov |first2=Andrei |last3=Toma\u0161ev |first3=Nenad |last4=Pearce |first4=Adam |last5=Wattenberg |first5=Martin |last6=Hassabis |first6=Demis |last7=Kim |first7=Been |last8=Paquet |first8=Ulrich |last9=Kramnik |first9=Vladimir |date=2022-11-22 |title=Acquisition of chess knowledge in AlphaZero |journal=Proceedings of the National Academy of Sciences |language=en |volume=119 |issue=47 |pages=e2206625119 |doi=10.1073/pnas.2206625119 |pmid=36375061 |pmc=9704706 |arxiv=2111.09259 |bibcode=2022PNAS..11906625M |issn=0027-8424}}</ref> For example, researchers identified a neuron in CLIP that responds to images of people in spider man costumes, sketches of spiderman, and the word \u2018spider.\u2019<ref>{{Cite journal| doi = 10.23915/distill.00030| last1 = Goh| first1 = Gabriel| last2 = Cammarata| first2 = Nick| last3 = Voss| first3 = Chelsea| last4 = Carter| first4 = Shan| last5 = Petrov| first5 = Michael| last6 = Schubert| first6 = Ludwig| last7 = Radford| first7 = Alec| last8 = Olah| first8 = Chris| title = Multimodal neurons in artificial neural networks| journal = Distill| date = 2021| volume = 6| issue = 3| s2cid = 233823418}}</ref> It also involves explaining connections between these neurons or \u2018circuits\u2019.<ref>{{Cite journal| doi = 10.23915/distill.00024.001| last1 = Olah| first1 = Chris| last2 = Cammarata| first2 = Nick| last3 = Schubert| first3 = Ludwig| last4 = Goh| first4 = Gabriel| last5 = Petrov| first5 = Michael| last6 = Carter| first6 = Shan| title = Zoom in: An introduction to circuits| journal = Distill| date = 2020| volume = 5| issue = 3| s2cid = 215930358}}</ref><ref>{{Cite journal| doi = 10.23915/distill.00024.006| last1 = Cammarata| first1 = Nick| last2 = Goh| first2 = Gabriel| last3 = Carter| first3 = Shan| last4 = Voss| first4 = Chelsea| last5 = Schubert| first5 = Ludwig| last6 = Olah| first6 = Chris| title = Curve circuits| journal = Distill| date = 2021| volume = 6| issue = 1| doi-broken-date = 31 December 2022| url = https://distill.pub/2020/circuits/curve-circuits/| access-date = 5 December 2022| archive-date = 5 December 2022| archive-url = https://web.archive.org/web/20221205140056/https://distill.pub/2020/circuits/curve-circuits/| url-status = live}}</ref> For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.<ref>{{Cite journal| last1 = Olsson| first1 = Catherine| last2 = Elhage| first2 = Nelson| last3 = Nanda| first3 = Neel| last4 = Joseph| first4 = Nicholas| last5 = DasSarma| first5 = Nova| last6 = Henighan| first6 = Tom| last7 = Mann| first7 = Ben| last8 = Askell| first8 = Amanda| last9 = Bai| first9 = Yuntao| last10 = Chen| first10 = Anna| last11 = Conerly| first11 = Tom| last12 = Drain| first12 = Dawn| last13 = Ganguli| first13 = Deep| last14 = Hatfield-Dodds| first14 = Zac| last15 = Hernandez| first15 = Danny| last16 = Johnston| first16 = Scott| last17 = Jones| first17 = Andy| last18 = Kernion| first18 = Jackson| last19 = Lovitt| first19 = Liane| last20 = Ndousse| first20 = Kamal| last21 = Amodei| first21 = Dario| last22 = Brown| first22 = Tom| last23 = Clark| first23 = Jack| last24 = Kaplan| first24 = Jared| last25 = McCandlish| first25 = Sam| last26 = Olah| first26 = Chris| title = In-context learning and induction heads| journal = Transformer Circuits Thread| date = 2022| arxiv = 2209.11895}}</ref> \u201cInner interpretability\u201d has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.<ref>{{Cite web| last = Olah| first = Christopher| title = Interpretability vs Neuroscience [rough note]| accessdate = 2022-11-24| url = https://colah.github.io/notes/interp-v-neuro/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124114744/https://colah.github.io/notes/interp-v-neuro/| url-status = live}}</ref>\n\n==== Detecting trojans ====\nML models can potentially contain \u2018trojans\u2019 or \u2018backdoors\u2019:\u00a0 vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;<ref name=\":7\" /> or a trojaned autonomous vehicle may function normally until a specific trigger is visible.<ref>{{Cite journal |last1=Gu |first1=Tianyu |last2=Dolan-Gavitt |first2=Brendan |last3=Garg |first3=Siddharth |date=2019-03-11 |title=BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain |arxiv=1708.06733 }}</ref> Note that an adversary must have access to the system\u2019s training data in order to plant a trojan. This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.<ref>{{Cite journal |last1=Chen |first1=Xinyun |last2=Liu |first2=Chang |last3=Li |first3=Bo |last4=Lu |first4=Kimberly |last5=Song |first5=Dawn |date=2017-12-14 |title=Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning |arxiv=1712.05526 }}</ref> Researchers were able to plant a trojan in an image classifier by changing just 3 out of 3 million of the training images.<ref>{{Cite journal |last1=Carlini |first1=Nicholas |last2=Terzis |first2=Andreas |date=2022-03-28 |title=Poisoning and Backdooring Contrastive Learning |arxiv=2106.09667 }}</ref> In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.<ref name=\"X-Risk Analysis for AI Research\"/>\n\n=== Alignment ===\n{{Excerpt|AI alignment}}\n\n=== Systemic safety and sociotechnical factors ===\nIt is common for AI risks (and technological risks more generally) to be categorized as ''misuse or accidents''.<ref name=\":12\">{{Cite web| last1 = Zwetsloot| first1 = Remco| last2 = Dafoe| first2 = Allan| title = Thinking About Risks From AI: Accidents, Misuse and Structure| work = Lawfare| accessdate = 2022-11-24| date = 2019-02-11| url = https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124122021/https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure| url-status = live}}</ref> Some scholars have suggested that this framework falls short.<ref name=\":12\" /> For example, the [[Cuban Missile Crisis]] was not clearly an accident or a misuse of technology.<ref name=\":12\" /> Policy analysts Zwetsloot and Dafoe wrote, \u201cThe misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways\u2026 Often, though, the relevant causal chain is much longer.\u201d Risks often arise from \u2018structural\u2019 or \u2018systemic\u2019 factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.<ref name=\":12\" /> In the broader context of [[safety engineering]], structural factors like \u2018organizational safety culture\u2019 play a central role in the popular STAMP risk analysis framework.<ref>{{Cite journal |last1=Zhang |first1=Yingyu |last2=Dong |first2=Chuntong |last3=Guo |first3=Weiqun |last4=Dai |first4=Jiabao |last5=Zhao |first5=Ziming |date=2022 |title=Systems theoretic accident model and process (STAMP): A literature review |url=https://linkinghub.elsevier.com/retrieve/pii/S0925753521004367 |journal=Safety Science |language=en |volume=152 |pages=105596 |doi=10.1016/j.ssci.2021.105596 |s2cid=244550153 |access-date=2022-11-28 |archive-date=2023-03-15 |archive-url=https://web.archive.org/web/20230315184342/https://www.sciencedirect.com/science/article/abs/pii/S0925753521004367?via%3Dihub |url-status=live }}</ref>\n\nInspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.<ref name=\":7\" />\n\n==== Cyber defense ====\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.<ref>{{Cite journal |last1=Center for Security and Emerging Technology |last2=Hoffman |first2=Wyatt |date=2021 |title=AI and the Future of Cyber Competition |url=https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/ |journal= |doi=10.51593/2020ca007 |s2cid=234245812 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124122253/https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/ |url-status=live }}</ref> This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential preventing powerful AI models from being stolen and misused.<ref name=\":13\" />\n\n==== Improving institutional decision-making ====\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges.<ref>{{Cite journal |last1=Center for Security and Emerging Technology |last2=Imbrie |first2=Andrew |last3=Kania |first3=Elsa |date=2019 |title=AI Safety, Security, and Stability Among Great Powers: Options, Challenges, and Lessons Learned for Pragmatic Engagement |url=https://cset.georgetown.edu/publication/ai-safety-security-and-stability-among-great-powers-options-challenges-and-lessons-learned-for-pragmatic-engagement/ |journal= |doi=10.51593/20190051 |s2cid=240957952 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124122652/https://cset.georgetown.edu/publication/ai-safety-security-and-stability-among-great-powers-options-challenges-and-lessons-learned-for-pragmatic-engagement/ |url-status=live }}</ref> Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.<ref name=\":11\">{{Cite AV media| people = Future of Life Institute| title = AI Strategy, Policy, and Governance (Allan Dafoe)| accessdate = 2022-11-23| date = 2019-03-27| time = 22:05| url = https://www.youtube.com/watch?v=2IpJ8TIKKtI| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123055429/https://www.youtube.com/watch?v=2IpJ8TIKKtI| url-status = live}}</ref> AI researchers have argued that AI technologies could also be used to assist decision-making.<ref name=\":7\" /> For example, researchers are beginning to develop AI forecasting<ref>{{Cite journal |last1=Zou |first1=Andy |last2=Xiao |first2=Tristan |last3=Jia |first3=Ryan |last4=Kwon |first4=Joe |last5=Mazeika |first5=Mantas |last6=Li |first6=Richard |last7=Song |first7=Dawn |last8=Steinhardt |first8=Jacob |last9=Evans |first9=Owain |last10=Hendrycks |first10=Dan |date=2022-10-09 |title=Forecasting Future World Events with Neural Networks |arxiv=2206.15474 }}</ref> and advisory systems.<ref>{{Cite journal |last1=Gathani |first1=Sneha |last2=Hulsebos |first2=Madelon |last3=Gale |first3=James |last4=Haas |first4=Peter J. |last5=Demiralp |first5=\u00c7a\u011fatay |date=2022-02-08 |title=Augmenting Decision Making via Interactive What-If Analysis |arxiv=2109.06160 }}</ref>\n\n==== Facilitating cooperation ====\n\nMany of the largest global threats (nuclear war,<ref>{{Citation |last=Lindelauf |first=Roy |title=Nuclear Deterrence in the Algorithmic Age: Game Theory Revisited |date=2021 |url=https://link.springer.com/10.1007/978-94-6265-419-8_22 |work=NL ARMS Netherlands Annual Review of Military Studies 2020 |series=Nl Arms |pages=421\u2013436 |editor-last=Osinga |editor-first=Frans |place=The Hague |publisher=T.M.C. Asser Press |language=en |doi=10.1007/978-94-6265-419-8_22 |isbn=978-94-6265-418-1 |s2cid=229449677 |access-date=2022-11-24 |editor2-last=Sweijs |editor2-first=Tim |archive-date=2023-03-15 |archive-url=https://web.archive.org/web/20230315184337/https://link.springer.com/chapter/10.1007/978-94-6265-419-8_22 |url-status=live }}</ref> climate change,<ref name=\":14\">{{Cite web| last = Newkirk II| first = Vann R.| title = Is Climate Change a Prisoner's Dilemma or a Stag Hunt?| work = The Atlantic| accessdate = 2022-11-24| date = 2016-04-21| url = https://www.theatlantic.com/politics/archive/2016/04/climate-change-game-theory-models/624253/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124123011/https://www.theatlantic.com/politics/archive/2016/04/climate-change-game-theory-models/624253/| url-status = live}}</ref> etc) have been framed as cooperation challenges. As in the well-known [[Prisoner's dilemma|prisoner\u2019s dilemma]] scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.<ref name=\":14\" />\n\nA salient AI cooperation challenge is avoiding a \u2018race to the bottom\u2019.<ref name=\":16\">{{Cite report | publisher = Future of Humanity Institute, Oxford University| last1 = Armstrong| first1 = Stuart| last2 = Bostrom| first2 = Nick| last3 = Shulman| first3 = Carl| title = Racing to the Precipice: a Model of Artificial Intelligence Development}}</ref> In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political<ref name=\":17\">{{Cite report | publisher = Centre for the Governance of AI, Future of Humanity Institute, University of Oxford| last = Dafoe| first = Allan| title = AI Governance: A Research Agenda}}</ref> and technical<ref>{{Cite journal |last1=Dafoe |first1=Allan |last2=Hughes |first2=Edward |last3=Bachrach |first3=Yoram |last4=Collins |first4=Tantum |last5=McKee |first5=Kevin R. |last6=Leibo |first6=Joel Z. |last7=Larson |first7=Kate |last8=Graepel |first8=Thore |date=2020-12-15 |title=Open Problems in Cooperative AI |arxiv=2012.08630 }}</ref> efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in \u2018single-player\u2019 games).<ref name=\":15\">{{Cite journal |last1=Dafoe |first1=Allan |last2=Bachrach |first2=Yoram |last3=Hadfield |first3=Gillian |last4=Horvitz |first4=Eric |last5=Larson |first5=Kate |last6=Graepel |first6=Thore |date=2021 |title=Cooperative AI: machines must learn to find common ground |url=https://www.nature.com/articles/d41586-021-01170-0 |journal=Nature |volume=593 |issue=7857 |pages=33\u201336 |doi=10.1038/d41586-021-01170-0 |pmid=33947992 |bibcode=2021Natur.593...33D |s2cid=233740521 |accessdate=2022-11-24 |archive-date=2022-11-22 |archive-url=https://web.archive.org/web/20221122230552/https://www.nature.com/articles/d41586-021-01170-0 |url-status=live }}</ref> Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.<ref name=\":15\" />\n\n== In governance ==\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.<ref name=\":11\" /> It involves formulating and implementing concrete recommendations, as well as conducting more foundational research in order to inform what these recommendations should be. This section focuses on the aspects of AI governance that are specifically related to ensuring AI systems are safe and beneficial.\n\n=== Research ===\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.<ref>{{Cite journal |last=Crafts |first=Nicholas |date=2021-09-23 |title=Artificial intelligence as a general-purpose technology: an historical perspective |url=https://academic.oup.com/oxrep/article/37/3/521/6374675 |journal=Oxford Review of Economic Policy |language=en |volume=37 |issue=3 |pages=521\u2013536 |doi=10.1093/oxrep/grab012 |issn=0266-903X |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124130718/https://academic.oup.com/oxrep/article/37/3/521/6374675 |url-status=live }}</ref> Some work has focused on anticipating specific risks that may arise from these impacts \u2013 for example, risks from mass unemployment,<ref>{{Cite journal |last1=\u8449\u4ff6\u798e |last2=\u9ec3\u5b50\u541b |last3=\u5f35\u5a81\u96ef |last4=\u8cf4\u5fd7\u6a2b |date=2020-12-01 |title=Labor Displacement in Artificial Intelligence Era: A Systematic Literature Review |journal=\u81fa\u7063\u6771\u4e9e\u6587\u660e\u7814\u7a76\u5b78\u520a |language=en |volume=17 |issue=2 |doi=10.6163/TJEAS.202012_17(2).0002 |issn=1812-6243}}</ref> weaponization,<ref>{{Cite journal |last=Johnson |first=James |date=2019-04-03 |title=Artificial intelligence & future warfare: implications for international security |url=https://www.tandfonline.com/doi/full/10.1080/14751798.2019.1600800 |journal=Defense & Security Analysis |language=en |volume=35 |issue=2 |pages=147\u2013169 |doi=10.1080/14751798.2019.1600800 |s2cid=159321626 |issn=1475-1798 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124125204/https://www.tandfonline.com/doi/full/10.1080/14751798.2019.1600800 |url-status=live }}</ref> disinformation,<ref>{{Cite journal |last=Kertysova |first=Katarina |date=2018-12-12 |title=Artificial Intelligence and Disinformation: How AI Changes the Way Disinformation is Produced, Disseminated, and Can Be Countered |url=https://brill.com/view/journals/shrs/29/1-4/article-p55_55.xml |journal=Security and Human Rights |volume=29 |issue=1\u20134 |pages=55\u201381 |doi=10.1163/18750230-02901005 |s2cid=216896677 |issn=1874-7337 |access-date=2022-11-28 |archive-date=2022-11-24 |archive-url=https://web.archive.org/web/20221124125204/https://brill.com/view/journals/shrs/29/1-4/article-p55_55.xml |url-status=live }}</ref> surveillance,<ref>{{Cite conference| publisher = Carnegie Endowment for International Peace| last = Feldstein| first = Steven| title = The Global Expansion of AI Surveillance| date = 2019}}</ref> and the concentration of power.<ref>{{Cite book |url=https://www.worldcat.org/oclc/1099435014 |title=The economics of artificial intelligence : an agenda |date=2019 |others=Ajay Agrawal, Joshua Gans, Avi Goldfarb |isbn=978-0-226-61347-5 |location=Chicago |oclc=1099435014 |access-date=2022-11-28 |archive-date=2023-03-15 |archive-url=https://web.archive.org/web/20230315184354/https://www.worldcat.org/title/1099435014 |url-status=live }}</ref> Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,<ref>{{Cite journal |last1=Whittlestone |first1=Jess |last2=Clark |first2=Jack |date=2021-08-31 |title=Why and How Governments Should Monitor AI Development |arxiv=2108.12427 }}</ref> the availability of AI models,<ref name=\":20\">{{Cite web| last = Shevlane| first = Toby| title = Sharing Powerful AI Models {{!}} GovAI Blog| work = Center for the Governance of AI| accessdate = 2022-11-24| date = 2022| url = https://www.governance.ai/post/sharing-powerful-ai-models| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124125202/https://www.governance.ai/post/sharing-powerful-ai-models| url-status = live}}</ref> and \u2018race to the bottom\u2019 dynamics.<ref name=\":16\" /><ref>{{Cite journal |last1=Askell |first1=Amanda |last2=Brundage |first2=Miles |last3=Hadfield |first3=Gillian |date=2019-07-10 |title=The Role of Cooperation in Responsible AI Development |arxiv=1907.04534 }}</ref> Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \u201cit may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution.\u201d<ref name=\":17\" />\n\n=== Government action ===\n{{See also|Regulation of artificial intelligence}}\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \u201crush to regulate in ignorance.\u201d<ref>{{Cite web| last = Ziegler| first = Bart| title = Is It Time to Regulate AI?| work = WSJ| accessdate = 2022-11-24| url = https://www.wsj.com/articles/is-it-time-to-regulate-ai-11649433600| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124125645/https://www.wsj.com/articles/is-it-time-to-regulate-ai-11649433600| url-status = live}}</ref><ref>{{Cite journal |last=Reed |first=Chris |date=2018-09-13 |title=How should we regulate artificial intelligence? |journal=Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences |language=en |volume=376 |issue=2128 |pages=20170360 |doi=10.1098/rsta.2017.0360 |issn=1364-503X |pmc=6107539 |pmid=30082306|bibcode=2018RSPTA.37670360R }}</ref> Others, such as business magnate [[Elon Musk]], call for pre-emptive action to mitigate catastrophic risks.<ref>{{Cite web| last = Belton| first = Keith B.| title = How Should AI Be Regulated?| work = IndustryWeek| accessdate = 2022-11-24| date = 2019-03-07| url = https://www.industryweek.com/technology-and-iiot/article/22027274/how-should-ai-be-regulated| archive-date = 2022-01-29| archive-url = https://web.archive.org/web/20220129114109/https://www.industryweek.com/technology-and-iiot/article/22027274/how-should-ai-be-regulated| url-status = live}}</ref> To date, very little AI safety regulation has been passed at the national level, though many bills have been introduced. A prominent example is the [[Artificial Intelligence Act|European Union\u2019s Artificial Intelligence Act]], which regulates certain \u2018high risk\u2019 AI applications and restricts potentially harmful uses such as facial recognition, subliminal manipulation, and social credit scoring.\n\nOutside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \u201cassure that systems are aligned with goals and values, including safety, robustness and trustworthiness.\"<ref>{{Citation| last = National Security Commission on Artificial Intelligence| title = Final Report| date = 2021}}</ref> Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present - development and deployment should cease in a safe manner until risks can be sufficiently managed.\"<ref>{{Cite journal| last = National Institute of Standards and Technology| title = AI Risk Management Framework| journal = NIST| accessdate = 2022-11-24| date = 2021-07-12| url = https://www.nist.gov/itl/ai-risk-management-framework| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124130402/https://www.nist.gov/itl/ai-risk-management-framework| url-status = live}}</ref>\n\nIn September 2021, the [[People's Republic of China]] published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The [[United Kingdom]] published its 10-year National AI Strategy,<ref>{{Cite web| last = Richardson| first = Tim| title = Britain publishes 10-year National Artificial Intelligence Strategy| accessdate = 2022-11-24| date = 2021| url = https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/| archive-date = 2023-02-10| archive-url = https://web.archive.org/web/20230210114137/https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/| url-status = live}}</ref> which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously.\"<ref name=\":18\">{{Cite web| last = Office for Artificial Intelligence, Department for Digital, Culture, Media & Sport, and Department for Business, Energy & Industrial Strategy| title = Guidance: National AI Strategy| work = GOV.UK| accessdate = 2022-11-24| date = 2021| url = https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version| archive-date = 2023-02-10| archive-url = https://web.archive.org/web/20230210114139/https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version| url-status = live}}</ref> The strategy describes actions to assess long-term AI risks, including catastrophic risks.<ref name=\":18\" />\n\nGovernment organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The [[Intelligence Advanced Research Projects Activity]] initiated the TrojAI project to identify and protect against [[Trojan horse (computing)|Trojan attacks]] on AI systems.<ref>{{Cite web| last1 = Office of the Director of National Intelligence| last2 = Office of the Director of National Intelligence, Intelligence Advanced Research Projects Activity| title = IARPA - TrojAI| accessdate = 2022-11-24| url = https://www.iarpa.gov/research-programs/trojai| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124131956/https://www.iarpa.gov/research-programs/trojai| url-status = live}}</ref> The [https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception Defense Advanced Research Projects Agency] engages in research on [[explainable artificial intelligence]] and improving robustness against [[Adversarial machine learning|adversarial attacks]]<ref>{{Cite web| last = Turek| first = Matt| title = Explainable Artificial Intelligence| accessdate = 2022-11-24| url = https://www.darpa.mil/program/explainable-artificial-intelligence| archive-date = 2021-02-19| archive-url = https://web.archive.org/web/20210219210013/https://www.darpa.mil/program/explainable-artificial-intelligence| url-status = live}}</ref><ref>{{Cite web| last = Draper| first = Bruce| title = Guaranteeing AI Robustness Against Deception| work = Defense Advanced Research Projects Agency| accessdate = 2022-11-24| url = https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception| archive-date = 2023-01-09| archive-url = https://web.archive.org/web/20230109021433/https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception| url-status = live}}</ref> and The National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions in funding for empirical AI safety research.<ref>{{Cite web | last = National Science Foundation | title = Safe Learning-Enabled Systems | accessdate = 2023-02-27 | url = https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems | archive-date = 2023-02-26 | archive-url = https://web.archive.org/web/20230226190627/https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems | url-status = live }}</ref>\n\n=== Corporate self-regulation ===\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.<ref>{{Cite journal |last1=M\u00e4ntym\u00e4ki |first1=Matti |last2=Minkkinen |first2=Matti |last3=Birkstedt |first3=Teemu |last4=Viljanen |first4=Mika |date=2022 |title=Defining organizational AI governance |url=https://link.springer.com/10.1007/s43681-022-00143-x |journal=AI and Ethics |language=en |volume=2 |issue=4 |pages=603\u2013609 |doi=10.1007/s43681-022-00143-x |s2cid=247119668 |issn=2730-5953 |access-date=2022-11-28 |archive-date=2023-03-15 |archive-url=https://web.archive.org/web/20230315184339/https://link.springer.com/article/10.1007/s43681-022-00143-x |url-status=live }}</ref> One aim of governance researchers is to shape these norms.[?] Examples of safety recommendations found in the literature include performing third-party auditing,<ref name=\":19\">{{Cite journal |last1=Brundage |first1=Miles |last2=Avin |first2=Shahar |last3=Wang |first3=Jasmine |last4=Belfield |first4=Haydn |last5=Krueger |first5=Gretchen |last6=Hadfield |first6=Gillian |last7=Khlaaf |first7=Heidy |last8=Yang |first8=Jingying |last9=Toner |first9=Helen |last10=Fong |first10=Ruth |last11=Maharaj |first11=Tegan |last12=Koh |first12=Pang Wei |last13=Hooker |first13=Sara |last14=Leung |first14=Jade |last15=Trask |first15=Andrew |date=2020-04-20 |title=Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims |arxiv=2004.07213 }}</ref> offering bounties for finding failures,<ref name=\":19\" /> sharing AI incidents<ref name=\":19\" /> (an AI incident database was created for this purpose),<ref>{{Cite web| title = Welcome to the Artificial Intelligence Incident Database| accessdate = 2022-11-24| url = https://incidentdatabase.ai/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124132715/https://incidentdatabase.ai/| url-status = live}}</ref> following guidelines to determine whether to publish research or models,<ref name=\":20\" /> and improving information and cyber security in AI labs.<ref>{{Cite web| last1 = Wiblin| first1 = Robert| last2 = Harris| first2 = Keiran| title = Nova DasSarma on why information security may be critical to the safe development of AI systems| work = 80,000 Hours| accessdate = 2022-11-24| date = 2022| url = https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124132927/https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/| url-status = live}}</ref>\n\nCompanies have also made concrete commitments. Cohere, OpenAI, and AI21 proposed and agreed on \u201cbest practices for deploying language models,\u201d focusing on mitigating misuse.<ref>{{Cite web| last = OpenAI| title = Best Practices for Deploying Language Models| work = OpenAI| accessdate = 2022-11-24| date = 2022-06-02| url = https://openai.com/blog/best-practices-for-deploying-language-models/| archive-date = 2023-03-15| archive-url = https://web.archive.org/web/20230315184334/https://openai.com/blog/best-practices-for-deploying-language-models/| url-status = live}}</ref> To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \u201cif a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\u201d<ref>{{Cite web| last = OpenAI| title = OpenAI Charter| work = OpenAI| accessdate = 2022-11-24| url = https://openai.com/charter/| archive-date = 2021-03-04| archive-url = https://web.archive.org/web/20210304235618/https://openai.com/charter/| url-status = live}}</ref> Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles<ref name=\":21\" /> and the Autonomous Weapons Open Letter.<ref>{{Cite web| last = Future of Life Institute| title = Autonomous Weapons Open Letter: AI & Robotics Researchers| work = Future of Life Institute| accessdate = 2022-11-24| date = 2016| url = https://futureoflife.org/open-letter/open-letter-autonomous-weapons-ai-robotics/| archive-date = 2022-11-24| archive-url = https://web.archive.org/web/20221124114555/https://futureoflife.org/open-letter/open-letter-autonomous-weapons-ai-robotics/| url-status = live}}</ref>\n\n== See also ==\n{{columns-list|colwidth=30em|\n* [[Ethics of artificial intelligence]]\n* [[Existential risk from artificial general intelligence]]\n* [[Robot ethics]]\n\n; Books\n* ''[[The Alignment Problem]]''\n* ''[[Human Compatible]]''\n* ''[[Normal Accidents]]''\n* ''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n; Organizations\n* [[Alignment Research Center]]\n* [[Center for Security and Emerging Technology]]\n* [[Centre for the Study of Existential Risk]]\n* [[Future of Humanity Institute]]\n* [[Institute for Ethics and Emerging Technologies]]\n* [[Machine Intelligence Research Institute]]\n* [[Center for Human-Compatible AI]]\n* [[Leverhulme Centre for the Future of Intelligence]]\n* [[Partnership on AI]]\n}}\n\n== Notes ==\n{{Notelist}}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [[arxiv:2108.07258|On the Opportunities and Risks of Foundation Models]]\n* [[arxiv:2109.13916.pdf|Unsolved Problems in ML Safety]]\n* [https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/ AI Accidents: An Emerging Threat]\n* [https://mitpress.mit.edu/9780262533690/engineering-a-safer-world/ Engineering a Safer World]\n\n{{Existential risk from artificial intelligence|state=expanded}}\n\n[[Category:Artificial intelligence]]\n[[Category:Existential risk from artificial general intelligence]]"}