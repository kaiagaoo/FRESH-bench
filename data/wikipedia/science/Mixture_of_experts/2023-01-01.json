{"title": "Mixture of experts", "page_id": 54238535, "revision_id": 1118565578, "revision_timestamp": "2022-10-27T18:37:54Z", "content": "'''Mixture of experts''' (MoE) refers to a [[machine learning]] technique where multiple expert networks (learners) are used to divide a problem space into homogeneous regions.<ref>{{cite journal|doi=10.1016/j.ymssp.2015.05.009|title=Variational Bayesian mixture of experts models and sensitivity analysis for nonlinear dynamical systems|journal=Mechanical Systems and Signal Processing |volume=66\u201367|pages=178\u2013200|year=2016|last1=Baldacchino|first1=Tara|last2=Cross|first2=Elizabeth J.|last3=Worden|first3=Keith|last4=Rowson|first4=Jennifer|bibcode=2016MSSP...66..178B}}</ref> It differs from [[ensemble learning|ensemble techniques]] in that typically only a few, or 1, expert model will be run, rather than combining results from all models.\n\nAn example from [[computer vision]] is combining one [[neural network]] model for human detection with another for [[pose estimation]].\n\n== Hierarchical mixture ==\nIf the output is conditioned on multiple levels of (probabilistic) gating functions, the mixture is called a hierarchical mixture of experts.<ref>{{cite web|last1=Hauskrecht|first1=Milos|title=Ensamble methods: Mixtures of experts (Presentation)|url=https://people.cs.pitt.edu/~milos/courses/cs2750-Spring04/lectures/class22.pdf}}</ref>\n\nA gating network decides which expert to use for each input region. Learning thus consists of learning the parameters of:\n\n* individual learners and \n* gating network.\n\n== Applications ==\n[[Meta Platforms|Meta]] uses MoE in its NLLB-200 system. It uses multiple MoE models that share capacity for use by low-resource [[language models]] with relatively little data.<ref>{{Cite web |last=Rodriguez |first=Jesus |title=\ud83d\uddfa Edge#214: NLLB-200, Meta AI\u2019s New Super Model that Achieved New Milestones in Machine Translations Across 200 Languages |url=https://thesequence.substack.com/p/edge214 |access-date=2022-08-04 |website=thesequence.substack.com |language=en}}</ref>\n\n==References==\n{{Reflist}}\n\n==Extra reading==\n*{{cite journal|last1=Masoudnia|first1=Saeed|last2=Ebrahimpour|first2=Reza|title=Mixture of experts: a literature survey|journal=Artificial Intelligence Review|date=12 May 2012|volume=42|issue=2|pages=275\u2013293|doi=10.1007/s10462-012-9338-y|s2cid=3185688}}\n\n[[Category:Machine learning algorithms]]"}