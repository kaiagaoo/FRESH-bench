{"title": "Mixture of experts", "page_id": 54238535, "revision_id": 1192390009, "revision_timestamp": "2023-12-29T02:21:27Z", "content": "{{Short description|Machine learning technique}}\n{{Machine learning}}'''Mixture of experts''' (MoE) is a [[machine learning]] technique where multiple expert networks (learners) are used to divide a problem space into homogeneous regions.<ref>{{cite journal|doi=10.1016/j.ymssp.2015.05.009|title=Variational Bayesian mixture of experts models and sensitivity analysis for nonlinear dynamical systems|journal=Mechanical Systems and Signal Processing |volume=66\u201367|pages=178\u2013200|year=2016|last1=Baldacchino|first1=Tara|last2=Cross|first2=Elizabeth J.|last3=Worden|first3=Keith|last4=Rowson|first4=Jennifer|bibcode=2016MSSP...66..178B}}</ref> It differs from [[ensemble learning|ensemble techniques]] in that typically only one or a few expert models will be run, rather than combining results from all models.\n\n== Basic theory ==\nIn mixture of experts, we always have the following ingredients, but they are constructed and combined differently.\n\n* There are experts <math>f_1, ..., f_n</math>, each taking in the same input <math>x</math>, and produces outputs <math>f_1(x), ..., f_n(x)</math>.\n* There is a single weighting function (aka gating function) <math>w</math>, which takes in <math>x</math> and produces a vector of outputs <math>(w(x)_1, ..., w(x)_n)</math>.\n* <math>\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)</math> is the set of parameters. The parameter <math>\\theta_0</math> is for the weighting function.\n* Given an input <math>x</math>, the mixture of experts produces a single combined output by combining <math>f_1(x), ..., f_n(x)</math> according to the weights <math>w(x)_1, ..., w(x)_n</math> in some way.\nBoth the experts and the weighting function are trained by minimizing some form of loss function, generally by gradient descent. There is a lot of freedom in choosing the precise form of experts, the weighting function, and the loss function.\n\n=== Meta-pi network ===\nThe '''meta-pi network''', reported by Hampshire and Waibel,<ref>{{Cite journal |last1=Hampshire |first1=J.B. |last2=Waibel |first2=A. |date=July 1992 |title=The Meta-Pi network: building distributed knowledge representations for robust multisource pattern recognition |url=https://isl.anthropomatik.kit.edu/downloads/The_Meta-Pi_Network-_Building_Distributed_Knowledge_Representations_for_Robust_Multi-Source_Pattern_Recognition.pdf |journal=IEEE Transactions on Pattern Analysis and Machine Intelligence |volume=14 |issue=7 |pages=751\u2013769 |doi=10.1109/34.142911}}</ref> uses <math>f(x) = \\sum_i w(x)_i f_i(x)</math> as the output. The model is trained by performing gradient descent on the mean-squared error loss <math>L := \\frac 1N \\sum_k \\|y_k - f(x_k)\\|^2</math>. The experts may be arbitrary functions.\n\nIn their original publication, they were solving the problem of classifying [[Phoneme|phonemes]] in speech signal from 6 different Japanese speakers, 2 females and 4 males. They trained 6 experts, each being a \"time-delayed neural network\"<ref>{{Cite book |author=Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, Kevin J. Lang |editor-first1=Yves |editor-first2=David E. |editor-last1=Chauvin |editor-last2=Rumelhart |chapter=Phoneme Recognition Using Time-Delay Neural Networks* |date=1995 |chapter-url=https://www.taylorfrancis.com/chapters/edit/10.4324/9780203763247-2/phoneme-recognition-using-time-delay-neural-networks-alexander-waibel-toshiyuki-hanazawa-geoffrey-hinton-kiyohiro-shikano-kevin-lang |title=Backpropagation  |publisher=Psychology Press |doi=10.4324/9780203763247 |isbn=978-0-203-76324-7}}</ref> (essentially a multilayered [[Convolutional neural network|convolution network]] over the [[Mel-frequency cepstrum|mel spectrogram]]). They found that the resulting mixture of experts dedicated 5 experts for 5 of the speakers, but the 6th (male) speaker does not have a dedicated expert, instead his voice was classified by a linear combination of the experts for the other 3 male speakers.\n\n=== Adaptive mixtures of local experts ===\n\nThe '''adaptive mixtures of local experts''' <ref>{{Cite journal |last1=Nowlan |first1=Steven |last2=Hinton |first2=Geoffrey E |date=1990 |title=Evaluation of Adaptive Mixtures of Competing Experts |url=https://proceedings.neurips.cc/paper/1990/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Morgan-Kaufmann |volume=3}}</ref><ref>{{Cite journal |last1=Jacobs |first1=Robert A. |last2=Jordan |first2=Michael I. |last3=Nowlan |first3=Steven J. |last4=Hinton |first4=Geoffrey E. |date=February 1991 |title=Adaptive Mixtures of Local Experts |url=https://direct.mit.edu/neco/article/3/1/79-87/5560 |journal=Neural Computation |language=en |volume=3 |issue=1 |pages=79\u201387 |doi=10.1162/neco.1991.3.1.79 |pmid=31141872 |s2cid=572361 |issn=0899-7667}}</ref> uses a [[gaussian mixture model]]. Each expert simply predicts a gaussian distribution, and totally ignores the input. Specifically, the <math>i</math>-th expert predicts that the output is <math>y \\sim N(\\mu_i, I)</math>, where <math>\\mu_i</math> is a learnable parameter. The weighting function is a linear-softmax function:<math display=\"block\">w(x)_i = \\frac{e^{k_i^T x + b_i}}{\\sum_j e^{k_j^T x + b_j}}</math>The mixture of experts predict that the output is distributed according to the probability density function:<math display=\"block\">f_\\theta(y|x) \n= \\ln\\left[\\sum_i \\frac{e^{k_i^T x + b_i}}{\\sum_j e^{k_j^T x + b_j}} N(y | \\mu_i, I)\\right] \n= \\ln\\left[(2\\pi)^{-d/2} \\sum_i \\frac{e^{k_i^T x + b_i}}{\\sum_j e^{k_j^T x + b_j}} e^{-\\frac 12 \\|y-\\mu_i\\|^2}\\right]</math>It is trained by maximal likelihood estimation, that is, gradient ascent on <math>f(y|x)</math>. The gradient for the <math>i</math>-th expert is\n\n<math display=\"block\">\\nabla_{\\mu_i} f_\\theta(y|x) = \n\\frac{w(x)_i N(y|\\mu_i, I)}{\\sum_j w(x)_j N(y|\\mu_j, I)}\\; (y-\\mu_i) </math>\n\nand the gradient for the weighting function is<math display=\"block\">\\nabla_{[k_i, b_i]} f_\\theta(y|x) = \\begin{bmatrix}x\\\\ 1\\end{bmatrix} \\frac{w(x)_i}{\\sum_j w(x)_j N(y|\\mu_j, I)} \n(f_{i}(x)- f_\\theta(y|x))</math>\n\nFor each input-output pair <math>(x, y)</math>, the weighting function is changed to increase the weight on all experts that performed above average, and decrease the weight on all experts that performed below average. This encourages the weighting function to learn to select only the expects that make the right predictions for each input.\n\nThe <math>i</math>-th expert is changed to make its prediction closer to <math>y</math>, but the amount of change is proportional to <math>w(x)_i N(y|\\mu_i, I)</math>. This has a [[Bayesian probability|Bayesian]] interpretation. Given input <math>x</math>, the prior probability that expert <math>i</math> is the right one is <math>w(x)_i</math>, and <math>N(y|\\mu_i, I)</math> is the likelihood of evidence <math>y</math>. So, <math>\\frac{w(x)_i N(y|\\mu_i, I)}{\\sum_j w(x)_j N(y|\\mu_j, I)}</math> is the posterior probability for expert <math>i</math>, and so the rate of change for the <math>i</math>-th expert is proportional to its posterior probability. \n\nIn words, the experts that, in hindsight, seemed like the good experts to consult, are asked to learn on the example. The experts that, in hindsight, were not, are left alone.\n\nThe combined effect is that the experts become specialized: Suppose two experts are both good at predicting a certain kind of input, but one is slightly better, then the weighting function would eventually learn to favor the better one. After that happens, the lesser expert is unable to obtain a high gradient signal, and becomes even worse at predicting such kind of input. Conversely, the lesser expert can become better at predicting other kinds of input, and increasingly pulled away into another region. This has a positive feedback effect, causing each expert to move apart from the rest and take care of a local region alone (thus the name \"''local'' experts\").\n\n=== Hierarchical MoE ===\n'''Hierarchical mixtures of experts'''<ref name=\":0\" /><ref name=\":2\">{{Cite journal |last1=Jordan |first1=Michael I. |last2=Jacobs |first2=Robert A. |date=March 1994 |title=Hierarchical Mixtures of Experts and the EM Algorithm |url=https://direct.mit.edu/neco/article/6/2/181-214/5779 |journal=Neural Computation |language=en |volume=6 |issue=2 |pages=181\u2013214 |doi=10.1162/neco.1994.6.2.181 |issn=0899-7667}}</ref> uses multiple levels of gating in a tree. Each gating is a probability distribution over the next level of gatings, and the experts are on the leaf nodes of the tree. They are similar to [[Decision tree learning|decision trees]].\n\nFor example, a 2-level hierarchical MoE would have a first order gating function <math>w_i</math>, and second order gating functions <math>w_{j|i}</math> and experts <math>f_{j|i}</math>. The total prediction is then <math>\\sum_i w_i(x) \\sum_j w_{j|i}(x) f_{j|i}(x)</math>.\n\n=== Variants ===\nThe mixture of experts, being similar to the gaussian mixture model, can also be trained by the [[Expectation\u2013maximization algorithm#Gaussian mixture|expectation-maximization algorithm, just like gaussian mixture models]]. Specifically, during the expectation step, the \"burden\" for explaining each data point is assigned over the experts, and during the maximization step, the experts are trained to improve the explanations they got a high burden for, while the gate is trained to improve its burden assignment. This can converge faster than gradient ascent on the log-likelihood.<ref name=\":2\" /><ref name=\":3\">{{Cite journal |last1=Jordan |first1=Michael I. |last2=Xu |first2=Lei |date=1995-01-01 |title=Convergence results for the EM approach to mixtures of experts architectures |url=https://dx.doi.org/10.1016/0893-6080%2895%2900014-3 |journal=Neural Networks |volume=8 |issue=9 |pages=1409\u20131431 |doi=10.1016/0893-6080(95)00014-3 |issn=0893-6080 |hdl=1721.1/6620}}</ref>\n\nThe choice of gating function is often a softmax gating. Other than that, <ref>{{Cite journal |last1=Xu |first1=Lei |last2=Jordan |first2=Michael |last3=Hinton |first3=Geoffrey E |date=1994 |title=An Alternative Model for Mixtures of Experts |url=https://proceedings.neurips.cc/paper/1994/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=MIT Press |volume=7}}</ref> proposed using gaussian distributions, and <ref name=\":3\" /> proposed using exponential families.\n\nInstead of performing a weighted sum of all the experts, in hard MoE <ref>{{Cite journal |last1=Collobert |first1=Ronan |last2=Bengio |first2=Samy |last3=Bengio |first3=Yoshua |date=2001 |title=A Parallel Mixture of SVMs for Very Large Scale Problems |url=https://proceedings.neurips.cc/paper_files/paper/2001/hash/36ac8e558ac7690b6f44e2cb5ef93322-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=MIT Press |volume=14}}</ref> only the highest ranked expert is chosen. That is, <math>f(x) = f_{\\arg\\max_i w_i(x)}(x)</math>. This can accelerate training and inference time.<ref>{{Cite book |last1=Goodfellow |first1=Ian |title=Deep learning |last2=Bengio |first2=Yoshua |last3=Courville |first3=Aaron |date=2016 |publisher=The MIT press |isbn=978-0-262-03561-3 |series=Adaptive computation and machine learning |location=Cambridge, Mass |chapter=12: Applications}}</ref> \n\nThe experts can use more general forms of multivariant gaussian distributions. For example, <ref name=\":0\">{{Cite journal |last1=Jordan |first1=Michael |last2=Jacobs |first2=Robert |date=1991 |title=Hierarchies of adaptive experts |url=https://proceedings.neurips.cc/paper_files/paper/1991/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Morgan-Kaufmann |volume=4}}</ref> proposed <math>f_i(y|x) = N(y | A_i x + b_i, \\Sigma_i)</math>, where <math>A_i, b_i, \\Sigma_i</math> are learnable parameters. In words, each expert learns to do linear regression, with a learnable uncertainty estimate.  \n\nOne can use different experts than gaussian distributions. For example, one can use [[Laplace distribution]],<ref>{{Cite journal |last1=Nguyen |first1=Hien D. |last2=McLachlan |first2=Geoffrey J. |date=2016-01-01 |title=Laplace mixture of linear experts |url=https://www.sciencedirect.com/science/article/pii/S0167947314003089 |journal=Computational Statistics & Data Analysis |volume=93 |pages=177\u2013191 |doi=10.1016/j.csda.2014.10.016 |issn=0167-9473}}</ref> or [[Student's t-distribution]].<ref>{{Cite journal |last=Chamroukhi |first=F. |date=2016-07-01 |title=Robust mixture of experts modeling using the t distribution |url=https://www.sciencedirect.com/science/article/pii/S0893608016000435 |journal=Neural Networks |volume=79 |pages=20\u201336 |arxiv=1701.07429 |doi=10.1016/j.neunet.2016.03.002 |issn=0893-6080 |pmid=27093693 |s2cid=3171144}}</ref> For binary classification, it also proposed [[logistic regression]] experts, with<math display=\"block\">f_i(y|x) = \\begin{cases}\n\\frac{1}{1+e^{\\beta_i^T x + \\beta_{i,0}}}, & y = 0 \\\\\n1-\\frac{1}{1+e^{\\beta_i^T x + \\beta_{i,0}}}, & y= 1\n\\end{cases}    </math>where <math>\\beta_{i}, \\beta_{i, 0}    </math> are learnable parameters. This is later generalized for multi-class classification, with [[multinomial logistic regression]] experts.<ref>{{Cite journal |last1=Chen |first1=K. |last2=Xu |first2=L. |last3=Chi |first3=H. |date=1999-11-01 |title=Improved learning algorithms for mixture of experts in multiclass classification |url=https://www.sciencedirect.com/science/article/pii/S089360809900043X |journal=Neural Networks |volume=12 |issue=9 |pages=1229\u20131252 |doi=10.1016/S0893-6080(99)00043-X |pmid=12662629 |issn=0893-6080}}</ref> \n\n== Deep learning ==\nThe previous section described MoE as it was used before the era of [[deep learning]]. After deep learning, MoE found applications in running the largest models, as a simple way to perform ''[[conditional computation]]'': only parts of the model are used, the parts chosen according to what the input is.<ref>{{Cite arXiv |last1=Bengio |first1=Yoshua |last2=L\u00e9onard |first2=Nicholas |last3=Courville |first3=Aaron |date=2013 |title=Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation |class=cs.LG |eprint=1308.3432 }}</ref>\n\nThe earliest paper that applies MoE to deep learning is,<ref>{{Cite arXiv |last1=Eigen |first1=David |last2=Ranzato |first2=Marc'Aurelio |last3=Sutskever |first3=Ilya |date=2013 |title=Learning Factored Representations in a Deep Mixture of Experts |class=cs.LG |eprint=1312.4314 }}</ref> which proposes to use a different gating network at each layer in a deep neural network. Specifically, each gating is a linear-ReLU-linear-softmax network, and each expert is a linear-ReLU network.\n\nThe key design desideratum for MoE in deep learning is to reduce computing cost. Consequently, for each query, only a small subset of the experts should be queried. This makes MoE in deep learning different from classical MoE. In classical MoE, the output for each query is a weighted sum of ''all'' experts' outputs. In deep learning MoE, the output for each query can only involve a few experts' outputs. Consequently, the key design choice in MoE becomes routing: given a batch of queries, how to route the queries to the best experts.\n\n=== Sparsely-gated MoE layer ===\nThe '''sparsely-gated MoE layer''',<ref>{{Cite arXiv |last1=Shazeer |first1=Noam |last2=Mirhoseini |first2=Azalia |last3=Maziarz |first3=Krzysztof |last4=Davis |first4=Andy |last5=Le |first5=Quoc |last6=Hinton |first6=Geoffrey |last7=Dean |first7=Jeff |date=2017 |title=Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer |class=cs.LG |eprint=1701.06538}}</ref> published by researchers from [[Google Brain]], uses [[Feedforward neural network|feedforward networks]] as experts, and linear-softmax gating. Similar to the previously proposed hard MoE, they achieve sparsity by a weighted sum of only the top-k experts, instead of the weighted sum of all of them. Specifically, in a MoE layer, there are [[Feedforward neural network|feedforward networks]] <math>f_1, ..., f_n</math>, and a gating network <math>w</math>. The gating network is defined by <math>w(x) = \\mathrm{softmax}(\\mathrm{top}_k(W x + \\text{noise}))  </math>, where <math>\\mathrm{top}_k</math> is a function that keeps the top-k entries of a vector the same, but sets all other entries to <math>-\\infty</math>. The addition of noise helps with load balancing. \n\nThe choice of <math>k</math> is a hyperparameter that is chosen according to application. Typical values are <math>k = 1, 2</math>. The <math>k = 1</math> version is also called the Switch Transformer.<ref name=\":1\" /> \n\nAs demonstration, they trained a series of models for machine translation with alternating layers of MoE and [[Long short-term memory|LSTM]], and compared with deep LSTM models.<ref>{{Cite arXiv |eprint=1609.08144 |class=cs.CL |first1=Yonghui |last1=Wu |first2=Mike |last2=Schuster |title=Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation |date=2016 |last3=Chen |first3=Zhifeng |last4=Le |first4=Quoc V. |last5=Norouzi |first5=Mohammad |last6=Macherey |first6=Wolfgang |last7=Krikun |first7=Maxim |last8=Cao |first8=Yuan |last9=Gao |first9=Qin |last10=Macherey |first10=Klaus |last11=Klingner |first11=Jeff |last12=Shah |first12=Apurva |last13=Johnson |first13=Melvin |last14=Liu |first14=Xiaobing |last15=Kaiser |first15=\u0141ukasz}}</ref> Table 3 shows that the MoE models used less inference time compute, despite having 30x more parameters. \n\nVanilla MoE tend to have issues of load balancing: some experts are consulted often, while other experts rarely or not at all. To encourage the gate to select each expert with equal frequency (proper load balancing) within each batch, each MoE layer has two auxiliary loss functions. This is improved by <ref name=\":1\">{{Cite journal |last1=Fedus |first1=William |last2=Zoph |first2=Barret |last3=Shazeer |first3=Noam |date=2022-01-01 |title=Switch transformers: scaling to trillion parameter models with simple and efficient sparsity |url=https://dl.acm.org/doi/abs/10.5555/3586589.3586709 |journal=The Journal of Machine Learning Research |volume=23 |issue=1 |pages= 5232\u20135270 |arxiv=2101.03961 |issn=1532-4435}}</ref> into a single auxiliary loss function. Specifically, let <math>n</math> be the number of experts, then for a given batch of queries <math>\\{x_1, x_2, ..., x_T\\}</math>, the auxiliary loss for the batch is<math display=\"block\">n\\sum_{i=1}^n f_i P_i</math>Here, <math>f_i = \\frac 1T \\#(\\text{queries sent to expert }i)</math> is the fraction of time where expert <math>i</math> is ranked highest, and <math>P_i = \\frac 1T \\sum_{j=1}^T w_i(x_j)</math> is the fraction of weight on expert <math>i</math>. This loss is minimized at <math>1</math>, precisely when every expert has equal weight <math>1/n</math> in all situations.\n\n=== Routing ===\nIn sparsely-gated MoE, only the top-k experts are queried, and their outputs are weighted-summed. There are other methods.<ref name=\":4\">{{Cite arXiv |last1=Zoph |first1=Barret |last2=Bello |first2=Irwan |last3=Kumar |first3=Sameer |last4=Du |first4=Nan |last5=Huang |first5=Yanping |last6=Dean |first6=Jeff |last7=Shazeer |first7=Noam |last8=Fedus |first8=William |date=2022 |title=ST-MoE: Designing Stable and Transferable Sparse Expert Models |class=cs.CL |eprint=2202.08906}}</ref> \n\nIn Hash MoE,<ref>{{Cite journal |last1=Roller |first1=Stephen |last2=Sukhbaatar |first2=Sainbayar |last3=szlam |first3=arthur |last4=Weston |first4=Jason |date=2021 |title=Hash Layers For Large Sparse Models |url=https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html |journal=Advances in Neural Information Processing Systems |publisher=Curran Associates |volume=34 |pages=17555\u201317566|arxiv=2106.04426 }}</ref> routing is performed deterministically by a hash function, fixed before learning begins. For example, if the model is a 4-layered Transformer, and input is a token for word \"eat\", and the hash of \"eat\" is <math>(1, 4, 2, 3)</math>, then the token would be routed to the 1st expert in layer 1, 4th expert in layer 2, etc. Despite its simplicity, it achieves competitive performance as sparsely gated MoE with <math>k = 1</math>. \n\nIn soft MoE, suppose in each batch, each expert can process <math>p</math> queries, then there are <math>n\\times p</math> queries that can be assigned per batch. Now for each batch of queries <math>\\{x_1, x_2, ..., x_T\\}</math>, the soft MoE layer computes an array <math>w_{i, j, k}</math>, such that <math>(w_{i, j, 1}, ..., w_{i, j, T})</math> is a probability distribution over queries, and the <math>i</math>-th expert's <math>j</math>-th query is <math>\\sum_k w_{i,j,k}x_k</math>.<ref>{{Cite arXiv |last1=Puigcerver |first1=Joan |last2=Riquelme |first2=Carlos |last3=Mustafa |first3=Basil |last4=Houlsby |first4=Neil |date=2023 |title=From Sparse to Soft Mixtures of Experts |class=cs.LG |eprint=2308.00951}}</ref> However, this does not work with autoregressive modelling, since the weights <math>w_{i, j, k}</math> over one token depends on all other tokens'.<ref>{{Citation |last=Wang |first=Phil |title=lucidrains/soft-moe-pytorch |date=2023-10-04 |url=https://github.com/lucidrains/soft-moe-pytorch |access-date=2023-10-08}}</ref>\n\nOther approaches include solving it as a [[Linear programming|constrained linear programming]] problem,<ref>{{Cite journal |last1=Lewis |first1=Mike |last2=Bhosale |first2=Shruti |last3=Dettmers |first3=Tim |last4=Goyal |first4=Naman |last5=Zettlemoyer |first5=Luke |date=2021-07-01 |title=BASE Layers: Simplifying Training of Large, Sparse Models |url=https://proceedings.mlr.press/v139/lewis21a.html |journal=Proceedings of the 38th International Conference on Machine Learning |language=en |publisher=PMLR |pages=6265\u20136274|arxiv=2103.16716 }}</ref> making each expert choose the top-k queries it wants (instead of each query choosing the top-k experts for it),<ref>{{Cite journal |last1=Zhou |first1=Yanqi |last2=Lei |first2=Tao |last3=Liu |first3=Hanxiao |last4=Du |first4=Nan |last5=Huang |first5=Yanping |last6=Zhao |first6=Vincent |last7=Dai |first7=Andrew M. |last8=Chen |first8=Zhifeng |last9=Le |first9=Quoc V. |last10=Laudon |first10=James |date=2022-12-06 |title=Mixture-of-Experts with Expert Choice Routing |url=https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html |journal=Advances in Neural Information Processing Systems |language=en |volume=35 |pages=7103\u20137114|arxiv=2202.09368 }}</ref> using [[reinforcement learning]] to train the routing algorithm (since picking an expert is a discrete action, like in RL).<ref>{{Cite arXiv |last1=Bengio |first1=Emmanuel |last2=Bacon |first2=Pierre-Luc |last3=Pineau |first3=Joelle |last4=Precup |first4=Doina |date=2015 |title=Conditional Computation in Neural Networks for faster models |class=cs.LG |eprint=1511.06297}}</ref>\n\n=== Capacity factor ===\nSuppose there are <math>n</math> experts in a layer. For a given batch of queries <math>\\{x_1, x_2, ..., x_T\\}</math>, each query is routed to one or more experts. For example, if each query is routed to one expert as in Switch Transformers, and if the experts are load-balanced, then each expert should expect on average <math>T/n</math> queries in a batch. In practice, the experts cannot expect perfect load balancing: in some batches, one expert might be underworked, while in other batches, it would be overworked.\n\nSince the inputs cannot move through the layer until every expert in the layer has finished the queries it is assigned, load balancing is important. As a hard constraint on load balancing, there is the '''capacity factor''': each expert is only allowed to process up to <math>c \\cdot T/n</math> queries in a batch. <ref name=\":4\" /> found <math>c \\in [1.25, 2]</math> to work in practice.\n\n=== Applications to Transformer models ===\nMoE layers are used in very large [[Transformer (machine learning model)|Transformer models]], for which learning and inferring over the full model is too costly. In Transformer models, the MoE layers are often used to select the [[Feedforward neural network|feedforward layers]] (typically a linear-ReLU-linear network), appearing in each Transformer block after the multiheaded attention. This is because the feedforward layers take up an increasing portion of the computing cost as models grow larger. For example, 90% of parameters in PALM-540B are in feedforward layers.<ref>{{Cite web |title=Transformer Deep Dive: Parameter Counting |url=https://orenleung.com/transformer-parameter-counting |access-date=2023-10-10 |website=Transformer Deep Dive: Parameter Counting |language=en}}</ref>\n\nA series of large language models from [[Google]] used MoE. GShard<ref>{{Cite arXiv |last1=Lepikhin |first1=Dmitry |last2=Lee |first2=HyoukJoong |last3=Xu |first3=Yuanzhong |last4=Chen |first4=Dehao |last5=Firat |first5=Orhan |last6=Huang |first6=Yanping |last7=Krikun |first7=Maxim |last8=Shazeer |first8=Noam |last9=Chen |first9=Zhifeng |date=2020 |title=GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding |class=cs.CL |eprint=2006.16668}}</ref> uses MoE with up to top-2 experts per layer. Specifically, the top-1 expert is always selected, and the top-2th expert is selected with probability proportional to that experts' weight according to the gating function. Later, GLaM<ref>{{Cite arXiv |last1=Du |first1=Nan |last2=Huang |first2=Yanping |last3=Dai |first3=Andrew M. |last4=Tong |first4=Simon |last5=Lepikhin |first5=Dmitry |last6=Xu |first6=Yuanzhong |last7=Krikun |first7=Maxim |last8=Zhou |first8=Yanqi |last9=Yu |first9=Adams Wei |last10=Firat |first10=Orhan |last11=Zoph |first11=Barret |last12=Fedus |first12=Liam |last13=Bosma |first13=Maarten |last14=Zhou |first14=Zongwei |last15=Wang |first15=Tao |date=2021 |title=GLaM: Efficient Scaling of Language Models with Mixture-of-Experts |class=cs.CL |eprint=2112.06905}}</ref> demonstrated a language model with 1.2 trillion parameters, each MoE layer using top-2 out of 64 experts. Switch Transformers<ref name=\":1\" /> use top-1 in all MoE layers. \n\nThe NLLB-200 by [[Meta AI]] is a machine translation model for 200 languages.<ref>{{Cite web |date=2022-06-19 |title=200 languages within a single AI model: A breakthrough in high-quality machine translation |url=https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/ |archive-url=https://web.archive.org/web/20230109051700/https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/ |archive-date=2023-01-09 |website=ai.facebook.com |language=en}}</ref> Each MoE layer uses a hierarchical MoE with two levels. On the first level, the gating function chooses to use either a \"shared\" feedforward layer, or to use the experts. If using the experts, then another gating function computes the weights and chooses the top-2 experts (see Figure 19).<ref>{{Cite arXiv |last1=NLLB Team |last2=Costa-juss\u00e0 |first2=Marta R. |last3=Cross |first3=James |last4=\u00c7elebi |first4=Onur |last5=Elbayad |first5=Maha |last6=Heafield |first6=Kenneth |last7=Heffernan |first7=Kevin |last8=Kalbassi |first8=Elahe |last9=Lam |first9=Janice |last10=Licht |first10=Daniel |last11=Maillard |first11=Jean |last12=Sun |first12=Anna |last13=Wang |first13=Skyler |last14=Wenzek |first14=Guillaume |last15=Youngblood |first15=Al |date=2022 |title=No Language Left Behind: Scaling Human-Centered Machine Translation |class=cs.CL |eprint=2207.04672}}</ref>\n\nMoE large language models can be adapted for downstream tasks by [[instruction tuning]].<ref>{{Cite arXiv|last1=Shen |first1=Sheng |last2=Hou |first2=Le |last3=Zhou |first3=Yanqi |last4=Du |first4=Nan |last5=Longpre |first5=Shayne |last6=Wei |first6=Jason |last7=Chung |first7=Hyung Won |last8=Zoph |first8=Barret |last9=Fedus |first9=William |last10=Chen |first10=Xinyun |last11=Vu |first11=Tu |last12=Wu |first12=Yuexin |last13=Chen |first13=Wuyang |last14=Webson |first14=Albert |last15=Li |first15=Yunxuan |date=2023 |title=Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models |class=cs.CL |eprint=2305.14705}}</ref>\n\nGenerally, MoE are used when dense models have become too costly. As of 2023, the largest models tend to be [[Large language model|large language models]]. Outside of those, Vision MoE<ref>{{Cite journal |last1=Riquelme |first1=Carlos |last2=Puigcerver |first2=Joan |last3=Mustafa |first3=Basil |last4=Neumann |first4=Maxim |last5=Jenatton |first5=Rodolphe |last6=Susano Pinto |first6=Andr\u00e9 |last7=Keysers |first7=Daniel |last8=Houlsby |first8=Neil |date=2021 |title=Scaling Vision with Sparse Mixture of Experts |url=https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html |journal=Advances in Neural Information Processing Systems |volume=34 |pages=8583\u20138595|arxiv=2106.05974 }}</ref> is a Transformer model with MoE layers. They demonstrated it by training a model with 15 billion parameters.\n\nIn December 2023 the french startup Mistral AI released the open source model Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. It is licensed under Apache 2.0 and outperforms Llama 2 70B on most benchmarks with 6x faster inference. Also compared to [[GPT-3.5]] the model shows superior performance according to the companies blog post.<ref>\n{{Cite web |date=2023-12-11 |title=200 Mixtral of experts: A high quality Sparse Mixture-of-Experts. |url=https://mistral.ai/news/mixtral-of-experts/|website=mistral.ai |language=en}}\n</ref> Mixtral 8x7B is noted for its cost/performance trade-offs, handling multiple languages (English, French, Italian, German, and Spanish), and strong code generation performance. The model is a decoder-only model with 46.7B total parameters, but uses only 12.9B parameters per token. Mixtral 8x7B is also available in an instructed version optimized for instruction following.\n\n== Further reading ==\n\n* Before deep learning era\n** {{Cite book |last1=McLachlan |first1=Geoffrey J. |title=Finite mixture models |last2=Peel |first2=David |date=2000 |publisher=John Wiley & Sons, Inc |isbn=978-0-471-00626-8 |series=Wiley series in probability and statistics applied probability and statistics section |location=New York Chichester Weinheim Brisbane Singapore Toronto}}\n** {{Cite journal |last1=Yuksel |first1=S. E. |last2=Wilson |first2=J. N. |last3=Gader |first3=P. D. |date=August 2012 |title=Twenty Years of Mixture of Experts |url=https://ieeexplore.ieee.org/document/6215056 |journal=IEEE Transactions on Neural Networks and Learning Systems |volume=23 |issue=8 |pages=1177\u20131193 |doi=10.1109/TNNLS.2012.2200299 |pmid=24807516 |s2cid=9922492 |issn=2162-237X}}\n** {{cite journal|last1=Masoudnia|first1=Saeed|last2=Ebrahimpour|first2=Reza|title=Mixture of experts: a literature survey|journal=Artificial Intelligence Review|date=12 May 2012|volume=42|issue=2|pages=275\u2013293|doi=10.1007/s10462-012-9338-y|s2cid=3185688}}\n** {{Cite journal |last1=Nguyen |first1=Hien D. |last2=Chamroukhi |first2=Faicel |date=July 2018 |title=Practical and theoretical aspects of mixture\u2010of\u2010experts modeling: An overview |url=https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1246 |journal=WIREs Data Mining and Knowledge Discovery |language=en |volume=8 |issue=4 |doi=10.1002/widm.1246 |s2cid=49301452 |issn=1942-4787}}\n* Deep learning era\n** {{Cite arXiv |last1=Zoph |first1=Barret |last2=Bello |first2=Irwan |last3=Kumar |first3=Sameer |last4=Du |first4=Nan |last5=Huang |first5=Yanping |last6=Dean |first6=Jeff |last7=Shazeer |first7=Noam |last8=Fedus |first8=William |date=2022 |title=ST-MoE: Designing Stable and Transferable Sparse Expert Models |class=cs.CL |eprint=2202.08906}}\n\n== See also ==\n\n* [[Product of experts]]\n* [[Mixture model|Mixture models]]\n* [[Mixture of gaussians]]\n* [[Ensemble learning]]\n\n==References==\n{{Reflist}}\n\n[[Category:Machine learning algorithms]]"}