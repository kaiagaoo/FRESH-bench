{"title": "Mixture of experts", "page_id": 54238535, "revision_id": 1148167077, "revision_timestamp": "2023-04-04T13:47:55Z", "content": "{{Short description|Machine learning technique}}\n'''Mixture of experts''' (MoE) is a [[machine learning]] technique where multiple expert networks (learners) are used to divide a problem space into homogeneous regions.<ref>{{cite journal|doi=10.1016/j.ymssp.2015.05.009|title=Variational Bayesian mixture of experts models and sensitivity analysis for nonlinear dynamical systems|journal=Mechanical Systems and Signal Processing |volume=66\u201367|pages=178\u2013200|year=2016|last1=Baldacchino|first1=Tara|last2=Cross|first2=Elizabeth J.|last3=Worden|first3=Keith|last4=Rowson|first4=Jennifer|bibcode=2016MSSP...66..178B}}</ref> It differs from [[ensemble learning|ensemble techniques]] in that typically only one or a few expert models will be run, rather than combining results from all models.\n\nAn example from [[computer vision]] is combining one [[neural network]] model for human detection with another for [[pose estimation]].\n\n== Hierarchical mixture ==\nIf the output is conditioned on multiple levels of (probabilistic) gating functions, the mixture is called a hierarchical mixture of experts.<ref>{{cite web|last1=Hauskrecht|first1=Milos|title=Ensamble methods: Mixtures of experts (Presentation)|url=https://people.cs.pitt.edu/~milos/courses/cs2750-Spring04/lectures/class22.pdf}}</ref>\n\nA gating network decides which expert to use for each input region. Learning thus consists of learning the parameters of:\n\n* individual learners and \n* gating network.\n\n== Applications ==\n[[Meta Platforms|Meta]] uses MoE in its NLLB-200 system. It uses multiple MoE models that share capacity for use by low-resource [[language models]] with relatively little data.<ref>{{Cite web |date=2022-06-19 |title=200 languages within a single AI model: A breakthrough in high-quality machine translation |url=https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/ |archive-url=https://web.archive.org/web/20230109051700/https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/ |archive-date=2023-01-09 |website=ai.facebook.com |language=en}}</ref>\n\n==References==\n{{Reflist}}\n\n==Extra reading==\n*{{cite journal|last1=Masoudnia|first1=Saeed|last2=Ebrahimpour|first2=Reza|title=Mixture of experts: a literature survey|journal=Artificial Intelligence Review|date=12 May 2012|volume=42|issue=2|pages=275\u2013293|doi=10.1007/s10462-012-9338-y|s2cid=3185688}}\n\n[[Category:Machine learning algorithms]]"}