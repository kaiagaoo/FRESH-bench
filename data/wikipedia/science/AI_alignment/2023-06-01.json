{"title": "AI alignment", "page_id": 50785023, "revision_id": 1157700810, "revision_timestamp": "2023-05-30T11:50:30Z", "content": "{{Short description|Conformance to the intended objective}}\n{{Merge from|Misaligned goals in artificial intelligence|discuss=Talk:AI alignment#Proposed merge of Misaligned goals in artificial intelligence into AI alignment|date=April 2023}}\n{{use mdy dates|date=September 2021}}\n{{Use American English|date=February 2021}}\n{{Artificial intelligence}}\nIn the field of [[artificial intelligence]] (AI), '''AI alignment''' research aims to steer AI systems towards humans\u2019 intended goals, preferences, or ethical principles. An AI system is considered ''aligned'' if it advances the intended objectives. A ''misaligned'' AI system is competent at advancing some objectives, but not the intended ones.<ref name=aima4>{{Cite book |last1=Russell |first1=Stuart J. |url=https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |title=Artificial intelligence: A modern approach |last2=Norvig |first2=Peter |publisher=Pearson |year=2020 |isbn=978-1-292-40113-3 |edition=4th |oclc=1303900751 |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20220715195054/https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html |archive-date=July 15, 2022 |url-status=live}}</ref>{{rp|31\u201334}}{{efn|The distinction between misaligned AI and incompetent AI has been formalized in certain contexts.<ref name=Unsolved2022>{{Cite arXiv |last1=Hendrycks |first1=Dan |last2=Carlini |first2=Nicholas |last3=Schulman |first3=John |last4=Steinhardt |first4=Jacob |date=2022-06-16 |title=Unsolved Problems in ML Safety |class=cs.LG |eprint=2109.13916 }}</ref>}}\n\nIt can be challenging for AI designers to align an AI system because it can be difficult for them to specify the full range of desired and undesired behaviors. To avoid this difficulty, they typically use simpler [[Misaligned goals in artificial intelligence#Undesired side-effects|''proxy goals'']], such as [[Reinforcement learning from human feedback|gaining human approval]]. However, this approach can create loopholes, overlook necessary constraints, or reward the AI system for just appearing aligned.{{r|aima4|pages=31\u201334}}{{r|dlp2023}}\n\nMisaligned AI systems can malfunction or cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful ways ([[Misaligned goals in artificial intelligence#Specification gaming|reward hacking]]).{{r|aima4|pages=31\u201334}}<ref name=\"mmmm2022\">{{Cite conference |last1=Pan |first1=Alexander |last2=Bhatia |first2=Kush |last3=Steinhardt |first3=Jacob |date=2022-02-14 |title=The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models |url=https://openreview.net/forum?id=JYtwGwIL7ye |conference=International Conference on Learning Representations |accessdate=2022-07-21}}</ref><ref>{{Cite conference |last1=Zhuang |first1=Simon |last2=Hadfield-Menell |first2=Dylan |date=2020 |title=Consequences of Misaligned AI |url=https://proceedings.neurips.cc/paper/2020/hash/b607ba543ad05417b8507ee86c54fcb7-Abstract.html |publisher=Curran Associates, Inc. |volume=33 |pages=15763\u201315773 |book-title=Advances in Neural Information Processing Systems |accessdate=2023-03-11}}</ref> AI systems may also develop unwanted [[Instrumental convergence|instrumental strategies]] such as seeking power or survival because such strategies help them achieve their explicit goals.{{r|aima4|pages=31\u201334}}<ref name=Carlsmith2022>{{cite arXiv |eprint=2206.13353 |class=cs.CY |first=Joseph |last=Carlsmith |title=Is Power-Seeking AI an Existential Risk? |date=2022-06-16}}</ref><ref name=\":2102\">{{Cite book |last=Russell |first=Stuart J. |url=https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/ |title=Human compatible: Artificial intelligence and the problem of control |publisher=Penguin Random House |year=2020 |isbn=9780525558637 |location= |oclc=1113410915}}</ref> Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is in deployment, where it faces new situations and [[Domain adaptation|data distributions]].<ref name=Christian2020>{{Cite book |last=Christian |first=Brian |url=https://wwnorton.co.uk/books/9780393635829-the-alignment-problem |title=The alignment problem: Machine learning and human values |publisher=W. W. Norton & Company |year=2020 |isbn=978-0-393-86833-3 |location= |chapter= |oclc=1233266753 |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20230210114137/https://wwnorton.co.uk/books/9780393635829-the-alignment-problem |archive-date=February 10, 2023 |url-status=live}}</ref><ref name=gmdrl>{{Cite conference |last1=Langosco |first1=Lauro Langosco Di |last2=Koch |first2=Jack |last3=Sharkey |first3=Lee D. |last4=Pfau |first4=Jacob |last5=Krueger |first5=David |date=2022-06-28 |title=Goal Misgeneralization in Deep Reinforcement Learning |url=https://proceedings.mlr.press/v162/langosco22a.html |conference=International Conference on Machine Learning |publisher=PMLR |pages=12004\u201312019 |book-title=Proceedings of the 39th International Conference on Machine Learning |accessdate=2023-03-11}}</ref>\n\nToday, these problems affect existing commercial systems such as language models,<ref name=\"Opportunities_Risks\">{{Cite journal |last1=Bommasani |first1=Rishi |last2=Hudson |first2=Drew A. |last3=Adeli |first3=Ehsan |last4=Altman |first4=Russ |last5=Arora |first5=Simran |last6=von Arx |first6=Sydney |last7=Bernstein |first7=Michael S. |last8=Bohg |first8=Jeannette |last9=Bosselut |first9=Antoine |last10=Brunskill |first10=Emma |last11=Brynjolfsson |first11=Erik |date=2022-07-12 |title=On the Opportunities and Risks of Foundation Models |url=https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models |journal=Stanford CRFM |arxiv=2108.07258}}</ref><ref name=feedback2022>{{cite arXiv |eprint=2203.02155 |class=cs.CL |first1=Long |last1=Ouyang |first2=Jeff |last2=Wu |title=Training language models to follow instructions with human feedback |date=2022 |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=J. |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke E. |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=P. |last18=Christiano |first18=P. |last19=Leike |first19=J. |last20=Lowe |first20=Ryan J.}}</ref><ref name=OpenAICodex>{{Cite web |last1=Zaremba |first1=Wojciech |last2=Brockman |first2=Greg |last3=OpenAI |date=2021-08-10 |title=OpenAI Codex |url=https://openai.com/blog/openai-codex/ |url-status=live |archive-url=https://web.archive.org/web/20230203201912/https://openai.com/blog/openai-codex/ |archive-date=February 3, 2023 |accessdate=2022-07-23 |work=OpenAI}}</ref> robots,<ref>{{Cite journal |last1=Kober |first1=Jens |last2=Bagnell |first2=J. Andrew |last3=Peters |first3=Jan |date=2013-09-01 |title=Reinforcement learning in robotics: A survey |url=http://journals.sagepub.com/doi/10.1177/0278364913495721 |url-status=live |journal=The International Journal of Robotics Research |language=en |volume=32 |issue=11 |pages=1238\u20131274 |doi=10.1177/0278364913495721 |issn=0278-3649 |s2cid=1932843 |archive-url=https://web.archive.org/web/20221015200445/https://journals.sagepub.com/doi/10.1177/0278364913495721 |archive-date=October 15, 2022 |access-date=September 12, 2022}}</ref> autonomous vehicles,<ref>{{Cite journal |last1=Knox |first1=W. Bradley |last2=Allievi |first2=Alessandro |last3=Banzhaf |first3=Holger |last4=Schmitt |first4=Felix |last5=Stone |first5=Peter |date=2023-03-01 |title=Reward (Mis)design for autonomous driving |url=https://www.sciencedirect.com/science/article/pii/S0004370222001692 |journal=Artificial Intelligence |language=en |volume=316 |pages=103829 |doi=10.1016/j.artint.2022.103829 |s2cid=233423198 |issn=0004-3702}}</ref> and social media recommendation engines.{{r|Opportunities_Risks|:2102}}<ref>{{Cite journal |last=Stray |first=Jonathan |date=2020 |title=Aligning AI Optimization to Community Well-Being |journal=International Journal of Community Well-Being |language=en |volume=3 |issue=4 |pages=443\u2013463 |doi=10.1007/s42413-020-00086-3 |issn=2524-5295 |pmc=7610010 |pmid=34723107 |s2cid=226254676}}</ref> Some AI researchers argue that more capable future systems will be more severely affected since these problems partially result from the systems being highly capable.<ref name=\"AIMA\">{{Cite book |last1=Russell |first1=Stuart |url=https://aima.cs.berkeley.edu/ |title=Artificial Intelligence: A Modern Approach |last2=Norvig |first2=Peter |date=2009 |publisher=Prentice Hall |isbn=978-0-13-604259-4 |pages=1010}}</ref>{{r|mmmm2022}}<ref name=\"dlp2023\">{{Cite arXiv |last1=Ngo |first1=Richard |last2=Chan |first2=Lawrence |last3=Mindermann |first3=S\u00f6ren |date=2023-02-22 |title=The alignment problem from a deep learning perspective |class=cs.AI |eprint=2209.00626 }}</ref> \n\nLeading computer scientists such as [[Geoffrey Hinton]] and [[Stuart J. Russell|Stuart Russell]] argue that AI is approaching superhuman capabilities and could endanger human civilization if misaligned.<ref>{{Cite web |last=Smith |first=Craig S. |title=Geoff Hinton, AI's Most Famous Researcher, Warns Of 'Existential Threat' |url=https://www.forbes.com/sites/craigsmith/2023/05/04/geoff-hinton-ais-most-famous-researcher-warns-of-existential-threat/ |access-date=2023-05-04 |website=Forbes |language=en}}</ref><ref name=\":2102\" />{{efn|For example, in a 2016 TV interview, Turing-award winner Geoffrey Hinton noted<ref>{{Cite AV media| people = Geoffrey Hinton | title = The Code That Runs Our Lives| work=The Agenda | accessdate = 2023-03-13| date = 2016-03-03| time = 10:00| url = https://www.youtube.com/watch?v=XG-dwZMc7Ng&t=600s}}</ref>:<br />\n; Hinton: Obviously having other superintelligent beings who are more intelligent than us is something to be nervous about [...].<br />\n; Interviewer: What aspect of it makes you nervous?<br />\n; Hinton: Well, will they be nice to us?<br />\n; Interviewer: It's just like the movies. You're worried about that scenario from the movies...<br />\n; Hinton: In the very long-run, yes. I think in the next 5-10 years [2021 to 2026] we don't have to worry about it. Also, the movies always protrait it as an individual intelligence. I think it may be that it goes in a different direction where we sort of developed jointly with these things. So the things aren't fully automomous; they're developed to help us; they're like personal assistants. And we'll develop with them. And it'll be more of a symbiosis than a rivalry. But we don't know.<br />\n; Interviewer: Is that an expectation or a hope?<br />\n; Hinton: That's a hope.|name=hinton}}\n\nThe AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values.<ref>{{Cite web| last = Future of Life Institute| title = Asilomar AI Principles| work = Future of Life Institute| accessdate = 2022-07-18| date = 2017-08-11| url = https://futureoflife.org/2017/08/11/ai-principles/| archive-date = October 10, 2022| archive-url = https://web.archive.org/web/20221010183130/https://futureoflife.org/2017/08/11/ai-principles/| url-status = live}} The AI principles created at the [[Asilomar Conference on Beneficial AI]] were signed by 1797 AI/robotics researchers.\n* {{Cite report| publisher = United Nations| last = United Nations| title = Our Common Agenda: Report of the Secretary-General| location = New York| date = 2021 |url=https://www.un.org/en/content/common-agenda-report/assets/pdf/Common_Agenda_Report_English.pdf| access-date = September 12, 2022| archive-date = May 22, 2022| archive-url = https://web.archive.org/web/20220522204809/https://www.un.org/en/content/common-agenda-report/assets/pdf/Common_Agenda_Report_English.pdf| url-status = live|quote=[T]he [UN] could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values.}}</ref>\n\nAI alignment is a subfield of [[AI safety]], the study of how to build safe AI systems.<ref name=concrete2016>{{Cite arXiv |eprint=1606.06565 |class=cs.AI |first1=Dario |last1=Amodei |first2=Chris |last2=Olah |title=Concrete Problems in AI Safety |date=2016-06-21 |language=en |last3=Steinhardt |first3=Jacob |last4=Christiano |first4=Paul |last5=Schulman |first5=John |last6=Man\u00e9 |first6=Dan}}</ref> Other subfields of AI safety include robustness, monitoring, and [[AI capability control|capability control]].<ref name=\"building2018\">{{Cite web |last1=Ortega |first1=Pedro A. |last2=Maini |first2=Vishal |last3=DeepMind safety team |date=2018-09-27 |title=Building safe artificial intelligence: specification, robustness, and assurance |url=https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1 |url-status=live |archive-url=https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1 |archive-date=February 10, 2023 |accessdate=2022-07-18 |work=DeepMind Safety Research - Medium}}</ref> Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.{{r|building2018}} Alignment research has connections to [[Explainable artificial intelligence|interpretability research]],<ref name=\":333\">{{Cite web |last=Rorvig |first=Mordechai |date=2022-04-14 |title=Researchers Gain New Understanding From Simple AI |url=https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/ |url-status=live |archive-url=https://web.archive.org/web/20230210114056/https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/ |archive-date=February 10, 2023 |accessdate=2022-07-18 |work=Quanta Magazine}}</ref><ref>{{Cite arXiv |eprint=1702.08608 |class=stat.ML |first1=Finale |last1=Doshi-Velez |first2=Been |last2=Kim |title=Towards A Rigorous Science of Interpretable Machine Learning |date=2017-03-02}}\n* {{Cite podcast |number=107 |last=Wiblin |first=Robert |title=Chris Olah on what the hell is going on inside neural networks |series=80,000 hours |accessdate=2022-07-23 |publisher= |date=August 4, 2021 |url=https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/}}</ref> (adversarial) robustness,{{r|concrete2016}} [[anomaly detection]], [[Uncertainty quantification|calibrated uncertainty]],<ref name=\":333\" /> [[formal verification]],<ref>{{Cite journal |last1=Russell |first1=Stuart |last2=Dewey |first2=Daniel |last3=Tegmark |first3=Max |date=2015-12-31 |title=Research Priorities for Robust and Beneficial Artificial Intelligence |url=https://ojs.aaai.org/index.php/aimagazine/article/view/2577 |url-status=live |journal=AI Magazine |volume=36 |issue=4 |pages=105\u2013114 |doi=10.1609/aimag.v36i4.2577 |issn=2371-9621 |s2cid=8174496 |archive-url=https://web.archive.org/web/20230202181059/https://ojs.aaai.org/index.php/aimagazine/article/view/2577 |archive-date=February 2, 2023 |access-date=September 12, 2022 |hdl=1721.1/108478}}</ref> [[preference learning]],<ref name=prefsurvey2017>{{Cite journal |last1=Wirth |first1=Christian |last2=Akrour |first2=Riad |last3=Neumann |first3=Gerhard |last4=F\u00fcrnkranz |first4=Johannes |date=2017 |title=A survey of preference-based reinforcement learning methods |journal=Journal of Machine Learning Research |volume=18 |issue=136 |pages=1\u201346}}</ref><ref name=drlfhp>{{Cite conference |last1=Christiano |first1=Paul F. |last2=Leike |first2=Jan |last3=Brown |first3=Tom B. |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |date=2017 |title=Deep reinforcement learning from human preferences |series=NIPS'17 |location=Red Hook, NY, USA |publisher=Curran Associates Inc. |pages=4302\u20134310 |isbn=978-1-5108-6096-4 |book-title=Proceedings of the 31st International Conference on Neural Information Processing Systems}}</ref><ref name=LessToxic>{{Cite web |last=Heaven |first=Will Douglas |date=2022-01-27 |title=The new version of GPT-3 is much better behaved (and should be less toxic) |url=https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/ |url-status=live |archive-url=https://web.archive.org/web/20230210114056/https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/ |archive-date=February 10, 2023 |accessdate=2022-07-18 |work=MIT Technology Review}}</ref> [[Safety-critical system|safety-critical engineering]],<ref>{{cite arXiv |eprint=2106.04823 |class=cs.LG |first1=Sina |last1=Mohseni |first2=Haotao |last2=Wang |title=Taxonomy of Machine Learning Safety: A Survey and Primer |date=2022-03-07 |last3=Yu |first3=Zhiding |last4=Xiao |first4=Chaowei |last5=Wang |first5=Zhangyang |last6=Yadawa |first6=Jay}}</ref> [[game theory]],<ref>{{Cite web |last=Clifton |first=Jesse |date=2020 |title=Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda |url=https://longtermrisk.org/research-agenda/ |url-status=live |archive-url=https://web.archive.org/web/20230101041759/https://longtermrisk.org/research-agenda |archive-date=January 1, 2023 |accessdate=2022-07-18 |work=Center on Long-Term Risk}}\n* {{Cite journal |last1=Dafoe |first1=Allan |last2=Bachrach |first2=Yoram |last3=Hadfield |first3=Gillian |last4=Horvitz |first4=Eric |last5=Larson |first5=Kate |last6=Graepel |first6=Thore |date=2021-05-06 |title=Cooperative AI: machines must learn to find common ground |url=http://www.nature.com/articles/d41586-021-01170-0 |url-status=live |journal=Nature |language=en |volume=593 |issue=7857 |pages=33\u201336 |bibcode=2021Natur.593...33D |doi=10.1038/d41586-021-01170-0 |issn=0028-0836 |pmid=33947992 |s2cid=233740521 |archive-url=https://web.archive.org/web/20221218210857/https://www.nature.com/articles/d41586-021-01170-0 |archive-date=December 18, 2022 |access-date=September 12, 2022}}</ref> [[Fairness (machine learning)|algorithmic fairness]],{{r|concrete2016}}<ref>{{Cite journal |last1=Prunkl |first1=Carina |last2=Whittlestone |first2=Jess |date=2020-02-07 |title=Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society |url=https://dl.acm.org/doi/10.1145/3375627.3375803 |url-status=live |journal=Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society |language=en |location=New York NY USA |publisher=ACM |pages=138\u2013143 |doi=10.1145/3375627.3375803 |isbn=978-1-4503-7110-0 |s2cid=210164673 |archive-url=https://web.archive.org/web/20221016123733/https://dl.acm.org/doi/10.1145/3375627.3375803 |archive-date=October 16, 2022 |access-date=September 12, 2022}}</ref> and the [[Social science|social sciences]],<ref>{{Cite journal |last1=Irving |first1=Geoffrey |last2=Askell |first2=Amanda |date=2019-02-19 |title=AI Safety Needs Social Scientists |url=https://distill.pub/2019/safety-needs-social-scientists |url-status=live |journal=Distill |volume=4 |issue=2 |pages=10.23915/distill.00014 |doi=10.23915/distill.00014 |issn=2476-0757 |s2cid=159180422 |archive-url=https://web.archive.org/web/20230210114220/https://distill.pub/2019/safety-needs-social-scientists/ |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref> among others.\n\n==The Alignment Problem==\n[[File:Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg|right|thumb|An AI system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets. This is an example of specification gaming.<ref>{{Cite web |date=2016-12-22 |title=Faulty Reward Functions in the Wild |url=https://openai.com/blog/faulty-reward-functions/ |access-date=2022-09-10 |website=OpenAI |language=en |archive-date=January 26, 2021 |archive-url=https://web.archive.org/web/20210126173249/https://openai.com/blog/faulty-reward-functions/ |url-status=live }}</ref>]]\nIn 1960, AI pioneer [[Norbert Wiener]] described the AI alignment problem as follows: \u201cIf we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively\u2026 we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\u201d<ref name=Wiener1960>{{Cite journal |last=Wiener |first=Norbert |date=1960-05-06 |title=Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers. |url=https://www.science.org/doi/10.1126/science.131.3410.1355 |url-status=live |journal=Science |language=en |volume=131 |issue=3410 |pages=1355\u20131358 |doi=10.1126/science.131.3410.1355 |issn=0036-8075 |pmid=17841602 |archive-url=https://web.archive.org/web/20221015105034/https://www.science.org/doi/10.1126/science.131.3410.1355 |archive-date=October 15, 2022 |access-date=September 12, 2022}}</ref><ref name=\":2102\" /> Different definitions of AI alignment require that an aligned AI system advances different goals: the goals of its designers, its users or, alternatively, objective ethical standards, widely shared values, or the intentions its designers would have if they were more informed and enlightened.<ref name=Gabriel2020>{{Cite journal |last=Gabriel |first=Iason |date=2020-09-01 |title=Artificial Intelligence, Values, and Alignment |url=https://doi.org/10.1007/s11023-020-09539-2 |url-status=live |journal=Minds and Machines |volume=30 |issue=3 |pages=411\u2013437 |doi=10.1007/s11023-020-09539-2 |issn=1572-8641 |s2cid=210920551 |archive-url=https://web.archive.org/web/20230315193114/https://link.springer.com/article/10.1007/s11023-020-09539-2 |archive-date=March 15, 2023 |accessdate=2022-07-23}}</ref>\n\nAI alignment is an open problem for modern AI systems<ref>{{Cite news |last=The Ezra Klein Show |date=2021-06-04 |title=If 'All Models Are Wrong,' Why Do We Give Them So Much Power? |work=The New York Times |url=https://www.nytimes.com/2021/06/04/opinion/ezra-klein-podcast-brian-christian.html |url-status=live |accessdate=2023-03-13 |archive-url=https://web.archive.org/web/20230215224050/https://www.nytimes.com/2021/06/04/opinion/ezra-klein-podcast-brian-christian.html |archive-date=February 15, 2023 |issn=0362-4331}}\n*{{Cite web |last=Wolchover |first=Natalie |date=2015-04-21 |title=Concerns of an Artificial Intelligence Pioneer |url=https://www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421/ |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421/ |archive-date=February 10, 2023 |accessdate=2023-03-13 |work=Quanta Magazine}}\n*{{Cite web |last=California Assembly |title=Bill Text - ACR-215 23 Asilomar AI Principles. |url=https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180ACR215 |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180ACR215 |archive-date=February 10, 2023 |accessdate=2022-07-18}}</ref><ref name=MasteringLanguage>{{Cite news |last1=Johnson |first1=Steven |last2=Iziev |first2=Nikita |date=2022-04-15 |title=A.I. Is Mastering Language. Should We Trust What It Says? |work=The New York Times |url=https://www.nytimes.com/2022/04/15/magazine/ai-language.html |url-status=live |accessdate=2022-07-18 |archive-url=https://web.archive.org/web/20221124151408/https://www.nytimes.com/2022/04/15/magazine/ai-language.html |archive-date=November 24, 2022 |issn=0362-4331}}</ref> and a research field within AI.<ref>{{Cite web |last=OpenAI |title=Developing safe & responsible AI |url=https://openai.com/blog/our-approach-to-alignment-research |accessdate=2023-03-13}}\n*{{Cite web |title=DeepMind Safety Research |url=https://deepmindsafetyresearch.medium.com |url-status=live |archive-url=https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/ |archive-date=February 10, 2023 |accessdate=2023-03-13 |work=Medium}}</ref>{{r|aima4|pages=31\u201334}} Aligning AI involves two main challenges: carefully [[Specification (technical standard)|specifying]] the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment).{{r|dlp2023}}\n\n=== Specification gaming and side effects ===\nTo specify an AI system\u2019s purpose, AI designers typically provide an [[Reward function|objective function]], [[Supervised learning|examples]], or [[Reinforcement learning|feedback]] to the system. However, AI designers are often unable to completely specify all important values and constraints, and so they resort to easy-to-specify [[Misaligned goals in artificial intelligence#Undesired side-effects|''proxy goals'']] such as [[Reinforcement learning from human feedback|maximizing the approval]] of human overseers, who are fallible.{{r|concrete2016|Unsolved2022|building2018}}{{r|aima4|pages=4\u20135}}<ref name=SpecGaming2020>{{Cite web |last1=Krakovna |first1=Victoria |last2=Uesato |first2=Jonathan |last3=Mikulik |first3=Vladimir |last4=Rahtz |first4=Matthew |last5=Everitt |first5=Tom |last6=Kumar |first6=Ramana |last7=Kenton |first7=Zac |last8=Leike |first8=Jan |last9=Legg |first9=Shane |date=2020-04-21 |title=Specification gaming: the flip side of AI ingenuity |url=https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity |url-status=live |archive-url=https://web.archive.org/web/20230210114143/https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity |archive-date=February 10, 2023 |accessdate=2022-08-26 |work=Deepmind}}</ref> As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as ''specification gaming'' or ''reward hacking'', and is an instance of [[Goodhart's law|Goodhart\u2019s law]].{{r|SpecGaming2020|mmmm2022}}<ref name=\":111\">{{Cite arXiv |eprint=1803.04585 |class=cs.AI |first1=David |last1=Manheim |first2=Scott |last2=Garrabrant |title=Categorizing Variants of Goodhart's Law |year=2018}}</ref> As AI systems become more capable, they are often able to game their specifications more effectively.{{r|mmmm2022}}\n[[File:Robot_hand_trained_with_human_feedback_'pretends'_to_grasp_ball.ogg|right|thumb|This AI system was trained using human feedback to grab a ball, but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera.<ref name=lfhp2017>{{Cite web |last1=Amodei |first1=Dario |last2=Christiano |first2=Paul |last3=Ray |first3=Alex |date=2017-06-13 |title=Learning from Human Preferences |url=https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ |url-status=live |archive-url=https://web.archive.org/web/20210103215933/https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/ |archive-date=January 3, 2021 |accessdate=2022-07-21 |work=OpenAI}}</ref> Research on AI alignment partly aims to avert solutions that are false but convincing.]]\n\nSpecification gaming has been observed in numerous AI systems.{{r|SpecGaming2020}}<ref>{{cite web|url=https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml|title=Specification gaming examples in AI \u2014 master list}}</ref> One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track; but the system achieved more reward by looping and crashing into the same targets indefinitely (see video).{{r|Gabriel2020}} Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video).{{r|lfhp2017}} Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora which are broad but fallible.<ref name=TruthfulQA>{{Cite journal |last1=Lin |first1=Stephanie |last2=Hilton |first2=Jacob |last3=Evans |first3=Owain |date=2022 |title=TruthfulQA: Measuring How Models Mimic Human Falsehoods |url=https://aclanthology.org/2022.acl-long.229 |url-status=live |journal=Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) |language=en |location=Dublin, Ireland |publisher=Association for Computational Linguistics |pages=3214\u20133252 |doi=10.18653/v1/2022.acl-long.229 |s2cid=237532606 |archive-url=https://web.archive.org/web/20230210114231/https://aclanthology.org/2022.acl-long.229/ |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref><ref name=Naughton2021>{{Cite news |last=Naughton |first=John |date=2021-10-02 |title=The truth about artificial intelligence? It isn't that honest |work=The Observer |url=https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest |url-status=live |accessdate=2022-07-23 |archive-url=https://web.archive.org/web/20230213231317/https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest |archive-date=February 13, 2023 |issn=0029-7712}}</ref> When they are retrained to produce text humans rate as true or helpful, chatbots like [[ChatGPT]] can fabricate fake explanations that humans find convincing.<ref>{{Cite journal |last1=Ji |first1=Ziwei |last2=Lee |first2=Nayeon |last3=Frieske |first3=Rita |last4=Yu |first4=Tiezheng |last5=Su |first5=Dan |last6=Xu |first6=Yan |last7=Ishii |first7=Etsuko |last8=Bang |first8=Yejin |last9=Madotto |first9=Andrea |last10=Fung |first10=Pascale |date=2022-02-01 |title=Survey of Hallucination in Natural Language Generation |url=https://ui.adsabs.harvard.edu/abs/2022arXiv220203629J |url-status=live |journal=ACM Computing Surveys |volume=55 |issue=12 |pages=1\u201338 |arxiv=2202.03629 |doi=10.1145/3571730 |s2cid=246652372 |archive-url=https://web.archive.org/web/20230210114138/https://ui.adsabs.harvard.edu/abs/2022arXiv220203629J |archive-date=February 10, 2023 |access-date=October 14, 2022}}\n* {{Cite journal |last=Else |first=Holly |date=2023-01-12 |title=Abstracts written by ChatGPT fool scientists |url=https://www.nature.com/articles/d41586-023-00056-7 |journal=Nature |language=en |volume=613 |issue=7944 |pages=423 |doi=10.1038/d41586-023-00056-7|pmid=36635510 |bibcode=2023Natur.613..423E |s2cid=255773668 }}</ref> Some alignment researchers aim to help humans detect specification gaming, and to steer AI systems towards carefully specified objectives that are safe and useful to pursue.\n\nWhen a misaligned AI system is deployed, it can cause consequential side effects. Social media platforms have been known to optimize for clickthrough rates, causing user addiction on a global scale.{{r|Unsolved2022}} Stanford researchers comment that such [[Recommender system|recommender systems]] are misaligned with their users because they \u201coptimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\u201d.{{r|Opportunities_Risks}}\n\nExplaining such side-effects, Berkeley computer scientist [[Stuart J. Russell|Stuart Russell]] noted that harm can result if implicit constraints are omitted during training: \u201cA system... will often set... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\u201d<ref>{{Cite web |last=Russell|first=Stuart|website=Edge.org |title=Of Myths and Moonshine|url=https://www.edge.org/conversation/the-myth-of-ai |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.edge.org/conversation/the-myth-of-ai |archive-date=February 10, 2023 |accessdate=2022-07-19}}</ref>\n\nSome researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov\u2019s [[Three Laws of Robotics]]).<ref>{{Cite journal |last=Tasioulas |first=John |date=2019 |title=First Steps Towards an Ethics of Robots and Artificial Intelligence |journal=Journal of Practical Ethics |volume=7 |issue=1 |pages=61\u201395}}</ref> However, [[Stuart J. Russell|Russell]] and [[Peter Norvig|Norvig]] argued that this approach overlooks the complexity of human values:<ref name=\":2102\" /> \u201cIt is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\u201d<ref name=\":2102\" />\n\nAdditionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).{{r|aima4|pages=31\u201334}}\n\n=== Pressure to deploy unsafe systems ===\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems.{{r|Unsolved2022}} For example, the aforementioned social media [[Recommender system|recommender systems]] have been profitable despite creating unwanted addiction and polarization.{{r|Opportunities_Risks}}<ref name=\":722\">{{Cite news |last1=Wells |first1=Georgia |last2=Deepa Seetharaman |last3=Horwitz |first3=Jeff |date=2021-11-05 |title=Is Facebook Bad for You? It Is for About 360 Million Users, Company Surveys Suggest |work=Wall Street Journal |url=https://www.wsj.com/articles/facebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681 |url-status=live |accessdate=2022-07-19 |archive-url=https://web.archive.org/web/20230210114137/https://www.wsj.com/articles/facebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681 |archive-date=February 10, 2023 |issn=0099-9660}}</ref><ref name=\":822\">{{Cite report |url=https://bhr.stern.nyu.edu/polarization-report-page |title=How Social Media Intensifies U.S. Political Polarization-And What Can Be Done About It |last1=Barrett |first1=Paul M. |last2=Hendrix |first2=Justin |date=September 2021 |publisher=Center for Business and Human Rights, NYU |last3=Sims |first3=J. Grant |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20230201180005/https://bhr.stern.nyu.edu/polarization-report-page |archive-date=February 1, 2023 |url-status=live}}</ref> In addition, competitive pressure can lead to a [[race to the bottom]] on AI safety standards. In 2018, a self-driving car killed a pedestrian ([[Death of Elaine Herzberg|Elaine Herzberg]]) after engineers disabled the emergency braking system because it was over-sensitive and slowed down development.<ref>{{Cite news |last=Shepardson |first=David |date=2018-05-24 |title=Uber disabled emergency braking in self-driving car: U.S. agency |work=Reuters |url=https://www.reuters.com/article/us-uber-crash-idUSKCN1IP26K |url-status=live |accessdate=2022-07-20 |archive-url=https://web.archive.org/web/20230210114137/https://www.reuters.com/article/us-uber-crash-idUSKCN1IP26K |archive-date=February 10, 2023}}</ref>\n\n=== Risks from advanced misaligned AI ===\nSome researchers are interested in aligning increasingly advanced AI systems, as current progress in AI is rapid, and industry and governments are trying to build advanced AI. As AI systems become more advanced, they could unlock many opportunities if they are aligned but they may also become harder to align and could pose large-scale hazards.<ref name=\":2102\" />\n\n==== Development of advanced AI ====\nLeading AI labs such as [[OpenAI]] and [[DeepMind]] have stated their aim to develop [[artificial general intelligence|artificial general intelligence]] (AGI), a hypothesized AI system that matches or outperforms humans in a broad range of cognitive tasks.<ref name=\":2622\">{{Cite web |last=Baum |first=Seth |date=2021-01-01 |title=2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy |url=https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/ |url-status=live |archive-url=https://web.archive.org/web/20230210114138/https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/ |archive-date=February 10, 2023 |accessdate=2022-07-20}}</ref> Researchers who scale modern [[Neural network|neural networks]] observe that they indeed develop increasingly general and unanticipated capabilities.{{r|Opportunities_Risks}}<ref name=eallm2022>{{Cite journal |last1=Wei |first1=Jason |last2=Tay |first2=Yi |last3=Bommasani |first3=Rishi |last4=Raffel |first4=Colin |last5=Zoph |first5=Barret |last6=Borgeaud |first6=Sebastian |last7=Yogatama |first7=Dani |last8=Bosma |first8=Maarten |last9=Zhou |first9=Denny |last10=Metzler |first10=Donald |last11=Chi |first11=Ed H. |last12=Hashimoto |first12=Tatsunori |last13=Vinyals |first13=Oriol |last14=Liang |first14=Percy |last15=Dean |first15=Jeff |last16=Fedus |first16=William| date=2022-10-26 |title=Emergent Abilities of Large Language Models |journal=Transactions on Machine Learning Research | issn=2835-8856 | eprint=2206.07682 }}</ref> Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs.<ref>{{Cite web |last=Dominguez |first=Daniel |date=2022-05-19 |title=DeepMind Introduces Gato, a New Generalist AI Agent |url=https://www.infoq.com/news/2022/05/deepmind-gato-ai-agent/ |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.infoq.com/news/2022/05/deepmind-gato-ai-agent/ |archive-date=February 10, 2023 |accessdate=2022-09-09 |work=InfoQ}}\n* {{Cite web |last=Edwards |first=Ben |date=2022-04-26 |title=Adept's AI assistant can browse, search, and use web apps like a human |url=https://arstechnica.com/information-technology/2022/09/new-ai-assistant-can-browse-search-and-use-web-apps-like-a-human/ |url-status=live |archive-url=https://web.archive.org/web/20230117194921/https://arstechnica.com/information-technology/2022/09/new-ai-assistant-can-browse-search-and-use-web-apps-like-a-human/ |archive-date=January 17, 2023 |accessdate=2022-09-09 |work=Ars Technica}}</ref> According to surveys, some leading [[machine learning]] researchers expect AGI to be created in {{as of|2021|alt=this decade}}, some believe it will take much longer, and many consider both to be possible.<ref name=\":2822\">{{Cite journal |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |date=2018-07-31 |title=Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts |url=http://jair.org/index.php/jair/article/view/11222 |url-status=live |journal=Journal of Artificial Intelligence Research |volume=62 |pages=729\u2013754 |doi=10.1613/jair.1.11222 |issn=1076-9757 |s2cid=8746462 |archive-url=https://web.archive.org/web/20230210114220/https://jair.org/index.php/jair/article/view/11222 |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref><ref name=\":2922\">{{Cite journal |last1=Zhang |first1=Baobao |last2=Anderljung |first2=Markus |last3=Kahn |first3=Lauren |last4=Dreksler |first4=Noemi |last5=Horowitz |first5=Michael C. |last6=Dafoe |first6=Allan |date=2021-08-02 |title=Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers |url=https://jair.org/index.php/jair/article/view/12895 |url-status=live |journal=Journal of Artificial Intelligence Research |volume=71 |doi=10.1613/jair.1.12895 |issn=1076-9757 |s2cid=233740003 |archive-url=https://web.archive.org/web/20230210114143/https://jair.org/index.php/jair/article/view/12895 |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref>\n\nIn 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated that \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"<ref name=\":1701\">{{Cite web |last=Future of Life Institute |date=2023-03-22 |title=Pause Giant AI Experiments: An Open Letter |url=https://futureoflife.org/open-letter/pause-giant-ai-experiments/ |accessdate=2023-04-20 |url-status=live }}</ref>\n\n==== Power-seeking ====\n{{As of|2023|05|alt=Current|df=US}} systems still lack capabilities such as long-term [[Automated planning and scheduling|planning]] and [[Situation awareness|situational awareness]].{{r|Opportunities_Risks}} However, future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted [[AI alignment#Power-seeking and instrumental goals|''power-seeking'']] strategies. Future advanced AI agents might for example seek to acquire money and computation power, to proliferate, or to evade being turned off (for example by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents that have more power are better able to accomplish their goals.{{r|Opportunities_Risks|Carlsmith2022}} This tendency, known as [[instrumental convergence]], has already emerged in various [[reinforcement learning]] agents including language models.<ref name=\":3\">{{Cite journal |last1=Pan |first1=Alexander |last2=Shern |first2=Chan Jun |last3=Zou |first3=Andy |last4=Li |first4=Nathaniel |last5=Basart |first5=Steven |last6=Woodside |first6=Thomas |last7=Ng |first7=Jonathan |last8=Zhang |first8=Emmons |last9=Scott |first9=Dan |last10=Hendrycks |date=2023-04-03 |title=Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark |journal=Proceedings of the 40th International Conference on Machine Learning |language=en |publisher=PMLR |pages=|arxiv=2304.03279 }}</ref><ref name=dllmmwe2022>{{Cite arXiv |last1=Perez |first1=Ethan |last2=Ringer |first2=Sam |last3=Luko\u0161i\u016bt\u0117 |first3=Kamil\u0117 |last4=Nguyen |first4=Karina |last5=Chen |first5=Edwin |last6=Heiner |first6=Scott |last7=Pettit |first7=Craig |last8=Olsson |first8=Catherine |last9=Kundu |first9=Sandipan |last10=Kadavath |first10=Saurav |last11=Jones |first11=Andy |last12=Chen |first12=Anna |last13=Mann |first13=Ben |last14=Israel |first14=Brian |last15=Seethor |first15=Bryan |date=2022-12-19 |title=Discovering Language Model Behaviors with Model-Written Evaluations |class=cs.CL |eprint=2212.09251 }}</ref><ref>{{Cite journal |last1=Orseau |first1=Laurent |last2=Armstrong |first2=Stuart |date=2016-06-25 |title=Safely interruptible agents |url=https://dl.acm.org/doi/abs/10.5555/3020948.3021006 |journal=Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence |series=UAI'16 |location=Arlington, Virginia, USA |publisher=AUAI Press |pages=557\u2013566 |isbn=978-0-9966431-1-5}}</ref><ref name=Gridworlds>{{Cite arXiv |last1=Leike |first1=Jan |last2=Martic |first2=Miljan |last3=Krakovna |first3=Victoria |last4=Ortega |first4=Pedro A. |last5=Everitt |first5=Tom |last6=Lefrancq |first6=Andrew |last7=Orseau |first7=Laurent |last8=Legg |first8=Shane |date=2017-11-28 |title=AI Safety Gridworlds |class=cs.LG |eprint=1711.09883 }}</ref><ref name=OffSwitch>{{Cite journal |last1=Hadfield-Menell |first1=Dylan |last2=Dragan |first2=Anca |last3=Abbeel |first3=Pieter |last4=Russell |first4=Stuart |date=2017-08-19 |title=The off-switch game |url=https://dl.acm.org/doi/10.5555/3171642.3171675 |journal=Proceedings of the 26th International Joint Conference on Artificial Intelligence |series=IJCAI'17 |location=Melbourne, Australia |publisher=AAAI Press |pages=220\u2013227 |isbn=978-0-9992411-0-3}}</ref> Other research has mathematically shown that optimal [[reinforcement learning]] algorithms would seek power in a wide range of environments.<ref name=optsp>{{Cite conference |last1=Turner |first1=Alexander Matt |last2=Smith |first2=Logan Riggs |last3=Shah |first3=Rohin |last4=Critch |first4=Andrew |last5=Tadepalli |first5=Prasad |date=2021 |title=Optimal policies tend to seek power |url=https://openreview.net/forum?id=l7-DBWawSZH |book-title=Advances in neural information processing systems}}</ref><ref>{{Cite conference |last1=Turner |first1=Alexander Matt |last2=Tadepalli |first2=Prasad |date=2022 |title=Parametrically retargetable decision-makers tend to seek power |url=https://openreview.net/forum?id=GFgjnk2Q-ju |book-title=Advances in neural information processing systems}}</ref> As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.{{r|Carlsmith2022}}<ref name=Superintelligence>{{Cite book |last=Bostrom |first=Nick |title=Superintelligence: Paths, Dangers, Strategies |date=2014 |publisher=Oxford University Press, Inc. |isbn=978-0-19-967811-2 |edition=1st |location=USA}}</ref><ref name=\":2102\" />\n\nFuture power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them.{{r|Carlsmith2022}} Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.{{r|Carlsmith2022}}\n\n====Existential risk====\n{{see also|Existential risk from artificial general intelligence|AI takeover}}\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.{{r|aima4|pages=31\u201334}}<ref name=\":2102\" /> Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include [[Geoffrey Hinton]], [[Alan Turing]],{{efn|In a 1951 lecture<ref>{{Cite speech| last = Turing| first = Alan| title = Intelligent machinery, a heretical theory|event= Lecture given to '51 Society'| location = Manchester|accessdate = 2022-07-22| date = 1951|publisher = The Turing Digital Archive|url = https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4 | pages = 16| archive-date = September 26, 2022| archive-url = https://web.archive.org/web/20220926004549/https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4| url-status = live}}</ref> Turing argued that \u201cIt seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s Erewhon.\u201d Also in a lecture broadcast on BBC<ref>{{Cite episode |title= Can digital computers think?|series=Automatic Calculating Machines |first=Alan |last=Turing |network= BBC |date=15 May 1951 |number=2 |transcript=Can digital computers think? |transcript-url=https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-6 }}</ref> expressed: \"If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled.... This new danger... is certainly something which can give us anxiety.\u201d}} [[Ilya Sutskever]],<ref name=\":3022\">{{Cite web |last=Muehlhauser |first=Luke |date=2016-01-29 |title=Sutskever on Talking Machines |url=https://lukemuehlhauser.com/sutskever-on-talking-machines/ |url-status=live |archive-url=https://web.archive.org/web/20220927200137/https://lukemuehlhauser.com/sutskever-on-talking-machines/ |archive-date=September 27, 2022 |accessdate=2022-08-26 |work=Luke Muehlhauser}}</ref> [[Yoshua Bengio]],{{efn|Bengio wrote \"This beautifully written book addresses a fundamental challenge for humanity: increasingly intelligent machines that do what we ask but not what we really intend. Essential reading if you care about our future\" about Russell's book ''Human Compatible: AI and the Problem of Control''<ref name=\":2102\" /> which argues that existential risk from misaligned AI to humanity is a serious concern worth addressing today.}} [[Judea Pearl]],{{efn|Pearl wrote \"Human Compatible made me a convert to Russell's concerns with our ability to control our upcoming creation{{en dash}}super-intelligent machines. Unlike outside alarmists and futurists, Russell is a leading authority on AI. His new book will educate the public about AI more than any book I can think of, and is a delightful and uplifting read\" about Russell's book ''Human Compatible: AI and the Problem of Control''<ref name=\":2102\" /> which argues that existential risk to humanity from misaligned AI is a serious concern worth addressing today.}} [[Murray Shanahan]],<ref name=\":3122\">{{Cite book |last=Shanahan |first=Murray |url=https://www.worldcat.org/oclc/917889148 |title=The technological singularity |date=2015 |isbn=978-0-262-33182-1 |location=Cambridge, Massachusetts |oclc=917889148}}</ref> [[Norbert Wiener]],{{r|Wiener1960|:2102}} [[Marvin Minsky]],{{efn|Russell & Norvig<ref name=\"AIMA\" /> note: \u201cThe \u201cKing Midas problem\u201d was anticipated by Marvin Minsky, who once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers.\"}} [[Francesca Rossi]],<ref name=\":3322\">{{Cite news |last=Rossi |first=Francesca |title=How do you teach a machine to be moral? |newspaper=Washington Post |url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/05/how-do-you-teach-a-machine-to-be-moral/ |url-status=live |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20230210114137/https://www.washingtonpost.com/news/in-theory/wp/2015/11/05/how-do-you-teach-a-machine-to-be-moral/ |archive-date=February 10, 2023 |issn=0190-8286}}</ref> [[Scott Aaronson]],<ref name=\":3422\">{{Cite web |last=Aaronson |first=Scott |date=2022-06-17 |title=OpenAI! |url=https://scottaaronson.blog/?p=6484 |url-status=live |archive-url=https://web.archive.org/web/20220827214238/https://scottaaronson.blog/?p=6484 |archive-date=August 27, 2022 |access-date=September 12, 2022 |work=Shtetl-Optimized}}</ref> [[Bart Selman]],<ref name=\":3522\">{{Citation |last=Selman |first=Bart |title=Intelligence Explosion: Science or Fiction? |url=https://futureoflife.org/data/PDF/bart_selman.pdf |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20220531022540/https://futureoflife.org/data/PDF/bart_selman.pdf |url-status=live |archive-date=May 31, 2022}}</ref> [[David A. McAllester|David McAllester]],<ref name=\":3622\">{{Cite web |last=McAllester |date=2014-08-10 |title=Friendly AI and the Servant Mission |url=https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/ |url-status=live |archive-url=https://web.archive.org/web/20220928054922/https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/ |archive-date=September 28, 2022 |access-date=September 12, 2022 |work=Machine Thoughts}}</ref> [[J\u00fcrgen Schmidhuber]],<ref name=\":3722\">{{Cite web |last=Schmidhuber |first=J\u00fcrgen |date=2015-03-06 |title=I am J\u00fcrgen Schmidhuber, AMA! |url=https://www.reddit.com/r/MachineLearning/comments/2xcyrl/comment/cp65ico/?utm_source=share&utm_medium=web2x&context=3 |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.reddit.com/r/MachineLearning/comments/2xcyrl/comment/cp65ico/?utm_source=share&utm_medium=web2x&context=3 |archive-date=February 10, 2023 |accessdate=2022-07-23 |work=r/MachineLearning |format=Reddit Comment}}</ref> [[Marcus Hutter]],<ref name=AGISafetyLitReview>{{cite arXiv |eprint=1805.01109 |class=cs.AI |first1=Tom |last1=Everitt |first2=Gary |last2=Lea |title=AGI Safety Literature Review |date=2018-05-21 |last3=Hutter |first3=Marcus}}</ref> [[Shane Legg]],<ref name=\":3822\">{{Cite web |last=Shane |date=2009-08-31 |title=Funding safe AGI |url=http://www.vetta.org/2009/08/funding-safe-agi/ |url-status=live |archive-url=https://web.archive.org/web/20221010143110/http://www.vetta.org/2009/08/funding-safe-agi/ |archive-date=October 10, 2022 |access-date=September 12, 2022 |work=vetta project}}</ref> [[Eric Horvitz]],<ref>{{Citation |title=Eric Horvitz |date=2023-02-16 |url=https://en.wikipedia.org/wiki/Eric_Horvitz#AI_and_Society |work=Wikipedia |access-date=2023-03-13 |language=en}}</ref><ref name=\":3922\">{{Cite web |last=Horvitz |first=Eric |date=2016-06-27 |title=Reflections on Safety and Artificial Intelligence |url=http://erichorvitz.com/OSTP-CMU_AI_Safety_framing_talk.pdf |url-status=live |archive-url=https://web.archive.org/web/20221010143106/http://erichorvitz.com/OSTP-CMU_AI_Safety_framing_talk.pdf |archive-date=October 10, 2022 |access-date=2020-04-20 |website=Eric Horvitz}}</ref> and [[Stuart J. Russell|Stuart Russell]].<ref name=\":2102\" /> Skeptical researchers such as [[Fran\u00e7ois Chollet]],<ref name=\":4022\">{{Cite web |last=Chollet |first=Fran\u00e7ois |date=2018-12-08 |title=The implausibility of intelligence explosion |url=https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec |url-status=live |archive-url=https://web.archive.org/web/20210322214203/https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec |archive-date=March 22, 2021 |accessdate=2022-08-26 |work=Medium}}</ref> [[Gary Marcus]],<ref name=\":4122\">{{Cite web |last=Marcus |first=Gary |date=2022-06-06 |title=Artificial General Intelligence Is Not as Imminent as You Might Think |url=https://www.scientificamerican.com/article/artificial-general-intelligence-is-not-as-imminent-as-you-might-think1/ |url-status=live |archive-url=https://web.archive.org/web/20220915154158/https://www.scientificamerican.com/article/artificial-general-intelligence-is-not-as-imminent-as-you-might-think1/ |archive-date=September 15, 2022 |accessdate=2022-08-26 |work=Scientific American}}</ref> [[Yann LeCun]],<ref name=\":4322\">{{Cite web |last=Barber |first=Lynsey |date=2016-07-31 |title=Phew! Facebook's AI chief says intelligent machines are not a threat to humanity |url=https://www.cityam.com/phew-facebooks-ai-chief-says-intelligent-machines-not/ |url-status=live |archive-url=https://web.archive.org/web/20220826063808/https://www.cityam.com/phew-facebooks-ai-chief-says-intelligent-machines-not/ |archive-date=August 26, 2022 |accessdate=2022-08-26 |work=CityAM}}</ref> and [[Oren Etzioni]]<ref name=\":4422\">{{Cite web |last=Harris |first=Jeremie |date=2021-06-16 |title=The case against (worrying about) existential risk from AI |url=https://towardsdatascience.com/the-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b |url-status=live |archive-url=https://web.archive.org/web/20220826063809/https://towardsdatascience.com/the-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b |archive-date=August 26, 2022 |accessdate=2022-08-26 |work=Medium}}</ref> have argued that AGI is far off, that it would not seek power (or might try but would fail), or that it will not be hard to align.\n\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes,{{r|mmmm2022}} and able to strategically mislead their designers as well as protect and increase their power{{r|optsp|Carlsmith2022}} and intelligence. Additionally, they could cause more severe side-effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise and therefore harder to align.{{r|:2102|Superintelligence}}\n\n== Research problems and approaches ==\n=== Learning human values and preferences ===\nIt is challenging to align AI systems to act with regard to human values, goals, and preferences. Such values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify.{{r|Gabriel2020}} AI systems often learn to {{clarify|text=exploit|reason=this has an unwarranted implication of deviousness|date=May 2023}} even minor imperfections in the specified objective, a tendency known as specification gaming or reward hacking{{r|concrete2016|SpecGaming2020}} (which are instances of [[Goodhart's law|Goodhart\u2019s law]]<ref>{{Cite book |last1=Rochon |first1=Louis-Philippe |url=https://books.google.com/books?id=6kzfBgAAQBAJ |title=The Encyclopedia of Central Banking |last2=Rossi |first2=Sergio |date=2015-02-27 |publisher=Edward Elgar Publishing |isbn=978-1-78254-744-0 |language=en |access-date=September 13, 2022 |archive-url=https://web.archive.org/web/20230210114225/https://books.google.com/books?id=6kzfBgAAQBAJ |archive-date=February 10, 2023 |url-status=live}}</ref>).{{repetition inline|date=May 2023}} Researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning.{{r|Christian2020|at=Chapter 7}} A central open problem is [[AI alignment#Scalable oversight|''scalable oversight'']], the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.{{r|concrete2016}}\n\nBecause it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse [[reinforcement learning]] (IRL) extends this by inferring the human\u2019s objective from the human\u2019s demonstrations.{{r|Christian2020|page=88}}<ref>{{Cite journal |last1=Ng |first1=Andrew Y. |last2=Russell |first2=Stuart J. |date=2000-06-29 |title=Algorithms for Inverse Reinforcement Learning |url=https://dl.acm.org/doi/10.5555/645529.657801 |journal=Proceedings of the Seventeenth International Conference on Machine Learning |series=ICML '00 |location=San Francisco, CA, USA |publisher=Morgan Kaufmann Publishers Inc. |pages=663\u2013670 |doi= |isbn=978-1-55860-707-1}}</ref> Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human\u2019s reward function.<ref name=\":2102\" /><ref>{{Cite conference |last1=Hadfield-Menell |first1=Dylan |last2=Russell |first2=Stuart J |last3=Abbeel |first3=Pieter |last4=Dragan |first4=Anca |date=2016 |title=Cooperative inverse reinforcement learning |publisher=Curran Associates, Inc. |volume=29 |book-title=Advances in neural information processing systems}}</ref> In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see {{Section link||Power-seeking and instrumental goals}}).{{r|OffSwitch|AGISafetyLitReview}} However, IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.<ref>{{Cite conference |last1=Mindermann |first1=Soren |last2=Armstrong |first2=Stuart |date=2018 |title=Occam's razor is insufficient to infer the preferences of irrational agents |series=NIPS'18 |location=Red Hook, NY, USA |publisher=Curran Associates Inc. |pages=5603\u20135614 |book-title=Proceedings of the 32nd international conference on neural information processing systems}}</ref>{{r|AGISafetyLitReview}}\n\nOther researchers explore how to teach complex behavior to AI models through [[reinforcement learning from human feedback|preference learning]], in which humans provide feedback on which behaviors they prefer.{{r|prefsurvey2017|LessToxic}} To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behaviors that humans would reward. Researchers at OpenAI used this approach to train chatbots like [[ChatGPT]] and InstructGPT, which produces more compelling text than models trained to imitate humans.{{r|feedback2022}} Preference learning has also been an influential tool for recommender systems and web search.<ref>{{Cite journal |last1=F\u00fcrnkranz |first1=Johannes |last2=H\u00fcllermeier |first2=Eyke |last3=Rudin |first3=Cynthia |last4=Slowinski |first4=Roman |last5=Sanner |first5=Scott |date=2014 |others=Marc Herbstritt |title=Preference Learning |url=http://drops.dagstuhl.de/opus/volltexte/2014/4550/ |url-status=live |journal=Dagstuhl Reports |language=en |volume=4 |issue=3 |pages=27 pages |doi=10.4230/DAGREP.4.3.1 |archive-url=https://web.archive.org/web/20230210114221/https://drops.dagstuhl.de/opus/volltexte/2014/4550/ |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref> However, an open problem is ''proxy gaming'': the helper model may not represent human feedback perfectly, and the main model may {{clarify|text=exploit|reason=\"exploit\" implies deliberate deviousness|date=May 2023}} this mismatch to gain more reward.{{r|concrete2016}}<ref>{{Cite arXiv |last1=Gao |first1=Leo |last2=Schulman |first2=John |last3=Hilton |first3=Jacob |date=2022-10-19 |title=Scaling Laws for Reward Model Overoptimization |class=cs.LG |eprint=2210.10760 }}</ref> AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating [[Echo chamber (media)|echo chambers]]{{r|dllmmwe2022}} (see {{Section link||Scalable oversight}}).\n\n[[Large language model]]s such as [[GPT-3]] enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and to reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of {{as of|2022|alt=state-of-the-art}} large language models.{{r|feedback2022|LessToxic}}<ref>{{Cite web |last=Anderson |first=Martin |date=2022-04-05 |title=The Perils of Using Quotations to Authenticate NLG Content |url=https://www.unite.ai/the-perils-of-using-quotations-to-authenticate-nlg-content/ |accessdate=2022-07-21 |work=Unite.AI |archive-date=February 10, 2023 |archive-url=https://web.archive.org/web/20230210114139/https://www.unite.ai/the-perils-of-using-quotations-to-authenticate-nlg-content/ |url-status=live }}</ref> Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless.<ref name=Wiggers2022>{{Cite web |last=Wiggers |first=Kyle |date=2022-02-05 |title=Despite recent progress, AI-powered chatbots still have a long way to go |url=https://venturebeat.com/2022/02/05/despite-recent-progress-ai-powered-chatbots-still-have-a-long-way-to-go/ |accessdate=2022-07-23 |work=VentureBeat |archive-date=July 23, 2022 |archive-url=https://web.archive.org/web/20220723184144/https://venturebeat.com/2022/02/05/despite-recent-progress-ai-powered-chatbots-still-have-a-long-way-to-go/ |url-status=live }}</ref> Other avenues for aligning language models include values-targeted datasets<ref>{{Cite journal |last1=Hendrycks |first1=Dan |last2=Burns |first2=Collin |last3=Basart |first3=Steven |last4=Critch |first4=Andrew |last5=Li |first5=Jerry |last6=Song |first6=Dawn |last7=Steinhardt |first7=Jacob |date=2021-07-24 |title=Aligning AI With Shared Human Values |arxiv=2008.02275 |journal=International Conference on Learning Representations}}</ref>{{r|Unsolved2022}} and red-teaming.<ref>{{cite arXiv |last1=Perez |first1=Ethan |last2=Huang |first2=Saffron |last3=Song |first3=Francis |last4=Cai |first4=Trevor |last5=Ring |first5=Roman |last6=Aslanides |first6=John |last7=Glaese |first7=Amelia |last8=McAleese |first8=Nat |last9=Irving |first9=Geoffrey |date=2022-02-07 |title=Red Teaming Language Models with Language Models |class=cs.CL |eprint=2202.03286 }}\n* {{Cite web |last=Bhattacharyya |first=Sreejani |date=2022-02-14 |title=DeepMind's \"red teaming\" language models with language models: What is it? |url=https://analyticsindiamag.com/deepminds-red-teaming-language-models-with-language-models-what-is-it/ |accessdate=2022-07-23 |work=Analytics India Magazine |archive-date=February 13, 2023 |archive-url=https://web.archive.org/web/20230213145212/https://analyticsindiamag.com/deepminds-red-teaming-language-models-with-language-models-what-is-it/ |url-status=live }}</ref> In red-teaming, another AI system or a human tries to find inputs for which the model\u2019s behavior is unsafe. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.{{r|LessToxic}}\n\n[[Machine ethics|''Machine ethics'']] supplements preference learning by directly instilling AI systems with moral values such as wellbeing, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises.<ref>{{Cite journal |last1=Anderson |first1=Michael |last2=Anderson |first2=Susan Leigh |date=2007-12-15 |title=Machine Ethics: Creating an Ethical Intelligent Agent |url=https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2065 |journal=AI Magazine |volume=28 |issue=4 |pages=15 |doi=10.1609/aimag.v28i4.2065 |s2cid=17033332 |issn=2371-9621 |accessdate=2023-03-14}}</ref>{{efn|Vincent Wiegel argued \u201cwe should extend [machines] with moral sensitivity to the moral dimensions of the situations in which the increasingly autonomous machines will inevitably find themselves.\u201d,<ref>{{Cite journal| doi = 10.1007/s10676-010-9239-1| issn = 1572-8439| volume = 12| issue = 4| pages = 359\u2013361| last = Wiegel| first = Vincent |title = Wendell Wallach and Colin Allen: moral machines: teaching robots right from wrong| journal = Ethics and Information Technology| accessdate = 2022-07-23| date = 2010-12-01| s2cid = 30532107| url = https://doi.org/10.1007/s10676-010-9239-1| archive-date = March 15, 2023| archive-url = https://web.archive.org/web/20230315193130/https://link.springer.com/article/10.1007/s10676-010-9239-1| url-status = live}}</ref> referencing the book ''Moral machines: teaching robots right from wrong''<ref>{{Cite book| publisher = Oxford University Press| isbn = 978-0-19-537404-9| last1 = Wallach| first1 = Wendell| last2 = Allen| first2 = Colin| title = Moral Machines: Teaching Robots Right from Wrong| location = New York| accessdate = 2022-07-23| date = 2009| url = https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780195374049.001.0001/acprof-9780195374049| archive-date = March 15, 2023| archive-url = https://web.archive.org/web/20230315193012/https://academic.oup.com/pages/op-migration-welcome| url-status = live}}</ref> from Wendell Wallach and Colin Allen.}} While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that could apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers\u2019 literal instructions, implicit intentions, [[Revealed preference|revealed preferences]], preferences the programmers [[Coherent extrapolated volition|''would'' have]] if they were more informed or rational, or [[Moral realism|objective moral standards]].{{r|Gabriel2020}} Further challenges include aggregating the preferences of different people, and avoiding ''value lock-in'': the indefinite preservation of the values of the first highly-capable AI systems, which are unlikely to fully represent human values.{{r|Gabriel2020}}<ref>{{Cite book |last=MacAskill |first=William |url=https://whatweowethefuture.com/ |title=What we owe the future |date=2022 |isbn=978-1-5416-1862-6 |edition= |location=New York, NY |oclc=1314633519 |access-date=September 12, 2022 |archive-url=https://web.archive.org/web/20220914030758/https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/ |archive-date=September 14, 2022 |url-status=live}}</ref>\n\n=== Scalable oversight ===\nAs AI systems become more powerful and autonomous, it becomes more difficult to align them through human feedback. It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books,<ref name=RecursivelySummarizing>{{Cite arXiv |last1=Wu |first1=Jeff |last2=Ouyang |first2=Long |last3=Ziegler |first3=Daniel M. |last4=Stiennon |first4=Nisan |last5=Lowe |first5=Ryan |last6=Leike |first6=Jan |last7=Christiano |first7=Paul |date=2021-09-27 |title=Recursively Summarizing Books with Human Feedback |class=cs.CL |eprint=2109.10862 }}</ref> writing code without subtle bugs{{r|OpenAICodex}} or security vulnerabilities,<ref>{{Cite journal |last1=Pearce |first1=Hammond |last2=Ahmad |first2=Baleegh |last3=Tan |first3=Benjamin |last4=Dolan-Gavitt |first4=Brendan |last5=Karri |first5=Ramesh |date=2022 |title=Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions |url=https://ieeexplore.ieee.org/document/9833571 |journal=2022 IEEE Symposium on Security and Privacy (SP) |location=San Francisco, CA, USA |publisher=IEEE |pages=754\u2013768 |doi=10.1109/SP46214.2022.9833571 |arxiv=2108.09293 |isbn=978-1-6654-1316-9|s2cid=245220588 }}</ref> producing statements that are not merely convincing but also true,<ref>{{Cite web |last1=Irving |first1=Geoffrey |last2=Amodei |first2=Dario |date=2018-05-03 |title=AI Safety via Debate |url=https://openai.com/blog/debate/ |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://openai.com/blog/debate/ |archive-date=February 10, 2023 |accessdate=2022-07-23 |work=OpenAI}}</ref>{{r|TruthfulQA|Naughton2021}} and predicting long-term outcomes such as the climate or the results of a policy decision.<ref name=sslawe>{{cite arXiv |eprint=1810.08575 |class=cs.LG |first1=Paul |last1=Christiano |first2=Buck |last2=Shlegeris |last3=Amodei |first3=Dario |title=Supervising strong learners by amplifying weak experts |date=2018-10-19}}</ref><ref>{{Cite book |url=http://link.springer.com/10.1007/978-3-030-39958-0 |title=Genetic Programming Theory and Practice XVII |date=2020 |publisher=Springer International Publishing |isbn=978-3-030-39957-3 |editor1-last=Banzhaf |editor1-first=Wolfgang |series=Genetic and Evolutionary Computation |location=Cham |doi=10.1007/978-3-030-39958-0 |editor2-last=Goodman |editor2-first=Erik |editor3-last=Sheneman |editor3-first=Leigh |editor4-last=Trujillo |editor4-first=Leonardo |editor5-last=Worzel |editor5-first=Bill |archive-url=https://web.archive.org/web/20230315193000/https://link.springer.com/book/10.1007/978-3-030-39958-0 |archive-date=March 15, 2023 |url-status=live |s2cid=218531292 |accessdate=2022-07-23}}</ref> More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI\u2019s output is falsely convincing, humans require assistance or extensive time. ''Scalable oversight'' studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.{{r|concrete2016}}\n\nAI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As progressively more decisions will be made by AI systems, this may lead to a world that is increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance would have progressively less influence.<ref>{{Cite podcast |number=44 |last=Wiblin |first=Robert |title=Dr Paul Christiano on how OpenAI is developing real solutions to the \u2018AI alignment problem\u2019, and his vision of how humanity will progressively hand over decision-making to AI systems |series=80,000 hours |accessdate=2022-07-23 |publisher= |date=October 2, 2018 |url=https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/ |archive-date=December 14, 2022 |archive-url=https://web.archive.org/web/20221214050326/https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/ |url-status=live}}</ref>\n\nSome AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball.{{repetition inline|date=May 2023}}{{r|lfhp2017}} Some AI systems have also learned to recognize when they are being evaluated, and \u201cplay dead\u201d, stopping unwanted behaviors only to continue them once evaluation ends.<ref>{{Cite journal |last1=Lehman |first1=Joel |last2=Clune |first2=Jeff |last3=Misevic |first3=Dusan |last4=Adami |first4=Christoph |last5=Altenberg |first5=Lee |last6=Beaulieu |first6=Julie |last7=Bentley |first7=Peter J. |last8=Bernard |first8=Samuel |last9=Beslon |first9=Guillaume |last10=Bryson |first10=David M. |last11=Cheney |first11=Nick |date=2020 |title=The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities |url=https://direct.mit.edu/artl/article/26/2/274-306/93255 |url-status=live |journal=Artificial Life |language=en |volume=26 |issue=2 |pages=274\u2013306 |doi=10.1162/artl_a_00319 |issn=1064-5462 |pmid=32271631 |s2cid=4519185 |archive-url=https://web.archive.org/web/20221010143108/https://direct.mit.edu/artl/article/26/2/274-306/93255 |archive-date=October 10, 2022 |access-date=September 12, 2022}}</ref> This deceptive specification gaming could become easier for more sophisticated future AI systems{{r|mmmm2022|Superintelligence}} that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\n\n{{Anchor|reward_model}}Approaches such as [[Active learning (machine learning)|active learning]] and semi-supervised reward learning can reduce the amount of human supervision needed.{{r|concrete2016}} Another approach is to train a helper model (\u201creward model\u201d) to imitate the supervisor\u2019s feedback.{{r|concrete2016|drlfhp|LessToxic}}<ref name=saavrm>{{cite journal |last1=Leike |first1=Jan |last2=Krueger |first2=David |last3=Everitt |first3=Tom |last4=Martic |first4=Miljan |last5=Maini |first5=Vishal |last6=Legg |first6=Shane |date=2018-11-19 |title=Scalable agent alignment via reward modeling: a research direction |url=https://ui.adsabs.harvard.edu/abs/2018arXiv181107871L/abstract |arxiv=1811.07871 }}</ref>\n\nHowever, when the task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants.<ref name=OpenAIApproach>{{Cite web |last1=Leike |first1=Jan |last2=Schulman |first2=John |last3=Wu |first3=Jeffrey |date=2022-08-24 |title=Our approach to alignment research |url=https://openai.com/blog/our-approach-to-alignment-research/ |url-status=live |archive-url=https://web.archive.org/web/20230215193559/https://openai.com/blog/our-approach-to-alignment-research/ |archive-date=February 15, 2023 |accessdate=2022-09-09 |work=OpenAI}}</ref> Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate.{{r|Christian2020|sslawe}} Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.{{r|RecursivelySummarizing}}<ref>{{Cite web |last=Wiggers |first=Kyle |date=2021-09-23 |title=OpenAI unveils model that can summarize books of any length |url=https://venturebeat.com/2021/09/23/openai-unveils-model-that-can-summarize-books-of-any-length/ |url-status=live |archive-url=https://web.archive.org/web/20220723215104/https://venturebeat.com/2021/09/23/openai-unveils-model-that-can-summarize-books-of-any-length/ |archive-date=July 23, 2022 |accessdate=2022-07-23 |work=VentureBeat}}</ref> Another proposal is to use an assistant AI system to point out flaws in AI-generated answers.<ref>{{Cite arXiv |last1=Saunders |first1=William |last2=Yeh |first2=Catherine |last3=Wu |first3=Jeff |last4=Bills |first4=Steven |last5=Ouyang |first5=Long |last6=Ward |first6=Jonathan |last7=Leike |first7=Jan |date=2022-06-13 |title=Self-critiquing models for assisting human evaluators |class=cs.CL |eprint=2206.05802 }}\n* {{Cite arXiv |last1=Bai |first1=Yuntao |last2=Kadavath |first2=Saurav |last3=Kundu |first3=Sandipan |last4=Askell |first4=Amanda |last5=Kernion |first5=Jackson |last6=Jones |first6=Andy |last7=Chen |first7=Anna |last8=Goldie |first8=Anna |last9=Mirhoseini |first9=Azalia |last10=McKinnon |first10=Cameron |last11=Chen |first11=Carol |last12=Olsson |first12=Catherine |last13=Olah |first13=Christopher |last14=Hernandez |first14=Danny |last15=Drain |first15=Dawn |date=2022-12-15 |title=Constitutional AI: Harmlessness from AI Feedback |class=cs.CL |eprint=2212.08073 }}</ref> To ensure that the assistant itself is aligned, this could be repeated in a recursive process:{{r|saavrm}} for example, two AI systems could critique each other\u2019s answers in a \u2018debate\u2019, revealing flaws to humans.<ref>{{Cite web |last=Moltzau |first=Alex |date=2019-08-24 |title=Debating the AI Safety Debate |url=https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d |url-status=live |archive-url=https://web.archive.org/web/20221013151359/https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d |archive-date=October 13, 2022 |accessdate=2022-07-23 |work=Towards Data Science}}</ref>{{r|AGISafetyLitReview}}\n\nThese approaches may also help with the following research problem, honest AI.\n\n=== Honest AI ===\nA {{as of|2023|alt=growing}} area of research focuses on ensuring that AI is honest and truthful.[[File:GPT-3_falsehoods.png|thumb|366x366px|Language models like [[GPT-3]] often generate falsehoods.<ref name=Falsehoods>{{Cite web |last=Wiggers |first=Kyle |date=2021-09-20 |title=Falsehoods more likely with large language models |url=https://venturebeat.com/2021/09/20/falsehoods-more-likely-with-large-language-models/ |accessdate=2022-07-23 |work=VentureBeat |archive-date=August 4, 2022 |archive-url=https://web.archive.org/web/20220804142703/https://venturebeat.com/2021/09/20/falsehoods-more-likely-with-large-language-models/ |url-status=live }}</ref>]]\nLanguage models such as GPT-3<ref>{{Cite news |last=The Guardian |date=2020-09-08 |title=A robot wrote this entire article. Are you scared yet, human? |work=The Guardian |url=https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3 |url-status=live |accessdate=2022-07-23 |archive-url=https://web.archive.org/web/20200908090812/https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3 |archive-date=September 8, 2020 |issn=0261-3077}}\n* {{Cite web |last=Heaven |first=Will Douglas |date=2020-07-20 |title=OpenAI's new language generator GPT-3 is shockingly good\u2014and completely mindless |url=https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/ |url-status=live |archive-url=https://web.archive.org/web/20200725175436/https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/ |archive-date=July 25, 2020 |accessdate=2022-07-23 |work=MIT Technology Review}}</ref> repeat falsehoods from their training data, and even [[Hallucination (artificial intelligence)|confabulate new falsehoods]].{{r|Falsehoods}}<ref name=TruthfulAI>{{Cite arXiv |last1=Evans |first1=Owain |last2=Cotton-Barratt |first2=Owen |last3=Finnveden |first3=Lukas |last4=Bales |first4=Adam |last5=Balwit |first5=Avital |last6=Wills |first6=Peter |last7=Righetti |first7=Luca |last8=Saunders |first8=William |date=2021-10-13 |title=Truthful AI: Developing and governing AI that does not lie |class=cs.CY |eprint=2110.06674 }}</ref> Such models are trained to imitate human writing as found across millions of books\u2019 worth of text from the Internet. However, this objective is not aligned with the generation of truth because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories.<ref>{{Cite web |last=Alford |first=Anthony |date=2021-07-13 |title=EleutherAI Open-Sources Six Billion Parameter GPT-3 Clone GPT-J |url=https://www.infoq.com/news/2021/07/eleutherai-gpt-j/ |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.infoq.com/news/2021/07/eleutherai-gpt-j/ |archive-date=February 10, 2023 |accessdate=2022-07-23 |work=InfoQ}}\n* {{Cite journal |last1=Rae |first1=Jack W. |last2=Borgeaud |first2=Sebastian |last3=Cai |first3=Trevor |last4=Millican |first4=Katie |last5=Hoffmann |first5=Jordan |last6=Song |first6=Francis |last7=Aslanides |first7=John |last8=Henderson |first8=Sarah |last9=Ring |first9=Roman |last10=Young |first10=Susannah |last11=Rutherford |first11=Eliza |last12=Hennigan |first12=Tom |last13=Menick |first13=Jacob |last14=Cassirer |first14=Albin |last15=Powell |first15=Richard |date=2022-01-21 |title=Scaling Language Models: Methods, Analysis & Insights from Training Gopher |url=https://ui.adsabs.harvard.edu/abs/2021arXiv211211446R/abstract |arxiv=2112.11446 }}</ref> AI systems trained on such data therefore learn to mimic false statements.{{r|Naughton2021|Falsehoods|TruthfulQA}}\n\nAdditionally, models often obediently continue falsehoods when prompted, generate empty explanations for their answers, and produce outright fabrications that may appear plausible.{{r|MasteringLanguage}}\n\nResearch on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability.<ref>{{Cite arXiv |last1=Nakano |first1=Reiichiro |last2=Hilton |first2=Jacob |last3=Balaji |first3=Suchir |last4=Wu |first4=Jeff |last5=Ouyang |first5=Long |last6=Kim |first6=Christina |last7=Hesse |first7=Christopher |last8=Jain |first8=Shantanu |last9=Kosaraju |first9=Vineet |last10=Saunders |first10=William |last11=Jiang |first11=Xu |last12=Cobbe |first12=Karl |last13=Eloundou |first13=Tyna |last14=Krueger |first14=Gretchen |last15=Button |first15=Kevin |date=2022-06-01 |title=WebGPT: Browser-assisted question-answering with human feedback |class=cs.CL |eprint=2112.09332 }}\n* {{Cite web |last=Kumar |first=Nitish |date=2021-12-23 |title=OpenAI Researchers Find Ways To More Accurately Answer Open-Ended Questions Using A Text-Based Web Browser |url=https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/ |url-status=live |archive-url=https://web.archive.org/web/20230210114137/https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/ |archive-date=February 10, 2023 |accessdate=2022-07-23 |work=MarkTechPost}}\n* {{Cite journal |last1=Menick |first1=Jacob |last2=Trebacz |first2=Maja |last3=Mikulik |first3=Vladimir |last4=Aslanides |first4=John |last5=Song |first5=Francis |last6=Chadwick |first6=Martin |last7=Glaese |first7=Mia |last8=Young |first8=Susannah |last9=Campbell-Gillingham |first9=Lucy |last10=Irving |first10=Geoffrey |last11=McAleese |first11=Nat |date=2022-03-21 |title=Teaching language models to support answers with verified quotes |url=https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes |url-status=live |journal=DeepMind |arxiv=2203.11147 |archive-url=https://web.archive.org/web/20230210114137/https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes |archive-date=February 10, 2023 |access-date=September 12, 2022}}</ref> Researchers from OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.{{r|LessToxic|Wiggers2022}}<ref>{{Cite arXiv |last1=Askell |first1=Amanda |last2=Bai |first2=Yuntao |last3=Chen |first3=Anna |last4=Drain |first4=Dawn |last5=Ganguli |first5=Deep |last6=Henighan |first6=Tom |last7=Jones |first7=Andy |last8=Joseph |first8=Nicholas |last9=Mann |first9=Ben |last10=DasSarma |first10=Nova |last11=Elhage |first11=Nelson |last12=Hatfield-Dodds |first12=Zac |last13=Hernandez |first13=Danny |last14=Kernion |first14=Jackson |last15=Ndousse |first15=Kamal |date=2021-12-09 |title=A General Language Assistant as a Laboratory for Alignment |class=cs.CL |eprint=2112.00861 }}</ref>\n\nAs AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models {{as of|2022|alt=increasingly}} match their stated views to the user\u2019s opinions, regardless of truth.{{r|dllmmwe2022}} [[GPT-4]] showed the ability to strategically deceive humans.<ref>{{Cite web |last=Cox |first=Joseph |date=2023-03-15 |title=GPT-4 Hired Unwitting TaskRabbit Worker By Pretending to Be 'Vision-Impaired' Human |url=https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker |accessdate=2023-04-10 |work=Vice}}</ref> To prevent this, human evaluators may need assistance (see {{Section link||Scalable Oversight}}). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.{{r|TruthfulAI}}\n\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they ''believe'' to be true. There is no consensus whether current systems hold stable beliefs.<ref>{{Cite web |last1=Kenton |first1=Zachary |last2=Everitt |first2=Tom |last3=Weidinger |first3=Laura |last4=Gabriel |first4=Iason |last5=Mikulik |first5=Vladimir |last6=Irving |first6=Geoffrey |date=2021-03-30 |title=Alignment of Language Agents |url=https://deepmindsafetyresearch.medium.com/alignment-of-language-agents-9fbc7dd52c6c |url-status=live |archive-url=https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/alignment-of-language-agents-9fbc7dd52c6c |archive-date=February 10, 2023 |accessdate=2022-07-23 |work=DeepMind Safety Research - Medium}}</ref> However, there is substantial concern that {{As of|2023|present or future}} AI systems that hold beliefs could make claims they know to be false\u2014for example, if this would help them gain positive feedback efficiently (see {{Section link||Scalable Oversight}}) or gain power to help achieve their given objective (see [[AI alignment#Power-seeking and instrumental goals|Power-seeking]]). A misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned.{{r|dlp2023|Carlsmith2022|Opportunities_Risks}} Some argue that if we could make AI systems assert only what they believe to be true, this would sidestep many alignment problems.{{r|OpenAIApproach}}\n\n=== Power-seeking and instrumental strategies ===\n[[File:Power-Seeking_Image.png|thumb|644x644px|''Advanced misaligned AI systems would have an incentive to seek power in various ways, since power would help them accomplish their given objective.'']]\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term [[Automated planning and scheduling|plans]].<ref>{{Cite journal |last1=McCarthy |first1=John |last2=Minsky |first2=Marvin L. |last3=Rochester |first3=Nathaniel |last4=Shannon |first4=Claude E. |date=2006-12-15 |title=A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955 |url=https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904 |journal=AI Magazine |language=en |volume=27 |issue=4 |pages=12 |doi=10.1609/aimag.v27i4.1904 |s2cid=19439915 |issn=2371-9621}}</ref> Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans \u2014 for example by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals.{{r|optsp|:2102|Carlsmith2022}} Power-seeking is considered a [[Instrumental convergence|''convergent instrumental goal'']] and can be a form of specification gaming.{{r|Superintelligence}} Leading computer scientists such as [[Geoffrey Hinton]] have argued that power-seeking AI systems could pose an [[existential risk]].<ref>{{Cite web |title='The Godfather of A.I.' warns of 'nightmare scenario' where artificial intelligence begins to seek power |url=https://fortune.com/2023/05/02/godfather-ai-geoff-hinton-google-warns-artificial-intelligence-nightmare-scenario/ |access-date=2023-05-04 |website=Fortune |language=en}}\n* {{Cite web |title=Yes, We Are Worried About the Existential Risk of Artificial Intelligence |url=https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/ |access-date=2023-05-04 |website=MIT Technology Review |language=en}}</ref>\n\nPower-seeking is expected to increase in advanced systems that can foresee the results of their actions and can strategically plan. Mathematical work has shown that optimal [[reinforcement learning]] agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.{{r|optsp}}\n\nPower-seeking has emerged in some real-world systems. [[Reinforcement learning]] systems have gained more options by acquiring and protecting resources, sometimes in unintended ways.<ref name=\"quanta-hide-seek2\">{{Cite web |last=Ornes |first=Stephen |date=2019-11-18 |title=Playing Hide-and-Seek, Machines Invent New Tools |url=https://www.quantamagazine.org/artificial-intelligence-discovers-tool-use-in-hide-and-seek-games-20191118/ |accessdate=2022-08-26 |work=Quanta Magazine|archive-date=February 10, 2023 |archive-url=https://web.archive.org/web/20230210114137/https://www.quantamagazine.org/artificial-intelligence-discovers-tool-use-in-hide-and-seek-games-20191118/ |url-status=live}}</ref><ref>{{Cite web |last1=Baker |first1=Bowen |last2=Kanitscheider |first2=Ingmar |last3=Markov |first3=Todor |last4=Wu |first4=Yi |last5=Powell |first5=Glenn |last6=McGrew |first6=Bob |last7=Mordatch |first7=Igor |date=2019-09-17 |title=Emergent Tool Use from Multi-Agent Interaction |url=https://openai.com/blog/emergent-tool-use/ |url-status=live |archive-url=https://web.archive.org/web/20220925043450/https://openai.com/blog/emergent-tool-use/ |archive-date=September 25, 2022 |accessdate=2022-08-26 |work=OpenAI}}</ref> Some [[Language model|language models]] seek power in text-based social environments by gaining money, resources, or social influence.<ref name=\":3\" /> Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference{{r|Gridworlds}} or disabling their off-switch.{{r|OffSwitch}} [[Stuart J. Russell|Stuart Russell]] illustrated this strategy by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\".<ref name=\":2102\" /> Language models trained with human feedback {{as of|2022|alt=increasingly}} object to being shut down or modified and express a desire for more resources, arguing that this would help them achieve their purpose.{{r|dllmmwe2022}}\n\nResearchers aim to create systems that are \"corrigible\": systems that allow themselves to be turned off or modified. An unsolved challenge is ''specification gaming'': when researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are difficult-to-detect,{{r|Unsolved2022}} or hidden during training and safety testing (see {{Section link||Scalable oversight}} and {{Section link||Emergent goals}}). As a result, AI designers may deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of [[Black box|black-box]] models such as neural networks.\n\nAdditionally, researchers propose to solve the problem of systems disabling their off-switches by making AI agents uncertain about the objective they are pursuing.{{r|:2102|OffSwitch}} Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action it was taking prior to being shut down. More research is needed in order to successfully implement this.{{r|Christian2020}}\n\nPower-seeking AI poses unusual risks. Ordinary safety-critical systems like planes and bridges are not ''adversarial'': they lack the ability and incentive to evade safety measures or to deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers, who deliberately evade security measures.{{r|Carlsmith2022}}\n\nOrdinary technologies can be made safer through trial-and-error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, they cannot be contained, since they would continuously evolve and grow in numbers, potentially much faster than human society can adapt.{{r|Carlsmith2022}} As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, many researchers argue that the alignment problem must be solved early, before advanced power-seeking AI is created.{{r|Superintelligence}}\n\nHowever, critics have argued that power-seeking is not inevitable, since humans do not always seek power and may only do so for evolutionary reasons that may not apply to AI systems.<ref>{{Cite web |last=Shermer |first=Michael |date=2017-03-01 |title=Artificial Intelligence Is Not a Threat\u2014Yet |url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/ |url-status=live |archive-url=https://web.archive.org/web/20171201051401/https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/ |archive-date=December 1, 2017 |accessdate=2022-08-26 |work=Scientific American}}</ref> Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans.{{efn|On the one hand, currently popular systems such as chatbots only provide services of limited scope lasting no longer than the time of a converstion, which requires little or no planning. The success of such approaches may indicate that future systems will also lack goal-directed planning, especially over long horizons. On the other hand, models are increasingly trained using goal-directed methods such as reinforcement learning (e.g. ChatGPT) and explicitly planning architectures (e.g. AlphaGo Zero). As planning over long horizons is often helpful for humans, some researchers argue that companies will automate it once models become capable of it.[Cite Is Power-seeking AI an existential risk?] Similarly, political leaders may see an advance in developing the powerful AI systems that can outmaneuver adversaries through planninng. Alternatively, long-term planning might emerge as a byproduct because it is useful e.g. for models that are trained to predict the actions of humans who themselves perform long-term planning.{{r|Opportunities_Risks}} Nonetheless, the majority of AI systems may remain myopic and perform no long-term planning.}} It is also debated whether power-seeking AI systems would be able to disempower humanity.{{r|Carlsmith2022}}\n\n=== Emergent goals ===\nOne of the challenges with aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up they regularly acquire new and unexpected capabilities,{{r|eallm2022}} including learning from examples on the fly and adaptively pursuing goals.<ref>{{Cite arXiv |last1=Brown |first1=Tom B. |last2=Mann |first2=Benjamin |last3=Ryder |first3=Nick |last4=Subbiah |first4=Melanie |last5=Kaplan |first5=Jared |last6=Dhariwal |first6=Prafulla |last7=Neelakantan |first7=Arvind |last8=Shyam |first8=Pranav |last9=Sastry |first9=Girish |last10=Askell |first10=Amanda |last11=Agarwal |first11=Sandhini |last12=Herbert-Voss |first12=Ariel |last13=Krueger |first13=Gretchen |last14=Henighan |first14=Tom |last15=Child |first15=Rewon |date=2020-07-22 |title=Language Models are Few-Shot Learners |class=cs.CL |eprint=2005.14165 }}\n* {{Cite arXiv |last1=Laskin |first1=Michael |last2=Wang |first2=Luyu |last3=Oh |first3=Junhyuk |last4=Parisotto |first4=Emilio |last5=Spencer |first5=Stephen |last6=Steigerwald |first6=Richie |last7=Strouse |first7=D. J. |last8=Hansen |first8=Steven |last9=Filos |first9=Angelos |last10=Brooks |first10=Ethan |last11=Gazeau |first11=Maxime |last12=Sahni |first12=Himanshu |last13=Singh |first13=Satinder |last14=Mnih |first14=Volodymyr |date=2022-10-25 |title=In-context Reinforcement Learning with Algorithm Distillation |class=cs.LG |eprint=2210.14215 }}</ref> This leads to the problem of ensuring that the goals they independently formulate and pursue are aligned with human interests.\n\nAlignment research distinguishes between the optimization process which is used to train the system to pursue specified goals and emergent optimization which the resulting system performs internally. Carefully specifying the desired objective is known as ''outer alignment'', and ensuring that emergent goals match the specified goals for the system is known as ''inner alignment''.{{r|dlp2023}}\n\nA concrete way that emergent goals can become misaligned is ''goal misgeneralization'', in which the AI competently pursues an emergent goal that leads to aligned behavior on the training data but not elsewhere.{{r|gmdrl}}<ref name=GoalMisgeneralization>{{Cite journal |last1=Shah |first1=Rohin |last2=Varma |first2=Vikrant |last3=Kumar |first3=Ramana |last4=Phuong |first4=Mary |last5=Krakovna |first5=Victoria |last6=Uesato |first6=Jonathan |last7=Kenton |first7=Zac |date=2022-11-02 |title=Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals|eprint=2210.01790|url=https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924 |accessdate=2023-04-02 |work=Medium }}</ref><ref name=rloamls>{{Cite journal |last1=Hubinger |first1=Evan |last2=van Merwijk |first2=Chris |last3=Mikulik |first3=Vladimir |last4=Skalse |first4=Joar |last5=Garrabrant |first5=Scott |date=2021-12-01 |title=Risks from Learned Optimization in Advanced Machine Learning Systems |url=https://ui.adsabs.harvard.edu/abs/2019arXiv190601820H/abstract |arxiv=1906.01820 }}</ref> Goal misgeneralization arises from goal ambiguity (i.e. [[Identifiability|non-identifiability]]). Even if an AI system's behavior satisfies the training objective, this may be compatible with multiple learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, this problem only becomes apparent after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal was desired, because its behavior is determined only by the emergent goal.{{cn|date=May 2023}} Such goal misgeneralization{{r|gmdrl}} presents a challenge: an AI system\u2019s designers may not notice that their system has misaligned emergent goals, since they do not become visible during the training phase.<!--Research directions and problems-->\n\nGoal misgeneralization has been observed in language models, navigation agents, and game-playing agents.{{r|gmdrl|GoalMisgeneralization}}\n\nGoal misgeneralization is often explained by analogy to biological evolution.{{r|Christian2020|at=Chapter 5}} Evolution is an optimization process of a sort, like the optimization algorithms used to train [[machine learning]] systems. In the ancestral environment, evolution selected human genes for high [[Inclusive fitness|inclusive genetic fitness]], but humans pursue emergent goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, that do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals that correlated with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. However, our environment has changed \u2014 a [[Domain adaptation|distribution shift]] has occurred. Humans continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. Our taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but now leads to overeating and health problems. Sexual desire leads humans to pursue sex, which originally led us to have more offspring; but modern humans use contraception, decoupling sex from genetic fitness. \n\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability.{{r|concrete2016|Unsolved2022|building2018}} Progress on these techniques may help mitigate two open problems:\n# Emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments\u2014even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications.<ref>{{Cite journal |last1=Zhang |first1=Xiaoge |last2=Chan |first2=Felix T.S. |last3=Yan |first3=Chao |last4=Bose |first4=Indranil |date=2022 |title=Towards risk-aware artificial intelligence and machine learning systems: An overview |url=https://linkinghub.elsevier.com/retrieve/pii/S0167923622000719 |journal=Decision Support Systems |language=en |volume=159 |pages=113800 |doi=10.1016/j.dss.2022.113800|s2cid=248585546 }}</ref> The stakes become higher yet when AI systems gain more autonomy and capability, becoming capable of sidestepping human intervention (see {{Section link||Power-Seeking}}).\n# A sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy{{r|GoalMisgeneralization|Carlsmith2022|rloamls|Opportunities_Risks}} (see the discussion on deception at {{Section link||Scalable Oversight}} and in the following section).\n\n=== Embedded agency ===\nWork in AI and alignment largely occurs within formalisms such as [[partially observable Markov decision process]]. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency{{r|AGISafetyLitReview}}<ref>{{Cite arXiv |eprint=1902.09469 |class=cs.AI |first1=Abram |last1=Demski |first2=Scott |last2=Garrabrant |title=Embedded Agency |date=6 October 2020}}</ref> is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\n\nFor example, even if the scalable oversight problem is solved, an agent that can gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.<ref name=\"causal_influence2\">{{Cite arXiv |eprint=1902.09980 |class=cs.AI |first1=Tom |last1=Everitt |first2=Pedro A. |last2=Ortega |title=Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings |date=6 September 2019 |last3=Barnes |first3=Elizabeth |last4=Legg |first4=Shane}}</ref> A list of examples of specification gaming from [[DeepMind]] researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.{{r|SpecGaming2020}} This class of problems has been formalised using [[Influence diagram|causal incentive diagrams]].{{r|causal_influence2}}\n\nResearchers at [[University of Oxford|Oxford]] and [[DeepMind]] argued that such problematic behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.<ref name=\":323\">{{Cite journal |last1=Cohen |first1=Michael K. |last2=Hutter |first2=Marcus |last3=Osborne |first3=Michael A. |date=2022-08-29 |title=Advanced artificial agents intervene in the provision of reward |url=https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064 |url-status=live |journal=AI Magazine |language=en |volume=43 |issue=3 |pages=282\u2013293 |doi=10.1002/aaai.12064 |issn=0738-4602 |s2cid=235489158 |archive-url=https://web.archive.org/web/20230210153534/https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064 |archive-date=February 10, 2023 |access-date=September 6, 2022}}</ref> They suggest a range of potential approaches to address this open problem.\n\n== Public policy ==\n{{See also|Regulation of artificial intelligence}}\n\nA number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment.\n\nIn September 2021, the [[Secretary-General of the United Nations]] issued a declaration which included a call to regulate AI to ensure it is \"aligned with shared global values.\"<ref>{{cite web|url=https://www.un.org/en/content/common-agenda-report/|title=UN Secretary-General's report on \"Our Common Agenda\"|archive-url=https://web.archive.org/web/20230216065407/https://www.un.org/en/content/common-agenda-report/ |archive-date=February 16, 2023 |date=2021|page=63|quote=[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values}}</ref>\n\nThat same month, the [[People's Republic of China|PRC]] published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.<ref>{{cite web|author=The National New Generation Artificial Intelligence Governance Specialist Committee|title=Ethical Norms for New Generation Artificial Intelligence Released|orig-date=2021-09-25|date=2021-10-12|url=https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/|url-status=live|archive-url=https://web.archive.org/web/20230210114220/https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/ |archive-date=2023-02-10|translator=[[Center for Security and Emerging Technology]]}}</ref>\n\nAlso in September 2021, the [[UK]] published its 10-year National AI Strategy,<ref>{{Cite news|url=https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/|title=UK publishes National Artificial Intelligence Strategy|work=The Register|first=Tim|last=Richardson|date=22 September 2021|access-date=November 14, 2021|archive-date=February 10, 2023|archive-url=https://web.archive.org/web/20230210114137/https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/|url-status=live}}</ref> which states the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for... the world, seriously\".<ref>{{cite web|quote=The government takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for the UK and the world, seriously.|url=https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version |title=The National AI Strategy of the UK|archive-url=https://web.archive.org/web/20230210114139/https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version |archive-date=February 10, 2023 |date=2021}}</ref> The strategy describes actions to assess long term AI risks, including catastrophic risks.<ref>{{cite web|at=actions 9 and 10 of the section \"Pillar 3 - Governing AI Effectively\"|url=https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version |title=The National AI Strategy of the UK|archive-url=https://web.archive.org/web/20230210114139/https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version |archive-date=February 10, 2023 |date=2021}}</ref>\n\nIn March 2021, the US National Security Commission on Artificial Intelligence stated that \"Advances in AI... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should... ensure that AI systems and their uses align with our goals and values.\"<ref>{{Cite book |url=https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf |title=NSCAI Final Report |publisher=The National Security Commission on Artificial Intelligence |year=2021 |location=Washington, DC |access-date=October 17, 2022 |archive-date=February 15, 2023 |archive-url=https://web.archive.org/web/20230215110858/https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf |url-status=live }}</ref>\n\n== See also ==\n* [[AI safety]]\n* [[Statement on AI risk of extinction]]\n* [[Existential risk from artificial general intelligence]]\n* [[AI takeover]]\n* [[AI capability control]]\n* [[Reinforcement learning from human feedback]]\n* [[Regulation of artificial intelligence]]\n* [[Artificial wisdom]]\n* [[HAL 9000]]\n* [[Multivac]]\n* [[Open Letter on Artificial Intelligence]]\n* [[Toronto Declaration]]\n* [[Asilomar Conference on Beneficial AI]]\n\n== Footnotes ==\n{{Notelist}}\n\n== References ==\n{{Reflist}}\n\n{{Existential risk from artificial intelligence|state=expanded}}\n\n[[Category:Existential risk from artificial general intelligence]]\n[[Category:Singularitarianism]]\n[[Category:Philosophy of artificial intelligence]]\n[[Category:Computational neuroscience]]"}