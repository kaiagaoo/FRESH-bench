{"title": "Hallucination (artificial intelligence)", "page_id": 72607666, "revision_id": 1130602076, "revision_timestamp": "2022-12-31T00:53:39Z", "content": "{{short description|Any confident unjustified claim by an AI}}\n{{Use dmy dates|date=December 2022}}\nIn artificial intelligence, a '''hallucination''' is a confident response by an artificial intelligence that does not seem to be justified by its training data.<ref name=\"2022 survey\">{{cite journal |last1=Ji |first1=Ziwei |last2=Lee |first2=Nayeon |last3=Frieske |first3=Rita |last4=Yu |first4=Tiezheng |last5=Su |first5=Dan |last6=Xu |first6=Yan |last7=Ishii |first7=Etsuko |last8=Bang |first8=Yejin |last9=Madotto |first9=Andrea |last10=Fung |first10=Pascale |title=Survey of Hallucination in Natural Language Generation |journal=ACM Computing Surveys |date=17 November 2022 |pages=3571730 |doi=10.1145/3571730}}</ref>\n\n==In natural language processing==\nIn [[natural language processing]], a hallucination is often defined as \"generated content that is nonsensical or unfaithful to the provided source content\". Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being \"summarized\". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as [[GPT-3]], an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated in the current response), causing a cascade of possible hallucination as the response grows longer.<ref name=\"2022 survey\"/> By 2022, papers such as the ''[[New York Times]]'' expressed concern that, as adoption of bots based on large [[language models]] continued to grow, unwarranted user confidence in bot output could lead to problems.<ref>{{cite news |last1=Metz |first1=Cade |title=The New Chatbots Could Change the World. Can You Trust Them? |url=https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html |access-date=30 December 2022 |work=The New York Times |date=10 December 2022}}</ref>\n\nIn August 2022, [[Meta Platforms|Meta]] warned during its release of BlenderBot 3 that the system was prone to \"hallucinations\", which Meta defined as \"confident statements that are not true\".<ref>{{cite news |title=Meta warns its new chatbot may forget that it's a bot |url=https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ |access-date=30 December 2022 |work=ZDNET |date=2022 |language=en}}</ref> On 15 November 2022, Meta unveiled a demo of Galactica, designed to \"store, combine and reason about scientific knowledge\". Content generated by Galactica came with the warning \"Outputs may be unreliable! Language Models are prone to hallucinate text.\" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.<ref>{{cite news |last1=Edwards |first1=Benj |title=New Meta AI demo writes racist and inaccurate scientific literature, gets pulled |url=https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/ |access-date=30 December 2022 |work=Ars Technica |date=18 November 2022 |language=en-us}}</ref><ref>{{cite web |title=Michael Black |url=https://twitter.com/Michael_J_Black/status/1593133722316189696 |website=Twitter |access-date=30 December 2022 |language=en}}</ref>\n\n[[OpenAI]] [[ChatGPT]] (2022) is based on the GPT-3.5 family of large language models. Professor Ethan Mollick of [[Wharton School of the University of Pennsylvania|Wharton]] has called ChatGPT an \"omniscient, eager-to-please intern who sometimes lies to you\". Data scientist Teresa Kubacka has recounted deliberately making up the phrase \"cycloidal inverted electromagnon\" and testing ChatGPT by asking ChatGPT about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as [[Oren Etzioni]] have joined Kubacka in assessing that such software can often give you \"a very impressive-sounding answer that's just dead wrong\".<ref>{{cite news |last1=Bowman |first1=Emma |title=A new AI chatbot might do your homework for you. But it's still not an A+ student |url=https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia |access-date=29 December 2022 |work=NPR |date=19 December 2022 |language=en}}</ref>\n\nAs an example, when asked about \"[[Harold Coward]]'s idea of dynamic canonicity\", ChatGPT fabricated that Coward wrote a book titled \"Dynamic Canoicity: A Model for Biblical and Theological Interpretation\" arguing that religious principles are actually in a constant a state of change. When pressed, ChatGPT continued to insist that the book was real.<ref>{{cite news |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |access-date=29 December 2022 |work=Ars Technica |date=1 December 2022 |language=en-us}}</ref><ref>{{cite web |title=@michael_nielsen@mastodon.social |url=https://twitter.com/michael_nielsen/status/1598369104166981632 |website=Twitter |access-date=29 December 2022 |language=en}}</ref> Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated \"Some species of dinosaurs even developed primitive forms of art, such as engravings on stones\".<ref>{{cite news |last1=Mollick |first1=Ethan |title=ChatGPT Is a Tipping Point for AI |url=https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai |access-date=29 December 2022 |work=Harvard Business Review |date=14 December 2022}}</ref><ref>{{cite web |title=Ethan Mollick |url=https://twitter.com/emollick/status/1598493794688712707 |website=Twitter |access-date=29 December 2022 |language=en}}</ref> When prompted that \"Scientists have recently discovered [[churros]], the delicious fried-dough pastries... (are) ideal tools for home surgery\", ChatGPT claimed that a \"study published in the journal ''[[Science (journal)|Science]]''\" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.<ref>{{cite news |last1=Kantrowitz |first1=Alex |title=Finally, an A.I. Chatbot That Reliably Passes \u201cthe Nazi Test\u201d |url=https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html |access-date=29 December 2022 |work=Slate Magazine |date=2 December 2022 |language=en}}</ref><ref>{{cite web |last1=Marcus |first1=Gary |title=How come GPT can seem so brilliant one minute and so breathtakingly dumb the next? |url=https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant |website=garymarcus.substack.com |access-date=29 December 2022 |language=en}}</ref> Mike Pearl of ''[[Mashable]]'' tested ChatGPT with multiple questions. In one example, he asked the model for \"the largest country in [[Central America]] that isn't [[Mexico]]\". ChatGPT responded with [[Guatemala]], when the answer is instead [[Nicaragua]].<ref name=\"MashableInfo\">{{Cite web |url=https://mashable.com/article/chatgpt-amazing-wrong |title=The ChatGPT chatbot from OpenAI is amazing, creative, and totally wrong |date=3 December 2022 |last=Pearl |first=Mike |work=[[Mashable]] |access-date=5 December 2022}}</ref> When CNBC asked ChatGPT for the lyrics to \"The Ballad of Dwight Fry\", ChatGPT supplied invented lyrics rather than the actual lyrics.<ref>{{cite news |last1=Pitt |first1=Sofia |title=Google vs. ChatGPT: Here's what happened when I swapped services for a day |url=https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html |access-date=30 December 2022 |work=CNBC |date=2022 |language=en}}</ref> In the process of writing a review for the new iPhone 14 pro, ChatGPT incorrectly volunteered the relevant chipset as the A15 Bionic rather than the A16 Bionic.<ref>{{cite news |title=OpenAI's ChatGPT is scary good at my job, but it can't replace me (yet) |url=https://www.zdnet.com/article/openais-chatgpt-is-scary-good-at-my-job-but-it-cant-replace-me-yet/ |access-date=30 December 2022 |work=ZDNET |date=2022 |language=en}}</ref> Asked questions about [[New Brunswick]], ChatGPT got many answers right but incorrectly classified [[Samantha Bee]] as a \"person from New Brunswick\".<ref>{{cite news |title=We asked an AI questions about New Brunswick. Some of the answers may surprise you |url=https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498 |access-date=30 December 2022 |work=CBC |date=2022}}</ref> Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that \"(strong) magnetic fields of [[black holes]] are generated by the extremely strong gravitational forces in their vicinity\". (In reality, as a consequence of the [[no-hair theorem]], a black hole without an accretion disk is believed to have no magnetic field.)<ref>{{cite news |title=We Asked ChatGPT Your Questions About Astronomy. It Didn't Go so Well. |url=https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well |access-date=31 December 2022 |work=Discover Magazine |date=2022 |language=en}}</ref>\n\n==In other artificial intelligence==\nThe concept of \"hallucination\" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination.<ref name=\"2022 survey\"/> ''[[Wired (magazine)|Wired]]'' noted in 2018 that, despite no recorded attacks \"in the wild\" (that is, outside of proof-of-concept attacks by researchers), there was \"little dispute\" that consumer gadgets, and systems such as automated driving, were susceptible to [[adversarial machine learning|adversarial attacks]] that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as \"evil dot com\"; and an image of two men on skis, that [[Google Cloud Vision]] identified as 91% likely to be \"a dog\".<ref>{{cite news |last1=Simonite |first1=Tom |title=AI Has a Hallucination Problem That's Proving Tough to Fix |url=https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/ |access-date=29 December 2022 |work=Wired |date=2018}}</ref>\n\n==Analysis==\nVarious researchers cited by ''Wired'' have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some \"incorrect\" AI responses classified by humans as \"hallucinations\" are in fact justified by the training data, or even that an AI may be giving the \"correct\" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to.<ref>{{cite news |last1=Matsakis |first1=Louise |title=Artificial Intelligence May Not 'Hallucinate' After All |url=https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/ |access-date=29 December 2022 |work=Wired |date=2019}}</ref>\n\n==See also==\n* [[Anthropomorphism of computers]]\n* [[Artificial stupidity]]\n* [[Misaligned goals in artificial intelligence]]\n\n==References==\n{{Reflist|15em}}\n\n==External links==\n\n{{Authority control}}\n\n[[Category:Artificial intelligence]]"}