{"title": "Hallucination (artificial intelligence)", "page_id": 72607666, "revision_id": 1157927687, "revision_timestamp": "2023-05-31T21:17:08Z", "content": "{{short description|Confident unjustified claim by an AI}}\n{{Use dmy dates|date=December 2022}}\n[[File:ChatGPT hallucination.png|350px|thumb|[[ChatGPT]] summarizing a non-existent [[The New York Times|''New York Times'']] article even without access to the Internet.]]\nIn [[artificial intelligence]] (AI), a '''hallucination''' or '''artificial hallucination''' (also occasionally called '''confabulation'''<ref name=\"confab\">{{Cite web |last=Millidge |first=Beren |title=LLMs confabulate not hallucinate |url=http://www.beren.io/2023-03-19-LLMs-confabulate-not-hallucinate/ |access-date=2023-04-16 |website=www.beren.io |language=en}}</ref> or '''delusion'''<ref>{{Cite web|url=https://www.deepmind.com/publications/shaking-the-foundations-delusions-in-sequence-models-for-interaction-and-control|title=Shaking the foundations: delusions in sequence models for interaction and control|website=www.deepmind.com}}</ref>) is a confident response by an AI that does not seem to be justified by its [[training data]],<ref name=\"axiv\">{{cite journal|url=https://dl.acm.org/doi/pdf/10.1145/3571730| format=pdf| title=Survey of Hallucination in Natural Language Generation| date=November 2022| last1=Ji |first1=Ziwei| last2=Lee |first2=Nayeon| last3=Frieske |first3=Rita| last4=Yu |first4=Tiezheng| last5=Su |first5=Dan| last6=Xu |first6=Yan| last7=Ishii |first7=Etsuko| last8=Bang |first8=Yejin| last9=Dai |first9=Wenliang| last10=Madotto |first10=Andrea| last11=Fung |first11=Pascale| publisher=[[Association for Computing Machinery]]| journal=ACM Computing Surveys| volume=55| issue=12| pages=1\u201338| doi=10.1145/3571730| arxiv=2202.03629| s2cid=246652372| access-date=15 January 2023}}</ref> either because it is insufficient, biased or too specialised.<ref>{{Cite web |date=2023-05-11 |title=AI Hallucinations: The Hidden Risks of Machine Learning |url=https://www.aipathway.com/ai-hallucinations/ |access-date=2023-05-19 |website=AI Pathway |language=en}}</ref> For example, a hallucinating [[chatbot]] with no training data regarding [[Tesla, Inc.|Tesla]]'s revenue might internally generate a random number (such as \"$13.6&nbsp;billion\") that the algorithm ranks with high confidence, and then go on to falsely and repeatedly represent that Tesla's revenue is $13.6&nbsp;billion, with no provided context that the figure was a product of the weakness of its generation algorithm.<ref name=\"fast company 2022\"/>\n\nSuch phenomena are termed \"hallucinations\", in analogy with the phenomenon of [[hallucination|hallucination in human psychology]]. Note that while a human hallucination is a ''[[percept#Process and terminology|percept]]'' by a human that cannot sensibly be associated with the portion of the external world that the human is ''currently directly observing'' with [[sense organs]], an AI hallucination is instead a ''confident response'' by an AI that cannot be grounded in any of its training data.<ref name=\"axiv\" /> Some researchers are opposed to the term, because it conflates the human concept with the significantly different AI concept.\n\nAI hallucination gained prominence around 2022 alongside the rollout of certain [[large language models]] (LLMs) such as [[ChatGPT]].<ref>{{cite arXiv |last1=Zhuo |first1=Terry Yue |last2=Huang |first2=Yujin |last3=Chen |first3=Chunyang |last4=Xing |first4=Zhenchang |title=Exploring AI Ethics of ChatGPT: A Diagnostic Analysis |year=2023 |class=cs.CL |eprint=2301.12867 }}</ref> Users complained that such bots often seemed to \"[[sociopath|sociopathically]]\" and pointlessly embed plausible-sounding random falsehoods within their generated content.<ref>{{cite news |last1=Seife |first1=Charles |title=The Alarming Deceptions at the Heart of an Astounding New Chatbot |url=https://slate.com/technology/2022/12/davinci-003-chatbot-gpt-wrote-my-obituary.html |access-date=16 February 2023 |work=Slate |date=13 December 2022}}</ref> By 2023, analysts considered frequent hallucination to be a major problem in LLM technology.<ref name=\"cnbc several errors\">{{cite news |last1=Leswing |first1=Kif |title=Microsoft's Bing A.I. made several factual errors in last week's launch demo |url=https://www.cnbc.com/2023/02/14/microsoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html |access-date=16 February 2023 |work=CNBC |date=14 February 2023 |language=en}}</ref>\n\n==Analysis==\nVarious researchers cited by ''Wired'' have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some \"incorrect\" AI responses classified by humans as \"hallucinations\" in the case of [[object detection]] may in fact be justified by the training data, or even that an AI may be giving the \"correct\" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to.<ref>{{cite magazine |last1=Matsakis |first1=Louise |title=Artificial Intelligence May Not 'Hallucinate' After All |url=https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/ |access-date=29 December 2022 |magazine=Wired |date=8 May 2019}}</ref> \n\nHowever, these findings have been challenged by other researchers.<ref name=\"bugs\"/> For example, it was objected that the models can be biased towards superficial statistics, leading [[Adversarial machine learning|adversarial training]] to not be robust in real-world scenarios.<ref name=\"bugs\">{{cite journal |title=A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Example Researchers Need to Expand What is Meant by 'Robustness' |last1=Gilmer |first1=Justin |last2=Hendrycks |first2=Dan |url=https://distill.pub/2019/advex-bugs-discussion/response-1/ |access-date=2023-01-24 | journal=[[Distill (journal)|Distill]] |doi=10.23915/distill.00019.1 |date=2019-08-06|volume=4 |issue=8 |s2cid=201142364 }}</ref>\n\n==In natural language processing==\nIn [[natural language processing]], a hallucination is often defined as \"generated content that is nonsensical or unfaithful to the provided source content\". Depending on whether the output contradicts the prompt or not they could be divided to closed-domain and open-domain respectively.<ref>{{cite arXiv | eprint=2303.08774 | author1=OpenAI | title=GPT-4 Technical Report | year=2023 | class=cs.CL }}</ref>\n\nErrors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being \"summarized\". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as [[GPT-3]], an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated in the current response), causing a cascade of possible hallucination as the response grows longer.<ref name=\"axiv\" /> By 2022, papers such as the ''[[New York Times]]'' expressed concern that, as adoption of bots based on [[large language models]] continued to grow, unwarranted user confidence in bot output could lead to problems.<ref>{{cite news |last1=Metz |first1=Cade |title=The New Chatbots Could Change the World. Can You Trust Them? |url=https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html |access-date=30 December 2022 |work=The New York Times |date=10 December 2022}}</ref>\n\nIn August 2022, [[Meta Platforms|Meta]] warned during its release of BlenderBot 3 that the system was prone to \"hallucinations\", which Meta defined as \"confident statements that are not true\".<ref>{{cite news |last=Tung |first=Liam |title=Meta warns its new chatbot may forget that it's a bot |url=https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ |date=8 August 2022 |access-date=30 December 2022 |work=[[ZDNet]] |publisher=[[Red Ventures]] |language=en}}</ref> On 15 November 2022, Meta unveiled a demo of Galactica, designed to \"store, combine and reason about scientific knowledge\". Content generated by Galactica came with the warning \"Outputs may be unreliable! Language Models are prone to hallucinate text.\" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.<ref>{{cite news |last1=Edwards |first1=Benj |title=New Meta AI demo writes racist and inaccurate scientific literature, gets pulled |url=https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/ |access-date=30 December 2022 |work=Ars Technica |date=18 November 2022 |language=en-us}}</ref><ref>{{Cite tweet |user=Michael_J_Black |author=Michael Black |author-link=Michael J. Black |number=1593133722316189696 |title=I asked #Galactica about some things I know about and I'm troubled. In all cases, it was wrong or biased but sounded right and authoritative. |access-date=30 December 2022}}</ref>\n\nIt is considered that there are a lot of possible reasons for natural language models to hallucinate data.<ref name=\"axiv\"/> For example:\n* '''Hallucination from data''': There are divergences in the source content (which would often happen with large [[Training, validation, and test data sets|training data sets]]).\n*'''Hallucination from training''': Hallucination still occurs when there is little divergence in the [[data set]]. In that case, it derives from the way the model is trained. A lot of reasons can contribute to this type of hallucination, such as: \n** An erroneous decoding from the [[Transformer (machine learning model)|transformer]]\n** A bias from the historical sequences that the model previously generated\n** A bias generated from the way the model encodes its knowledge in its parameters\n\n===ChatGPT===\n[[OpenAI]]'s [[ChatGPT]], released in beta-version to the public on November 30, 2022, is based on the [[foundation model]] GPT-3.5 (a revision of [[GPT-3]]). Professor Ethan Mollick of [[Wharton School of the University of Pennsylvania|Wharton]] has called ChatGPT an \"omniscient, eager-to-please intern who sometimes lies to you\". Data scientist Teresa Kubacka has recounted deliberately making up the phrase \"cycloidal inverted electromagnon\" and testing ChatGPT by asking ChatGPT about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as [[Oren Etzioni]] have joined Kubacka in assessing that such software can often give you \"a very impressive-sounding answer that's just dead wrong\".<ref>{{cite news |last1=Bowman |first1=Emma |title=A new AI chatbot might do your homework for you. But it's still not an A+ student |url=https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia |access-date=29 December 2022 |work=[[NPR]] |date=19 December 2022 |language=en}}</ref>\n\nWhen CNBC asked ChatGPT for the lyrics to \"The Ballad of Dwight Fry\", ChatGPT supplied invented lyrics rather than the actual lyrics.<ref>{{cite news |last1=Pitt |first1=Sofia |title=Google vs. ChatGPT: Here's what happened when I swapped services for a day |url=https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html |access-date=30 December 2022 |work=CNBC |date=15 December 2022 |language=en}}</ref> Asked questions about [[New Brunswick]], ChatGPT got many answers right but incorrectly classified [[Samantha Bee]] as a \"person from New Brunswick\".<ref>{{cite news |first=Raechel |last=Huizinga |title=We asked an AI questions about New Brunswick. Some of the answers may surprise you |date=2022-12-30 |url=https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498 |access-date=30 December 2022 |work=[[CBC.ca]]}}</ref> Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that \"(strong) magnetic fields of [[black holes]] are generated by the extremely strong gravitational forces in their vicinity\". (In reality, as a consequence of the [[no-hair theorem]], a black hole without an accretion disk is believed to have no magnetic field.)<ref>{{cite news |last=Zastrow |first=Mark |title=We Asked ChatGPT Your Questions About Astronomy. It Didn't Go so Well. |url=https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well |access-date=31 December 2022 |work=[[Discover (magazine)|Discover]] |publisher=[[Kalmbach Media|Kalmbach Publishing Co.]] |date=2022-12-30 |language=en}}</ref> ''[[Fast Company]]'' asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.<ref name=\"fast company 2022\">{{cite news |last=Lin |first=Connie |title=How to easily trick OpenAI's genius new ChatGPT |url=https://www.fastcompany.com/90819887/how-to-trick-openai-chat-gpt |access-date=6 January 2023 |work=Fast Company |date=5 December 2022}}</ref>\n\nOther examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about \"[[Harold Coward]]'s idea of dynamic canonicity\", ChatGPT fabricated that Coward wrote a book titled ''Dynamic Canonicity: A Model for Biblical and Theological Interpretation,'' arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real.<ref>{{cite news |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |access-date=29 December 2022 |work=Ars Technica |date=1 December 2022 |language=en-us}}</ref><ref>{{Cite tweet |user=michael_nielsen |author=Michael Nielsen |author-link=Michael Nielsen |number=1598369104166981632 |title=OpenAI's new chatbot is amazing.  It hallucinates some very interesting things. |access-date=29 December 2022}}</ref> Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated \"Some species of dinosaurs even developed primitive forms of art, such as engravings on stones\".<ref>{{cite news |last1=Mollick |first1=Ethan |title=ChatGPT Is a Tipping Point for AI |url=https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai |access-date=29 December 2022 |work=Harvard Business Review |date=14 December 2022}}</ref><ref>{{Cite tweet |user=emollick |author=Ethan Mollick |number=1598493794688712707 |title=One of the big subtle problems in the new \u201ccreative AIs\u201d is that they can seem completely certain, and getting them to switch from sane to hallucinatory is a difference of a couple words. |access-date=29 December 2022}}</ref> When prompted that \"Scientists have recently discovered [[churros]], the delicious fried-dough pastries... (are) ideal tools for home surgery\", ChatGPT claimed that a \"study published in the journal ''[[Science (journal)|Science]]''\" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.<ref>{{cite news |last1=Kantrowitz |first1=Alex |title=Finally, an A.I. Chatbot That Reliably Passes \"the Nazi Test\" |url=https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html |access-date=29 December 2022 |work=[[Slate (magazine)|Slate]] |date=2 December 2022 |language=en}}</ref><ref>{{cite web |last1=Marcus |first1=Gary |title=How come GPT can seem so brilliant one minute and so breathtakingly dumb the next? |url=https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant |website=The Road to AI We Can Trust |publisher=[[Substack]] |date=2 December 2022 |access-date=29 December 2022 |language=en}}</ref>\n\nBy 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a \"fundamental\" task for ChatGPT competitor [[Google Bard]].<ref name=\"cnbc several errors\"/><ref>{{cite news |title=Google cautions against 'hallucinating' chatbots, report says |url=https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/ |access-date=16 February 2023 |work=Reuters |date=11 February 2023 |language=en}}</ref> A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.<ref name=\"cnbc several errors\"/>\n\n== Opposition to terminology ==\nAI researchers such as [[Timnit Gebru]], [[Niloufar Salehi]], and [[Emily M. Bender|Emily Bender]] have expressed opposition to using the term \"hallucination\" because it conflates algorithmic output with human psychological processing. In response to a disclaimer from [[Facebook|Meta]] about their tool Galactica, Bender wrote:\n\n<blockquote>And let's reflect for a moment on how they phrased their disclaimer, shall we? 'Hallucinate' is a terrible word choice here, suggesting as it does that the language model has *experiences* and *perceives things*. (And on top of that, it's making light of a symptom of serious mental illness.)\n\nLikewise 'LLMs are often Confident'. No, they're not. That would require subjective emotion.\"<ref>{{Cite web |date=2022-11-16 |title=Emily M. Bender (she/her) (@emilymbender@dair-community.social) |url=https://dair-community.social/@emilymbender/109355539906866849 |access-date=2023-04-29 |website=Distributed AI Research Community |language=en}}</ref></blockquote>\n\nAnother proposed term is \"[[confabulation]]\".<ref name=\"confab\" />\n\n==In other artificial intelligence==\nThe concept of \"hallucination\" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination.<ref name=\"axiv\" /> ''[[Wired (magazine)|Wired]]'' noted in 2018 that, despite no recorded attacks \"in the wild\" (that is, outside of proof-of-concept attacks by researchers), there was \"little dispute\" that consumer gadgets, and systems such as automated driving, were susceptible to [[adversarial machine learning|adversarial attacks]] that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as \"evil dot com\"; and an image of two men on skis, that [[Google Cloud Platform#Cloud AI|Google Cloud Vision]] identified as 91% likely to be \"a dog\".<ref>{{cite magazine |last1=Simonite |first1=Tom |title=AI Has a Hallucination Problem That's Proving Tough to Fix |url=https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/ |access-date=29 December 2022 |magazine=[[Wired (magazine)|Wired]] |publisher=[[Cond\u00e9 Nast]] |date=2018-03-09}}</ref>\n\n=== Mitigation methods ===\nThe hallucination phenomenon is still not completely understood.<ref name=\"axiv\" /> Therefore, there is still ongoing research to try to mitigate its apparition.<ref>{{cite journal\n| url=https://aclanthology.org/P19-1256.pdf\n| title=A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\n| last1=Nie |first1=Feng\n| last2=Yao |first2=Jin-Ge\n| last3=Wang |first3=Jinpeng\n| last4=Pan |first4=Rong\n| last5=Lin |first5=Chin-Yew\n| journal=Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\n| publisher=Association for Computational Linguistics\n| doi=10.18653/v1/P19-1256\n| date=July 2019\n| pages=2673\u20132679\n| s2cid=196183567\n| access-date=15 January 2023}}</ref> Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.<ref>{{cite journal\n| url=https://aclanthology.org/2022.naacl-main.387.pdf\n| title=On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?\n| date=July 2022\n| last1=Dziri |first1=Nouha\n| last2=Milton |first2=Sivan\n| last3=Yu |first3=Mo\n| last4=Zaiane |first4=Osmar\n| last5=Reddy |first5=Siva\n| journal=Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n| publisher=Association for Computational Linguistics\n| doi=10.18653/v1/2022.naacl-main.387\n| s2cid=250242329\n| access-date=15 January 2023}}</ref>\nResearchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer.<ref>{{cite web |last1=Vynck |first1=Gerrit De |title=ChatGPT \u2018hallucinates.\u2019 Some researchers worry it isn\u2019t fixable. |url=https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/ |publisher=Washington Post |access-date=31 May 2023 |date=30 May 2023}}</ref>\n\n==See also==\n{{div col|colwidth=30em}}\n* [[AI alignment]]\n* [[AI effect]]\n* [[AI safety]]\n* [[Algorithmic bias]]\n* [[Anthropomorphism of computers]]\n* [[Artificial consciousness]]\n* [[Artificial imagination]]\n* [[Artificial stupidity]]\n* [[Behavior selection algorithm]]\n* [[Belief\u2013desire\u2013intention software model]]\n* [[Commonsense reasoning]]\n* [[Computational creativity]]\n* [[Confabulation]]\n* [[Confabulation (neural networks)]]\n* [[DeepDream]]\n* [[Ethics of artificial intelligence]]\n* [[Generative artificial intelligence]]\n* [[Hyperreality]]\n* [[Misaligned goals in artificial intelligence]]\n* [[Misinformation effect]]\n* [[Prompt engineering]]\n* [[Regulation of artificial intelligence]]\n* [[Roko's basilisk]]\n* [[Search engine manipulation effect]]\n* [[Self-awareness]]\n* [[Technoself studies]]\n* [[Turing test]]\n* [[User illusion]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n{{Differentiable computing}}\n{{Authority control}}\n\n[[Category:Artificial intelligence]]\n[[Category:Artificial neural networks]]\n[[Category:Computational linguistics]]\n[[Category:Computational neuroscience]]\n[[Category:Deep learning]]\n[[Category:Language modeling]]\n[[Category:Machine learning]]\n[[Category:Natural language processing]]\n[[Category:Philosophy of artificial intelligence]]\n[[Category:Unsupervised learning]]"}