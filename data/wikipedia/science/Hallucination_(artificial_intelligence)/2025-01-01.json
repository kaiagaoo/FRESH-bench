{"title": "Hallucination (artificial intelligence)", "page_id": 72607666, "revision_id": 1265604904, "revision_timestamp": "2024-12-27T19:11:29Z", "content": "{{Short description|Erroneous material generated by AI}}\n{{distinguish|Artificial imagination}}\n{{Use dmy dates|date=December 2022}}\n[[File:Photoreal-train.webm|upright=1.35|thumbtime=6|thumb|A [[Sora (text-to-video model)|Sora]]-generated video of the [[Glenfinnan Viaduct]], incorrectly showing a [[Double-track railway|second track]] whereas the real viaduct has only [[Single-track railway|one]], a second chimney on its interpretation of the train [[The Jacobite (steam train)|''The Jacobite'']], and some carriages much longer than others.]]\nIn the field of [[artificial intelligence]] (AI), a '''hallucination''' or '''artificial hallucination''' (also called '''bullshitting''',<ref>{{Cite web |last=Dolan |first=Eric W. |date=2024-06-09 |title=Scholars: AI isn't \"hallucinating\" -- it's bullshitting |url=https://www.psypost.org/scholars-ai-isnt-hallucinating-its-bullshitting/ |access-date=2024-06-11 |website=PsyPost - Psychology News |language=en-US |archive-date=11 June 2024 |archive-url=https://web.archive.org/web/20240611082425/https://www.psypost.org/scholars-ai-isnt-hallucinating-its-bullshitting/ |url-status=live }}</ref><ref>{{Cite journal |last1=Hicks |first1=Michael Townsen |last2=Humphries |first2=James |last3=Slater |first3=Joe |date=2024-06-08 |title=ChatGPT is bullshit |journal=Ethics and Information Technology |language=en |volume=26 |issue=2 |pages=38 |doi=10.1007/s10676-024-09775-5 |issn=1572-8439|doi-access=free }}</ref> '''confabulation'''<ref name=\"ars making things up\">{{cite news |last1=Edwards |first1=Benj |title=Why ChatGPT and Bing Chat are so good at making things up |url=https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/ |access-date=11 June 2023 |work=Ars Technica |date=6 April 2023 |language=en-us |archive-date=11 June 2023 |archive-url=https://web.archive.org/web/20230611024338/https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/ |url-status=live }}</ref> or '''delusion'''<ref>{{Cite web|url=https://www.deepmind.com/publications/shaking-the-foundations-delusions-in-sequence-models-for-interaction-and-control|title=Shaking the foundations: delusions in sequence models for interaction and control|website=www.deepmind.com|date=22 December 2023|access-date=18 March 2023|archive-date=26 March 2023|archive-url=https://web.archive.org/web/20230326021424/https://www.deepmind.com/publications/shaking-the-foundations-delusions-in-sequence-models-for-interaction-and-control|url-status=live}}</ref>) is a response generated by AI that contains false or [[misleading information]] presented as [[fact]].<ref name=\"Merriam-Webster2023\">{{Cite web |date=2023-10-21 |title=Definition of HALLUCINATION |url=https://www.merriam-webster.com/dictionary/hallucination |access-date=2023-10-29 |website=www.merriam-webster.com |language=en |archive-date=7 October 2023 |archive-url=https://web.archive.org/web/20231007013633/https://www.merriam-webster.com/dictionary/hallucination |url-status=live }}</ref><ref>{{cite conference |author=Joshua Maynez |author2=Shashi Narayan |author3=Bernd Bohnet |author4=Ryan McDonald |year=2020 |title=On Faithfulness and Factuality in Abstractive Summarization |book-title=Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL) (2020) |url=https://aclanthology.org/2020.acl-main.173/ |arxiv=2005.00661 |access-date=2023-09-26 |archive-date=26 September 2023 |archive-url=https://web.archive.org/web/20230926103551/https://aclanthology.org/2020.acl-main.173/ |url-status=live }}</ref><ref name=\"axiv\">{{cite journal|url=https://dl.acm.org/doi/pdf/10.1145/3571730|format=pdf|title=Survey of Hallucination in Natural Language Generation|date=November 2022|last1=Ji|first1=Ziwei|last2=Lee|first2=Nayeon|last3=Frieske|first3=Rita|last4=Yu|first4=Tiezheng|last5=Su|first5=Dan|last6=Xu|first6=Yan|last7=Ishii|first7=Etsuko|last8=Bang|first8=Yejin|last9=Dai|first9=Wenliang|last10=Madotto|first10=Andrea|last11=Fung|first11=Pascale|publisher=[[Association for Computing Machinery]]|journal=ACM Computing Surveys|volume=55|issue=12|pages=1\u201338|doi=10.1145/3571730|arxiv=2202.03629|s2cid=246652372|access-date=15 January 2023|archive-date=26 March 2023|archive-url=https://web.archive.org/web/20230326145635/https://dl.acm.org/doi/pdf/10.1145/3571730|url-status=live}}</ref> This term draws a loose analogy with human psychology, where hallucination typically involves false ''[[percept#Process and terminology|percept]]s''. However, there is a key difference: AI hallucination is associated with erroneous responses rather than perceptual experiences.<ref name=\"axiv\" />\n\nFor example, a [[chatbot]] powered by [[large language model]]s (LLMs), like [[ChatGPT]], may embed plausible-sounding random falsehoods within its generated content. Researchers have recognized this issue, and by 2023, analysts estimated that chatbots hallucinate as much as 27% of the time,<ref name=\"nyt\"/> with factual errors present in 46% of generated texts.<ref name=\"de Wynter-2023\"/> Detecting and mitigating these hallucinations pose significant challenges for practical deployment and reliability of LLMs in real-world scenarios.<ref name=\"cnbc several errors\" /><ref name=\"nyt\">{{Cite news |last1=Metz |first1=Cade |date=6 November 2023 |title=Chatbots May 'Hallucinate' More Often Than Many Realize |url=https://www.nytimes.com/2023/11/06/technology/chatbots-hallucination-rates.html |work=The New York Times |access-date=6 November 2023 |archive-date=7 December 2023 |archive-url=https://web.archive.org/web/20231207081252/https://www.nytimes.com/2023/11/06/technology/chatbots-hallucination-rates.html |url-status=live }}</ref><ref name=\"de Wynter-2023\">{{Cite journal |last1=de Wynter |first1=Adrian |last2=Wang |first2=Xun |last3=Sokolov |first3=Alex |last4=Gu |first4=Qilong |last5=Chen |first5=Si-Qing |date=2023-07-13 |title=An evaluation on large language model outputs: Discourse and memorization |journal=Natural Language Processing Journal |volume=4 |pages= |arxiv=2304.08637 |doi=10.1016/j.nlp.2023.100024 |issn=2949-7191 |doi-access=free}}</ref> Some researchers believe the specific term \"AI hallucination\" unreasonably anthropomorphizes computers.<ref name=\"ars making things up\" />\n\n== Term ==\n\n=== Origin ===\n\nIn 1995, Stephen Thaler introduced the concept of \"virtual input phenomena\" in the context of neural networks and artificial intelligence.<ref>{{cite journal | last1 = Thaler | first1 = Stephen | title = Virtual input phenomena within the death of a simple pattern associator | journal = Neural Networks | volume = 8 | issue = 1 | pages = 55\u20136 | date = December 1995 | doi = 10.1016/0893-6080(94)00065-T\n}}</ref>  This idea is closely tied to his work on the Creativity Machine.<ref>{{cite book | last1 = Thaler | first1 = Stephen | chapter = The Creativity Machine Paradigm | date = January 2013 | doi = 10.1007/978-1-4614-3858-8_396 | title = Encyclopedia of Creativity, Invention, Innovation and Entrepreneurship | pages = 447\u2013456 | publisher = Springer Science+Business Media, LLC | isbn = 978-1-4614-3857-1 | editor1-first = Elias G. | editor1-last = Carayannis}}\n</ref> Virtual input phenomena refer to the spontaneous generation of new ideas or concepts within a neural network, akin to hallucinations, without explicit external inputs. Thaler's key work on this topic is encapsulated in his U.S. patent \"Device for the Autonomous Generation of Useful Information\" (Patent No. US 5,659,666), granted in 1997. This patent describes a neural network system that can autonomously generate new information by simulating virtual inputs. The system effectively \"imagines\" new data, due to a variety of transient and permanent network disturbances, leading to innovative and creative outputs.\n\nThis concept is crucial in understanding how neural networks can be designed to exhibit creative behaviors, producing results that go beyond their initial training data and mimic aspects of human creativity and cognitive processes.\n\nIn the early 2000s, the term \"hallucination\" was used in [[computer vision]]  with a positive connotation to describe the process of adding detail to an image. For example, the task of generating high-resolution face images from low-resolution inputs is called [[face hallucination]].<ref name=\"arxiv.org2024\">{{Cite web |title=AI Hallucinations: A Misnomer Worth Clarifying |url=https://arxiv.org/html/2401.06796v1 |access-date=2024-04-02 |website=arxiv.org |archive-date=2 April 2024 |archive-url=https://web.archive.org/web/20240402203628/https://arxiv.org/html/2401.06796v1 |url-status=live }}</ref><ref>{{Cite web |title=Face Hallucination |url=https://people.csail.mit.edu/celiu/FaceHallucination/fh.html |access-date=2024-04-02 |website=people.csail.mit.edu |archive-date=30 March 2024 |archive-url=https://web.archive.org/web/20240330074041/https://people.csail.mit.edu/celiu/FaceHallucination/fh.html |url-status=live }}</ref>\n\nIn the late 2010s, the term underwent a [[semantic shift]] to signify the generation of factually incorrect or misleading outputs by AI systems in tasks like translation or [[object detection]].<ref name=\"arxiv.org2024\" /> For example, in 2017, Google researchers used the term to describe the responses generated by neural machine translation (NMT) models when they are not related to the source text,<ref>{{Cite web |title=Hallucinations in Neural Machine Translation |url=https://research.google/pubs/hallucinations-in-neural-machine-translation/ |access-date=2024-04-02 |website=research.google |archive-date=2 April 2024 |archive-url=https://web.archive.org/web/20240402084731/https://research.google/pubs/hallucinations-in-neural-machine-translation/ |url-status=live }}</ref> and in 2018, the term was used in computer vision to describe instances where non-existent objects are erroneously detected because of adversarial attacks.<ref name=\"Simonite2018\" />\n\nThe term \"hallucinations\" in AI gained wider recognition during the [[AI boom]], alongside the rollout of widely used chatbots based on large language models (LLMs).<ref>{{cite arXiv |eprint=2301.12867 |class=cs.CL |first1=Terry Yue |last1=Zhuo |first2=Yujin |last2=Huang |title=Exploring AI Ethics of ChatGPT: A Diagnostic Analysis |last3=Chen |first3=Chunyang |last4=Xing |first4=Zhenchang |year=2023}}</ref> In July 2021, [[Meta Platforms|Meta]] warned during its release of BlenderBot 2 that the system is prone to \"hallucinations\", which Meta defined as \"confident statements that are not true\".<ref>{{Cite web |title=Blender Bot 2.0: An open source chatbot that builds long-term memory and searches the internet |url=https://ai.meta.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/ |access-date=2024-03-02 |website=ai.meta.com |language=en}}</ref><ref>{{cite news |last=Tung |first=Liam |date=8 August 2022 |title=Meta warns its new chatbot may forget that it's a bot |url=https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ |access-date=30 December 2022 |publisher=[[ZDNET]] |language=en |archive-date=26 March 2023 |archive-url=https://web.archive.org/web/20230326022031/https://www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ |url-status=live }}</ref> Following [[OpenAI]]'s [[ChatGPT]] release in beta-version in November 2022, some users complained that such chatbots often seem to pointlessly embed plausible-sounding random falsehoods within their generated content.<ref>{{cite news |last1=Seife |first1=Charles |date=13 December 2022 |title=The Alarming Deceptions at the Heart of an Astounding New Chatbot |url=https://slate.com/technology/2022/12/davinci-003-chatbot-gpt-wrote-my-obituary.html |access-date=16 February 2023 |work=Slate |archive-date=26 March 2023 |archive-url=https://web.archive.org/web/20230326021521/https://slate.com/technology/2022/12/davinci-003-chatbot-gpt-wrote-my-obituary.html |url-status=live }}</ref> Many news outlets, including ''[[The New York Times]]'', started to use the term \"hallucinations\" to describe these model's occasionally incorrect or inconsistent responses.<ref>{{Cite journal|language=en-US|first1=Karen|last1=Weise|first2=Cade|last2=Metz|title=When A.I. Chatbots Hallucinate|periodical=The New York Times|date=2023-05-01|issn=0362-4331|url=https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html|access-date=2023-05-08|archive-date=4 April 2024|archive-url=https://web.archive.org/web/20240404080348/https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html|url-status=live}}</ref>\n\nIn 2023, some dictionaries updated their definition of hallucination to include this new meaning specific to the field of AI.<ref name=\"Merriam-Webster2023\" /><ref>{{Cite web|url=https://www.theguardian.com/books/2023/nov/15/hallucinate-cambridge-dictionary-word-of-the-year|title='Hallucinate' chosen as Cambridge dictionary's word of the year|date=2023-11-15|access-date=2024-06-07|website=The Guardian|last=Creamer|first=Ella}}</ref>\n\n=== Criticism ===\nThe term \"hallucination\" has been criticized by [[Usama Fayyad]], executive director of the Institute for Experimental Artificial Intelligence at [[Northeastern University]], on the grounds that it misleadingly personifies large language models, and that it is vague.<ref>{{Cite web |last=Stening |first=Tanner |date=2023-11-10 |title=What are AI chatbots actually doing when they 'hallucinate'? Here's why experts don't like the term |url=https://news.northeastern.edu/2023/11/10/ai-chatbot-hallucinations/ |access-date=2024-06-14 |website=Northeastern Global News |language=en-US}}</ref>\n\n==In natural language processing==\n\n[[File:ChatPGTLojbanLion123.png|upright=1.3|thumb|A translation on the [[Vicuna LLM]] [[test bed]] of English into the [[constructed language]] [[Lojban]], and then back into English in a new round, generates a [[surrealism|surreal]] artifact from Genesis 1:6 ([[Revised Standard Version|RSV]]).]]\nIn [[natural language processing]], a hallucination is often defined as \"generated content that appears factual but is ungrounded\".<ref>{{cite arXiv |last1=Tonmoy |first1=S. M. Towhidul Islam |title=A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models |date=2024-01-08 |eprint=2401.01313 |last2=Zaman |first2=S. M. Mehedi |last3=Jain |first3=Vinija |last4=Rani |first4=Anku |last5=Rawte |first5=Vipula |last6=Chadha |first6=Aman |last7=Das |first7=Amitava|class=cs.CL }}</ref> There are different ways to categorize hallucinations.  Depending on whether the output contradicts the source or cannot be verified from the source, they are divided into intrinsic and extrinsic, respectively.<ref name=\"axiv\" /> Depending on whether the output contradicts the prompt or not they could be divided into closed-domain and open-domain respectively.<ref>{{cite arXiv | eprint=2303.08774 | author1=OpenAI | title=GPT-4 Technical Report | year=2023 | class=cs.CL }}</ref>\n\n===Causes===\nThere are several reasons for natural language models to hallucinate data.<ref name=\"axiv\"/>\n\n====Hallucination from data====\nThe main cause of hallucination from data is source-reference divergence. This divergence happens 1) as an artifact of heuristic data collection or 2) due to the nature of some [[Natural language generation|NLG]] tasks that inevitably contain such divergence. When a model is trained on data with source-reference (target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source.<ref name=\"axiv\"/>\n\n====Hallucination from modeling====\nHallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood, such as [[GPT-3]], and requires [[active learning (machine learning)|active learning]] (such as [[reinforcement learning from human feedback]]) to be avoided.<ref>{{Cite conference |url=http://proceedings.mlr.press/v75/hanneke18a.html |title=Actively Avoiding Nonsense in Generative Models |last1=Hanneke |first1=Steve |last2=Kalai |first2=Adam Tauman |last3=Kamath |first3=Gautam |last4=Tzamos |first4=Christos |date=2018 |publisher=Proceedings of Machine Learning Research (PMLR) |volume=75 |pages=209\u2013227}}</ref> Other research takes an anthropomorphic perspective and posits hallucinations as arising from a tension between [[novelty]] and usefulness. For instance, [[Teresa Amabile]] and Pratt define human creativity as the production of novel and useful ideas.<ref>{{cite journal |last1=Amabile |first1=Teresa M. |last2=Pratt |first2=Michael G. |title=The dynamic componential model of creativity and innovation in organizations: Making progress, making meaning |journal=Research in Organizational Behavior |year=2016 |volume=36 |pages=157\u2013183 |doi=10.1016/j.riob.2016.10.001|s2cid=44444992 }}</ref> By extension, a focus on novelty in machine creativity can lead to production of original but inaccurate responses, i.e. falsehoods, whereas a focus on usefulness can result in ineffectual rote memorized responses.<ref>{{cite journal |last1=Mukherjee |first1=Anirban |last2=Chang |first2=Hannah H. |title=Managing the Creative Frontier of Generative AI: The Novelty-Usefulness Tradeoff |journal=California Management Review |year=2023 |url=https://cmr.berkeley.edu/2023/07/managing-the-creative-frontier-of-generative-ai-the-novelty-usefulness-tradeoff |access-date=5 January 2024 |archive-date=5 January 2024 |archive-url=https://web.archive.org/web/20240105204458/https://cmr.berkeley.edu/2023/07/managing-the-creative-frontier-of-generative-ai-the-novelty-usefulness-tradeoff/ |url-status=live }}</ref>\n\nErrors in encoding and decoding between text and representations can cause hallucinations. When encoders learn the wrong correlations between different parts of the training data, it could result in an erroneous generation that diverges from the input.\nThe decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation. Second, the design of the decoding strategy itself can contribute to hallucinations. A decoding strategy that improves the generation diversity, such as top-k sampling, is positively correlated with increased hallucination.{{Citation needed|date=March 2024}}\n\nPre-training of models on a large corpus is known to result in the model memorizing knowledge in its parameters, creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer.<ref name=\"axiv\" /> By 2022, papers such as ''[[The New York Times]]'' expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.<ref>{{cite news |last1=Metz |first1=Cade |title=The New Chatbots Could Change the World. Can You Trust Them? |url=https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html |access-date=30 December 2022 |work=The New York Times |date=10 December 2022 |archive-date=17 January 2023 |archive-url=https://web.archive.org/web/20230117124217/https://www.nytimes.com/2022/12/10/technology/ai-chat-bot-chatgpt.html |url-status=live }}</ref>\n\n====Examples====\nOn 15 November 2022, researchers from [[Meta AI]] published Galactica,<ref>{{cite arXiv |last1=Taylor |first1=Ross |title=Galactica: A Large Language Model for Science |date=2022-11-16 |eprint=2211.09085 |last2=Kardas |first2=Marcin |last3=Cucurull |first3=Guillem |last4=Scialom |first4=Thomas |last5=Hartshorn |first5=Anthony |last6=Saravia |first6=Elvis |last7=Poulton |first7=Andrew |last8=Kerkez |first8=Viktor |last9=Stojnic |first9=Robert|class=cs.CL }}</ref> designed to \"store, combine and reason about scientific knowledge\". Content generated by Galactica came with the warning \"Outputs may be unreliable! Language Models are prone to hallucinate text.\" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.<ref>{{cite news |last1=Edwards |first1=Benj |date=18 November 2022 |title=New Meta AI demo writes racist and inaccurate scientific literature, gets pulled |url=https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/ |access-date=30 December 2022 |work=[[Ars Technica]] |language=en-us |archive-date=10 April 2023 |archive-url=https://web.archive.org/web/20230410190326/https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers/ |url-status=live }}</ref> Before the cancellation, researchers were working on Galactica Instruct, which would use [[instruction tuning]] to allow the model to follow instructions to manipulate [[LaTeX]] documents on [[Overleaf]].<ref>{{Cite interview |last=Scialom |first=Thomas |interviewer=swyx & Alessio |title=Llama 2, 3 & 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI |url=https://www.latent.space/p/llama-3 |archive-url=https://web.archive.org/web/20240724180028/https://www.latent.space/p/llama-3 |archive-date=July 24, 2024 |work=Latent Space |date=July 23, 2024}}</ref>\n\n[[OpenAI]]'s [[ChatGPT]], released in beta-version to the public on November 30, 2022, is based on the [[foundation model]] GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of [[Wharton School of the University of Pennsylvania|Wharton]] has called ChatGPT an \"omniscient, eager-to-please intern who sometimes lies to you\". Data scientist Teresa Kubacka has recounted deliberately making up the phrase \"cycloidal inverted electromagnon\" and testing ChatGPT by asking it about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as [[Oren Etzioni]] have joined Kubacka in assessing that such software can often give \"a very impressive-sounding answer that's just dead wrong\".<ref>{{cite news |last1=Bowman |first1=Emma |title=A new AI chatbot might do your homework for you. But it's still not an A+ student |url=https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia |access-date=29 December 2022 |publisher=[[NPR]] |date=19 December 2022 |language=en |archive-date=20 January 2023 |archive-url=https://web.archive.org/web/20230120095239/https://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia/ |url-status=live }}</ref>\n\nWhen [[CNBC]] asked ChatGPT for the lyrics to \"[[Ballad of Dwight Fry]]\", ChatGPT supplied invented lyrics rather than the actual lyrics.<ref>{{cite news |last1=Pitt |first1=Sofia |title=Google vs. ChatGPT: Here's what happened when I swapped services for a day |url=https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html |access-date=30 December 2022 |publisher=[[CNBC]] |date=15 December 2022 |language=en |archive-date=16 January 2023 |archive-url=https://web.archive.org/web/20230116171232/https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html |url-status=live }}</ref> Asked questions about [[New Brunswick]], ChatGPT got many answers right but incorrectly classified [[Samantha Bee]] as a \"person from New Brunswick\".<ref>{{cite news |first=Raechel |last=Huizinga |title=We asked an AI questions about New Brunswick. Some of the answers may surprise you |date=2022-12-30 |url=https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498 |access-date=30 December 2022 |publisher=[[CBC News]] |archive-date=6 January 2023 |archive-url=https://web.archive.org/web/20230106052253/https://www.cbc.ca/news/canada/new-brunswick/ai-question-about-nb-1.6699498 |url-status=live }}</ref> Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that \"(strong) magnetic fields of [[black holes]] are generated by the extremely strong gravitational forces in their vicinity\". (In reality, as a consequence of the [[no-hair theorem]], a black hole without an accretion disk is believed to have no magnetic field.)<ref>{{cite news |last=Zastrow |first=Mark |title=We Asked ChatGPT Your Questions About Astronomy. It Didn't Go so Well. |url=https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well |access-date=31 December 2022 |work=[[Discover (magazine)|Discover]] |date=2022-12-30 |language=en |archive-date=26 March 2023 |archive-url=https://web.archive.org/web/20230326021742/https://www.discovermagazine.com/technology/we-asked-chatgpt-your-questions-about-astronomy-it-didnt-go-so-well |url-status=live }}</ref> ''[[Fast Company]]'' asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.<ref name=\"fast company 2022\">{{cite news |last=Lin |first=Connie |title=How to easily trick OpenAI's genius new ChatGPT |url=https://www.fastcompany.com/90819887/how-to-trick-openai-chat-gpt |access-date=6 January 2023 |work=Fast Company |date=5 December 2022 |archive-date=29 March 2023 |archive-url=https://web.archive.org/web/20230329155859/https://www.fastcompany.com/90819887/how-to-trick-openai-chat-gpt |url-status=live }}</ref>\n\n[[File:ChatGPT hallucination.png|thumb|When prompted to \"summarize an article\" with a fake URL that contains meaningful keywords, even with no Internet connection, the chatbot generates a response that seems valid at first glance.]]\nOther examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about \"[[Harold Coward]]'s idea of dynamic canonicity\", ChatGPT fabricated that Coward wrote a book titled ''Dynamic Canonicity: A Model for Biblical and Theological Interpretation'', arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real.<ref>{{cite news |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |access-date=29 December 2022 |work=[[Ars Technica]] |date=1 December 2022 |language=en-us |archive-date=15 March 2023 |archive-url=https://web.archive.org/web/20230315222006/https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |url-status=live }}</ref> Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated \"Some species of dinosaurs even developed primitive forms of art, such as engravings on stones\".<ref>{{cite news |last1=Mollick |first1=Ethan |title=ChatGPT Is a Tipping Point for AI |url=https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai |access-date=29 December 2022 |work=Harvard Business Review |date=14 December 2022 |archive-date=11 April 2023 |archive-url=https://web.archive.org/web/20230411151336/https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai |url-status=live }}</ref> When prompted that \"Scientists have recently discovered [[churros]], the delicious fried-dough pastries&nbsp;... (are) ideal tools for home surgery\", ChatGPT claimed that a \"study published in the journal ''[[Science (journal)|Science]]''{{-\"}} found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.<ref>{{cite news |last1=Kantrowitz |first1=Alex |title=Finally, an A.I. Chatbot That Reliably Passes 'the Nazi Test' |url=https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html |access-date=29 December 2022 |work=[[Slate (magazine)|Slate]] |date=2 December 2022 |language=en |archive-date=17 January 2023 |archive-url=https://web.archive.org/web/20230117012901/https://slate.com/technology/2022/12/chatgpt-openai-artificial-intelligence-chatbot-whoa.html |url-status=live }}</ref><ref>{{cite web |last1=Marcus |first1=Gary |title=How come GPT can seem so brilliant one minute and so breathtakingly dumb the next? |url=https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant |website=The Road to AI We Can Trust |publisher=[[Substack]] |date=2 December 2022 |access-date=29 December 2022 |language=en |archive-date=30 December 2022 |archive-url=https://web.archive.org/web/20221230003606/https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant |url-status=live }}</ref>\n\nBy 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a \"fundamental\" task for ChatGPT competitor [[Google Bard]].<ref name=\"cnbc several errors\">{{cite news |last1=Leswing |first1=Kif |date=14 February 2023 |title=Microsoft's Bing A.I. made several factual errors in last week's launch demo |url=https://www.cnbc.com/2023/02/14/microsoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html |access-date=16 February 2023 |work=CNBC |language=en |archive-date=16 February 2023 |archive-url=https://web.archive.org/web/20230216072604/https://www.cnbc.com/2023/02/14/microsoft-bing-ai-made-several-errors-in-launch-demo-last-week-.html |url-status=live }}</ref><ref>{{cite news |title=Google cautions against 'hallucinating' chatbots, report says |url=https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/ |access-date=16 February 2023 |publisher=Reuters |date=11 February 2023 |language=en |archive-date=6 April 2023 |archive-url=https://web.archive.org/web/20230406154650/https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/ |url-status=live }}</ref> A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.<ref name=\"cnbc several errors\"/>\n\nIn May 2023, it was discovered that Stephen Schwartz had submitted six fake case precedents generated by ChatGPT in his [[brief (law)|brief]] to the [[Southern District of New York]] on ''Mata v. Avianca'', a [[personal injury]] case against the airline [[Avianca]]. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered.<ref>{{cite news |last1=Maruf |first1=Ramishah |title=Lawyer apologizes for fake court citations from ChatGPT  |url=https://www.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html |publisher=CNN Business |date=27 May 2023 |language=en}}</ref> In response, [[Brantley Starr]] of the [[Northern District of Texas]] banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:<ref>{{cite news |last1=Brodkin |first1=Jon |title=Federal judge: No AI in my courtroom unless a human verifies its accuracy |url=https://arstechnica.com/tech-policy/2023/05/federal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy/ |work=[[Ars Technica]] |date=31 May 2023 |language=en-us |access-date=26 June 2023 |archive-date=26 June 2023 |archive-url=https://web.archive.org/web/20230626111111/https://arstechnica.com/tech-policy/2023/05/federal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy/ |url-status=live }}</ref><ref>{{cite web |title=Judge Brantley Starr |url=https://www.txnd.uscourts.gov/judge/judge-brantley-starr |publisher=Northern District of Texas {{!}} United States District Court |access-date=26 June 2023 |archive-date=26 June 2023 |archive-url=https://web.archive.org/web/20230626111110/https://www.txnd.uscourts.gov/judge/judge-brantley-starr |url-status=live }}</ref>\n\n{{blockquote|[Generative artificial intelligence] platforms in their current states are prone to hallucinations and [[Algorithmic bias|bias]]. On hallucinations, they make stuff up\u2014even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.\n}}\n\nOn June 23, judge [[P. Kevin Castel]] dismissed the ''Mata'' case and issued a $5,000 fine to Schwartz and another lawyer\u2014who had both continued to stand by the fictitious precedents despite Schwartz's previous claims\u2014for [[bad faith]] conduct. Castel characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as \"gibberish\" and \"[bordering] on nonsensical\".<ref>{{cite news |last1=Brodkin |first1=Jon |title=Lawyers have real bad day in court after citing fake cases made up by ChatGPT |url=https://arstechnica.com/tech-policy/2023/06/lawyers-have-real-bad-day-in-court-after-citing-fake-cases-made-up-by-chatgpt/ |work=[[Ars Technica]] |date=23 June 2023 |language=en-us |access-date=26 June 2023 |archive-date=26 January 2024 |archive-url=https://web.archive.org/web/20240126071502/https://arstechnica.com/tech-policy/2023/06/lawyers-have-real-bad-day-in-court-after-citing-fake-cases-made-up-by-chatgpt/ |url-status=live }}</ref>\n\nIn June 2023, Mark Walters, a [[gun rights]] activist and radio personality, sued OpenAI in a [[Georgia (U.S. state)|Georgia]] state court after ChatGPT mischaracterized a legal [[complaint]] in a manner alleged to be [[defamatory]] against Walters. The complaint in question was brought in May 2023 by the [[Second Amendment Foundation]] against Washington attorney general [[Robert W. Ferguson]] for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of [[embezzlement]] and [[fraud]] while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert [[Eugene Volokh]], OpenAI is likely not shielded against this claim by [[Section 230]], because OpenAI likely \"materially contributed\" to the creation of the defamatory content.<ref>{{cite news |last1=Belanger |first1=Ashley |title=OpenAI faces defamation suit after ChatGPT completely fabricated another lawsuit |url=https://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/ |work=[[Ars Technica]] |date=9 June 2023 |language=en-us |access-date=1 July 2023 |archive-date=1 July 2023 |archive-url=https://web.archive.org/web/20230701023643/https://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/ |url-status=live }}</ref>\n\n==== Scientific research ====\n===== Problems =====\n\nAI models can cause problems in the world of academic and scientific research due to their hallucinations. Specifically, models like ChatGPT have been recorded in multiple cases to cite sources for information that are either not correct or do not exist. A study conducted in the ''[[Cureus|Cureus Journal of Medical Science]]'' showed that out of 178 total references cited by GPT-3, 69 returned an incorrect or nonexistent [[digital object identifier]] (DOI). An additional 28 had no known DOI nor could be located in a [[Google Search|Google search]].<ref name=\"Athaluri2023\">{{Cite journal |last1=Athaluri |first1=Sai Anirudh |last2=Manthena |first2=Sandeep Varma |last3=Kesapragada |first3=V S R Krishna Manoj |last4=Yarlagadda |first4=Vineel |last5=Dave |first5=Tirth |last6=Duddumpudi |first6=Rama Tulasi Siri |date=2023-04-11 |title=Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References |journal=Cureus |volume=15 |issue=4 |pages=e37432 |language=en |doi=10.7759/cureus.37432 |doi-access=free |issn=2168-8184 |pmc=10173677 |pmid=37182055}}</ref>\n\nAnother instance was documented by Jerome Goddard from [[Mississippi State University]]. In an experiment, ChatGPT had provided questionable information about [[tick]]s. Unsure about the validity of the response, they inquired about the source that the information had been gathered from. Upon looking at the source, it was apparent that the DOI and the names of the authors had been hallucinated. Some of the authors were contacted and confirmed that they had no knowledge of the paper's existence whatsoever.<ref name=\"Goddard2023\">{{Cite journal |last=Goddard |first=Jerome |date=2023-06-25 |title=Hallucinations in ChatGPT: A Cautionary Tale for Biomedical Researchers |url=https://doi.org/10.1016/j.amjmed.2023.06.012 |journal=The American Journal of Medicine |volume=136 |issue=11 |pages=1059\u20131060 |doi=10.1016/j.amjmed.2023.06.012 |pmid=37369274 |s2cid=259274217 |issn=0002-9343}}</ref> Goddard says that, \"in [ChatGPT's] current state of development, physicians and biomedical researchers should NOT ask ChatGPT for sources, references, or citations on a particular topic. Or, if they do, all such references should be carefully vetted for accuracy.\"<ref name=\"Goddard2023\" /> The use of these language models is not ready for fields of academic research and that their use should be handled carefully.<ref>{{Cite conference |url=https://aclanthology.org/2023.findings-emnlp.123/ |title=Towards Mitigating Hallucination in Large Language Models via Self-Reflection |last1=Ji |first1=Ziwei |last2=Yu |first2=Tiezheng |last3=Xu |first3=Yan |last4=lee |first4=Nayeon |date=2023 |publisher=EMNLP Findings |access-date=28 January 2024 |archive-date=28 January 2024 |archive-url=https://web.archive.org/web/20240128141829/https://aclanthology.org/2023.findings-emnlp.123/ |url-status=live }}</ref>\n\nOn top of providing incorrect or missing reference material, ChatGPT also has issues with hallucinating the contents of some reference material. A study that analyzed a total of 115 references provided by ChatGPT documented that 47% of them were fabricated. Another 46% cited real references but extracted incorrect information from them. Only the remaining 7% of references were cited correctly and provided accurate information. ChatGPT has also been observed to \"double-down\" on a lot of the incorrect information. When asked about a mistake that may have been hallucinated, sometimes ChatGPT will try to correct itself but other times it will claim the response is correct and provide even more [[Misinformation|misleading information]].<ref>{{Cite journal |last1=Bhattacharyya |first1=Mehul |last2=Miller |first2=Valerie M. |last3=Bhattacharyya |first3=Debjani |last4=Miller |first4=Larry E. |last5=Bhattacharyya |first5=Mehul |last6=Miller |first6=Valerie |last7=Bhattacharyya |first7=Debjani |last8=Miller |first8=Larry E. |date=2023-05-19 |title=High Rates of Fabricated and Inaccurate References in ChatGPT-Generated Medical Content |journal=Cureus |language=en |volume=15 |issue=5 |pages=e39238 |doi=10.7759/cureus.39238 |doi-access=free |issn=2168-8184 |pmc=10277170 |pmid=37337480}}</ref>\n\nThese hallucinated articles generated by [[language model]]s also pose an issue because it is difficult to tell whether an article was generated by an AI. To show this, a group of researchers at the [[Northwestern University|Northwestern University of Chicago]] generated 50 [[Abstract (summary)|abstracts]] based on existing reports and analyzed their originality. Plagiarism detectors gave the generated articles an originality score of 100%, meaning that the information presented appears to be completely original. Other software designed to detect AI generated text was only able to correctly identify these generated articles with an accuracy of 66%. Research scientists had a similar rate of human error, identifying these abstracts at a rate of 68%.<ref>{{Cite journal |last=Else |first=Holly |date=2023-01-12 |title=Abstracts written by ChatGPT fool scientists |url=https://www.nature.com/articles/d41586-023-00056-7 |journal=Nature |language=en |volume=613 |issue=7944 |pages=423 |doi=10.1038/d41586-023-00056-7 |pmid=36635510 |bibcode=2023Natur.613..423E |s2cid=255773668 |access-date=24 October 2023 |archive-date=25 October 2023 |archive-url=https://archive.today/20231025054650/https://www.nature.com/articles/d41586-023-00056-7 |url-status=live }}</ref> From this information, the authors of this study concluded, \"[t]he ethical and acceptable boundaries of ChatGPT's use in scientific writing remain unclear, although some publishers are beginning to lay down policies.\"<ref>{{Cite journal |last1=Gao |first1=Catherine A. |last2=Howard |first2=Frederick M. |last3=Markov |first3=Nikolay S. |last4=Dyer |first4=Emma C. |last5=Ramesh |first5=Siddhi |last6=Luo |first6=Yuan |last7=Pearson |first7=Alexander T. |date=2023-04-26 |title=Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers |journal=npj Digital Medicine |language=en |volume=6 |issue=1 |page=75 |doi=10.1038/s41746-023-00819-6 |issn=2398-6352 |pmc=10133283 |pmid=37100871}}</ref> Because of AI's ability to fabricate research undetected, the use of AI in the field of research will make determining the originality of research more difficult and require new policies regulating its use in the future.\n\nGiven the ability of AI generated language to pass as real scientific research in some cases, AI hallucinations present problems for the application of language models in the Academic and Scientific fields of research due to their ability to be undetectable when presented to real researchers. The high likelihood of returning non-existent reference material and incorrect information may require limitations to be put in place regarding these language models. Some say that rather than hallucinations, these events are more akin to \"fabrications\" and \"falsifications\" and that the use of these language models presents a risk to the integrity of the field as a whole.<ref>{{Cite journal |last=Emsley |first=Robin |date=2023-08-19 |title=ChatGPT: these are not hallucinations \u2013 they're fabrications and falsifications |journal=Schizophrenia |language=en |volume=9 |issue=1 |page=52 |doi=10.1038/s41537-023-00379-4 |issn=2754-6993 |pmc=10439949 |pmid=37598184}}</ref>\n\n===== Benefits =====\nScientists have also found that hallucinations can serve as a valuable tool for scientific discovery, particularly in fields requiring innovative approaches to complex problems. At the [[University of Washington]], [[David Baker (biochemist)|David Baker]]'s lab has used AI hallucinations to design \"ten million brand-new\" proteins that don't occur in nature, leading to roughly 100 patents and the founding of over 20 biotech companies. This work contributed to Baker receiving the 2023 [[Nobel Prize in Chemistry]], although the committee avoided using the \"hallucinations\" language.<ref name=\"nyt-science\">{{cite news |url=https://www.nytimes.com/2024/12/23/science/ai-hallucinations-science.html |last=Broad |first=William J. |date=23 December 2024 |title=How Hallucinatory A.I. Helps Science Dream Up Big Breakthroughs |newspaper=The New York Times}}</ref>\n\nIn medical research and device development, hallucinations have enabled practical innovations. At [[California Institute of Technology]], researchers used hallucinations to design a novel catheter geometry that significantly reduces bacterial contamination. The design features sawtooth-like spikes on the inner walls that prevent bacteria from gaining traction, potentially addressing a global health issue that causes millions of urinary tract infections annually. These scientific application of hallucinations differs fundamentally from chatbot hallucinations, as they are grounded in physical reality and scientific facts rather than ambiguous language or internet data. [[Anima Anandkumar]], a professor at Caltech, emphasizes that these AI models are \"taught physics\" and their outputs must be validated through rigorous testing. In meteorology, scientists use AI to generate thousands of subtle forecast variations, helping identify unexpected factors that can influence extreme weather events.<ref name=\"nyt-science\" />\n\nAt [[Memorial Sloan Kettering Cancer Center]], researchers have applied hallucinatory techniques to enhance blurry medical images, while the [[University of Texas at Austin]] has utilized them to improve robot navigation systems. These applications demonstrate how hallucinations, when properly constrained by scientific methodology, can accelerate the discovery process from years to days or even minutes.<ref name=\"nyt-science\" />\n\n=== Terminologies ===\nIn ''[[Salon (magazine)|Salon]]'', statistician Gary N. Smith argues that LLMs \"do not understand what words mean\" and consequently that the term \"hallucination\" unreasonably anthropomorphizes the machine.<ref>{{cite news |title=An AI that can \"write\" is feeding delusions about how smart artificial intelligence really is |url=https://www.salon.com/2023/01/01/an-ai-that-can-write-is-feeding-delusions-about-how-smart-artificial-intelligence-really-is/ |access-date=11 June 2023 |work=Salon |date=2 January 2023 |language=en |archive-date=5 January 2023 |archive-url=https://web.archive.org/web/20230105065932/https://www.salon.com/2023/01/01/an-ai-that-can-write-is-feeding-delusions-about-how-smart-artificial-intelligence-really-is/ |url-status=live }}</ref> Journalist Benj Edwards, in ''[[Ars Technica]]'', writes that the term \"hallucination\" is controversial, but that some form of metaphor remains necessary; Edwards suggests \"[[confabulation]]\" as an analogy for processes that involve \"creative gap-filling\".<ref name=\"ars making things up\" />\n\nIn the scientific community, some researchers avoid the term \"hallucination\" as potentially misleading. Some see the AI outputs not as illusory but as prospective\u2014having (some chance of being true), similar to early-stage scientific conjectures. The term has also been criticized for its association with psychedelic drug experiences. In July 2024, a White House report on fostering public trust in AI research mentioned hallucinations only in the context of reducing them. Notably, when acknowledging [[David Baker (biochemist)|David Baker]]'s Nobel Prize-winning work with AI-generated proteins, the Nobel committee avoided the term entirely, instead referring to \"imaginative protein creation.\"<ref name=\"nyt-science\" />\n\nA list of uses of the term \"hallucination\", definitions or characterizations in the context of LLMs include:\n\n* \"a tendency to invent facts in moments of uncertainty\" (OpenAI, May 2023)<ref name=\"cnbc new way\">{{cite news |last1=Field |first1=Hayden |title=OpenAI is pursuing a new way to fight A.I. 'hallucinations' |url=https://www.cnbc.com/2023/05/31/openai-is-pursuing-a-new-way-to-fight-ai-hallucinations.html |access-date=11 June 2023 |work=[[CNBC]] |date=31 May 2023 |language=en |archive-date=10 June 2023 |archive-url=https://web.archive.org/web/20230610231908/https://www.cnbc.com/2023/05/31/openai-is-pursuing-a-new-way-to-fight-ai-hallucinations.html |url-status=live }}</ref>\n* \"a model's logical mistakes\" (OpenAI, May 2023)<ref name=\"cnbc new way\"/>\n* \"fabricating information entirely, but behaving as if spouting facts\" ([[CNBC]], May 2023)<ref name=\"cnbc new way\"/>\n* \"making up information\" (''[[The Verge]]'', February 2023)<ref>{{cite news |last1=Vincent |first1=James |title=Google's AI chatbot Bard makes factual error in first demo |url=https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo |access-date=11 June 2023 |work=[[The Verge]] |date=8 February 2023 |archive-date=12 February 2023 |archive-url=https://web.archive.org/web/20230212094317/https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo |url-status=live }}</ref>\n* \"probability distributions\" (in scientific contexts)<ref name=\"nyt-science\" />\n\n==In other artificial intelligence use==\n{{multiple image\n   | direction = vertical\n   | total_width = 275px\n   | footer    = \n   | image1    = Simplified neural network training example.svg\n   | alt1      = \n   | caption1  = The images above demonstrate an example of how an [[artificial neural network]] might make a [[false positive]] result in [[object detection]]. The input image is a simplified example of the training phase, using multiple images that are known to depict [[starfish]] and [[sea urchin]]s, respectively. The starfish match with a ringed texture and a star outline, whereas most sea urchins match with a striped texture and oval shape. However, the instance of a ring textured sea urchin creates a weakly weighted association between them.\n   | image2    = Simplified neural network example.svg\n   | alt2      = \n   | caption2  = Subsequent run of the network on an input image (left):<ref>{{Cite book |last1=Ferrie |first1=C. |last2=Kaiser |first2=S. |year=2019 |title=Neural Networks for Babies |location=Naperville, Illinois |publisher=Sourcebooks Jabberwocky |isbn=978-1492671206 |oclc=1086346753}}</ref> The network correctly detects the starfish. However, the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes. In addition, a shell that was not included in the training gives a weak signal for the oval shape, also resulting in a weak signal for the sea urchin output. These weak signals may result in a false positive result for the presence of a sea urchin although there was none in the input image.\n   \n   In reality, textures and outlines would not be represented by single nodes, but rather by associated weight patterns of multiple nodes.}}\n\nThe concept of \"hallucination\" is applied more broadly than just natural language processing. A confident response from any AI that seems erroneous by the training data can be labeled a hallucination.<ref name=\"axiv\" />\n\n=== Object detection ===\nVarious researchers cited by [[Wired (magazine)|''Wired'']] have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some \"incorrect\" AI responses classified by humans as \"hallucinations\" in the case of [[object detection]] may in fact be justified by the training data, or even that an AI may be giving the \"correct\" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to.<ref>{{cite magazine |last1=Matsakis |first1=Louise |date=8 May 2019 |title=Artificial Intelligence May Not 'Hallucinate' After All |url=https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/ |access-date=29 December 2022 |magazine=Wired |archive-date=26 March 2023 |archive-url=https://web.archive.org/web/20230326022415/https://www.wired.com/story/adversarial-examples-ai-may-not-hallucinate/ |url-status=live }}</ref>\n\n''[[Wired (magazine)|Wired]]'' noted in 2018 that, despite no recorded attacks \"in the wild\" (that is, outside of [[proof-of-concept]] attacks by researchers), there was \"little dispute\" that consumer gadgets, and systems such as [[automated driving]], were susceptible to [[adversarial machine learning|adversarial attacks]] that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as \"evil dot com\"; and an image of two men on skis, that [[Google Cloud Platform#Cloud AI|Google Cloud Vision]] identified as 91% likely to be \"a dog\".<ref name=\"Simonite2018\">{{cite magazine |last1=Simonite |first1=Tom |title=AI Has a Hallucination Problem That's Proving Tough to Fix |url=https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/ |access-date=29 December 2022 |magazine=[[Wired (magazine)|Wired]] |publisher=[[Cond\u00e9 Nast]] |date=2018-03-09 |archive-date=5 April 2023 |archive-url=https://web.archive.org/web/20230405082603/https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/ |url-status=live }}</ref> However, these findings have been challenged by other researchers.<ref name=\"bugs\">{{cite journal |last1=Gilmer |first1=Justin |last2=Hendrycks |first2=Dan |date=2019-08-06 |title=A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Example Researchers Need to Expand What is Meant by 'Robustness' |url=https://distill.pub/2019/advex-bugs-discussion/response-1/ |journal=[[Distill (journal)|Distill]] |volume=4 |issue=8 |doi=10.23915/distill.00019.1 |s2cid=201142364 |access-date=2023-01-24 |doi-access=free |archive-date=15 January 2023 |archive-url=https://web.archive.org/web/20230115044807/https://distill.pub/2019/advex-bugs-discussion/response-1/ |url-status=live }}</ref> For example, it was objected that the models can be biased towards superficial statistics, leading [[Adversarial machine learning|adversarial training]] to not be robust in real-world scenarios.<ref name=\"bugs\" />\n\n=== Text-to-audio generative AI ===\nText-to-audio generative AI {{endash}} more narrowly known as [[Speech synthesis|text-to-speech]] (TTS) synthesis, depending on the modality {{endash}} are known to produce inaccurate and unexpected results.<ref>{{cite arXiv |last1=Zhang |first1=Chenshuang |title=A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI |date=2023-04-02 |eprint=2303.13336 |last2=Zhang |first2=Chaoning |last3=Zheng |first3=Sheng |last4=Zhang |first4=Mengchun |last5=Qamar |first5=Maryam |last6=Bae |first6=Sung-Ho |last7=Kweon |first7=In So|class=cs.SD }}</ref>\n\n=== Text-to-image generative AI ===\nText-to-image models, such as [[Stable Diffusion]], [[Midjourney]] and others, while impressive in their ability to generate images from text descriptions, often produce inaccurate or unexpected results.\n\nOne notable issue is the generation of historically inaccurate images. For instance, [[Gemini (chatbot)|Gemini]] depicted ancient Romans as black individuals<ref>{{cite web |last1=Jonathan |first1=Pageau |title=Google Gemini is a nice image of one of the dangers of AI as we give it more power. Ideology is so thickly overlaid that it skews everything, then doubles down. First image looks about right, but scroll down. |url=https://x.com/PageauJonathan/status/1760253642257334492 |website=Twitter |access-date=14 August 2024 |archive-date=14 August 2024 |archive-url=https://web.archive.org/web/20240814114457/https://x.com/PageauJonathan/status/1760253642257334492 |url-status=live }}</ref> or Nazi German soldiers as people of color,<ref>{{cite news |last1=Robertson |first1=Adi |title=Google apologizes for \"missing the mark\" after Gemini generated racially diverse Nazis |url=https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical |access-date=14 August 2024 |work=The Verge |date=21 February 2024 |language=en |archive-date=21 February 2024 |archive-url=https://web.archive.org/web/20240221232308/https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical |url-status=live }}</ref> causing controversy and leading Google to pause image generation involving people in Gemini.<ref>{{cite web |title=Gemini image generation got it wrong. We'll do better. |url=https://blog.google/products/gemini/gemini-image-generation-issue/ |website=Google |access-date=14 August 2024 |language=en-us |date=23 February 2024 |archive-date=21 April 2024 |archive-url=https://web.archive.org/web/20240421033917/https://blog.google/products/gemini/gemini-image-generation-issue/ |url-status=live }}</ref>\n\n=== Text-to-video generative AI ===\nText-to-video generative models, like [[Sora (text-to-video model)|Sora]], can introduce inaccuracies in generated videos. One example involves the Glenfinnan Viaduct, a famous landmark featured in the ''[[Harry Potter (film series)|Harry Potter]]'' film series. Sora mistakenly added a [[double-track railway|second track]] to the viaduct railway, resulting in an unrealistic depiction.\n\n== Mitigation methods ==\nThe hallucination phenomenon is still not completely understood. Researchers have also proposed that hallucinations are inevitable and are an innate limitation of large language models.<ref>{{cite arXiv |last1=Ji |first1=Ziwei  | last2=Jain | first2=Sanjay | last3=Kankanhalli | first3=Mohan |title=Hallucination is Inevitable: An Innate Limitation of Large Language Models |date=2024 |class=cs.CL |eprint=2401.11817}}</ref> Therefore, there is still ongoing research to try to mitigate its occurrence.<ref>{{cite journal\n| url=https://aclanthology.org/P19-1256.pdf\n| title=A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation\n| last1=Nie\n| first1=Feng\n| last2=Yao\n| first2=Jin-Ge\n| last3=Wang\n| first3=Jinpeng\n| last4=Pan\n| first4=Rong\n| last5=Lin\n| first5=Chin-Yew\n| journal=Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\n| publisher=Association for Computational Linguistics\n| doi=10.18653/v1/P19-1256\n| date=July 2019\n| pages=2673\u20132679\n| s2cid=196183567\n| access-date=15 January 2023\n| archive-date=27 March 2023\n| archive-url=https://web.archive.org/web/20230327222544/https://aclanthology.org/P19-1256.pdf\n| url-status=live\n}}</ref> Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.<ref>{{cite book\n| chapter-url=https://aclanthology.org/2022.naacl-main.387.pdf\n| title=Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n| date=July 2022\n| last1=Dziri\n| first1=Nouha\n| last2=Milton\n| first2=Sivan\n| last3=Yu\n| first3=Mo\n| last4=Zaiane\n| first4=Osmar\n| last5=Reddy\n| first5=Siva\n| chapter=On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?\n| pages=5271\u20135285\n| publisher=Association for Computational Linguistics\n| doi=10.18653/v1/2022.naacl-main.387\n| s2cid=250242329\n| access-date=15 January 2023\n| archive-date=6 April 2023\n| archive-url=https://web.archive.org/web/20230406170020/https://aclanthology.org/2022.naacl-main.387.pdf\n| url-status=live\n}}</ref>\n\nJi et al.<ref>{{cite journal |last1=Ji |first1=Ziwei  | last2=Lee | first2=Nayeon | last3=Frieske | first3=Rita | last4=Yu | first4=Tiezheng | last5=Su | first5=Dan | last6=Xu | first6=Yan | last7=Ishii | first7=Etsuko | last8=Bang | first8=Yejin |last9=Chen | first9=Delong | last10=Chan | first10=Ho Shu |last11=Dai |first11=Wenliang | last12=Madotto |first12=Andrea | last13=Fung | first13=Pascale|title=Survey of Hallucination in Natural Language Generation |journal=ACM Computing Surveys |date=2023 |volume=55 |issue=12 |pages=1\u201338 |doi=10.1145/3571730 |arxiv=2202.03629}}</ref> divide common mitigation method into two categories: <em>data-related methods</em> and <em>modeling and inference methods</em>. Data-related methods include building a faithful dataset, cleaning data automatically and information augmentation by augmenting the inputs with external information. Model and inference methods include changes in the architecture (either modifying the encoder, attention or the decoder in various ways),  changes in the training process, such as using [[reinforcement learning]], along with post-processing methods that can correct hallucinations in the output.\n \nResearchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer.<ref>{{cite news |last1=Vynck |first1=Gerrit De |title=ChatGPT 'hallucinates.' Some researchers worry it isn't fixable. |url=https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/ |newspaper=Washington Post |access-date=31 May 2023 |date=30 May 2023 |archive-date=17 June 2023 |archive-url=https://web.archive.org/web/20230617021157/https://www.washingtonpost.com/technology/2023/05/30/ai-chatbots-chatgpt-bard-trustworthy/ |url-status=live }}</ref> Another approach proposes to actively validate the correctness corresponding to the low-confidence generation of the model using web search results. They have shown that a generated sentence is hallucinated more often when the model has already hallucinated in its previously generated sentences for the input, and they are instructing the model to create a validation question checking the correctness of the information about the selected concept using [[Microsoft Bing|Bing]] search API.<ref>{{cite arXiv |last1=Varshney |first1=Neeraj | last2=Yao | first2=Wenling | last3=Zhang |first3=Hongming | last4=Chen | first4=Jianshu | last5=Yu | first5=Dong|title=A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation |date=2023 |class=cs.CL |eprint=2307.03987}}</ref> An extra layer of [[logic]]-based rules was proposed for the web search mitigation method, by utilizing different ranks of web pages as a knowledge base, which differ in hierarchy.<ref>{{cite encyclopedia |title=Unjustified untrue \"beliefs\": AI hallucinations and justification logics |encyclopedia=Logic, Knowledge, and Tradition: Essays in Honor of Srecko Kova\u010d |last=\u0160ekrst |first=Kristina |editor1-last=Grgi\u0107 |editor1-first=Filip | editor2-first=Kordula | editor2-last=\u015awi\u0119torzecka | editor3-first=Anna | editor3-last=Bro\u017cek |url=https://philarchive.org/archive/EKRUUQ |access-date=Jun 4, 2024}}</ref>\n\nAccording to Luo et al.,<ref name=\"Luo-2024\">{{cite arXiv |last1=Luo |first1=Junliang| last2=Li | first2=Tianyu | last3=Wu | first3=Di | last4=Jenkin | first4=Michael | last5=Liu | first5=Steve | last6=Dudek | first6=Gregory|title=Hallucination Detection and Hallucination Mitigation: An Investigation |date=2024 |class=cs.CL |eprint=2401.08358}}</ref> the previous methods fall into knowledge and retrieval-based approaches which ground LLM responses in factual data using external knowledge sources, such as path grounding.<ref>{{cite arXiv |last1=Dziri |first1=Nouha | last2=Madotto | first2=Andrea | last3=Zaiane | first3=Osmar | last4=Bose | first4=Avishek Joey |title=Neural path hunter: Reducing hallucination in dialogue systems via path grounding |date=2021 |class=cs.CL |eprint=2104.08455}}</ref> Luo et al. also mention training or reference guiding for language models, involving strategies like employing control codes<ref>{{cite encyclopedia | title=Increasing faithfulness in knowledge-grounded dialogue with controllable features | encyclopedia=Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing | last1=Rashkin | first1=Hannah | last2=Reitter | first2=David | last3=Tomar | first3=Gaurav Singh | last4=Das | first4=Dipanjan | date=2021 | url=https://aclanthology.org/2021.acl-long.58.pdf | access-date=4 June 2024 | archive-date=15 May 2024 | archive-url=https://web.archive.org/web/20240515220353/https://aclanthology.org/2021.acl-long.58.pdf | url-status=live }}</ref>  or contrastive learning<ref>{{cite arXiv |last1=Sun |first1=Weiwei| last2=Shi | first2=Zhengliang| last3=Gao | first3=Shen | last4=Ren | first4=Pengjie | last5=de Rijke | first5=Maarten | last6=Ren | first6=Zhaochun|title=Contrastive Learning Reduces Hallucination in Conversations |date=2022 |class=cs.CL |eprint=2212.10400}}</ref> to guide the generation process to differentiate between correct and hallucinated content. Another category is evaluation and mitigation focused on specific hallucination types,<ref name=\"Luo-2024\"/> such as employing methods to evaluate quantity entity in summarization<ref>{{cite encyclopedia | title=Reducing Quantity Hallucinations in Abstractive Summarization | encyclopedia=Findings of the Association for Computational Linguistics: EMNLP 2020 | last1=Zhao | first1=Zheng | last2=Cohen | first2=Shay B | last3=Webber | first3=Cohen Bonnie | date=2020 | url=https://aclanthology.org/2020.findings-emnlp.203.pdf | access-date=4 June 2024 | archive-date=4 June 2024 | archive-url=https://web.archive.org/web/20240604091751/https://aclanthology.org/2020.findings-emnlp.203.pdf | url-status=live }}</ref>  and methods to detect and mitigate self-contradictory statements.<ref>{{cite arXiv |last1=M\u00fcndler |first1=Niels| last2=He | first2=Jingxuan | last3=Jenko | first3=Slobodan | last4=Vechev | first4=Martin |title=Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation |date=2023 |class=cs.CL |eprint=2305.15852}}</ref>\n\n[[Nvidia]] Guardrails, launched in 2023, can be configured to [[Hard coding|hard-code]] certain responses via script instead of leaving them to the LLM.<ref>{{cite news |last1=Leswing |first1=Kif |title=Nvidia has a new way to prevent A.I. chatbots from 'hallucinating' wrong facts |url=https://www.cnbc.com/2023/04/25/nvidia-nemo-guardrails-software-stops-ai-chatbots-from-hallucinating.html |access-date=15 June 2023 |work=CNBC |date=25 April 2023 |language=en}}</ref> Furthermore, numerous tools like SelfCheckGPT<ref>{{cite web |last=Potsawee |title=potsawee/selfcheckgpt |website=[[GitHub]] |date=2024-05-09 |url=https://github.com/potsawee/selfcheckgpt |access-date=2024-05-09 |archive-date=9 May 2024 |archive-url=https://web.archive.org/web/20240509231835/https://github.com/potsawee/selfcheckgpt |url-status=live }}</ref> and Aimon<ref>{{cite web |title=Aimon |date=2024-05-08 |url=https://aimon.ai/ |access-date=2024-05-09 |publisher=aimonlabs |archive-date=8 May 2024 |archive-url=https://web.archive.org/web/20240508193601/https://aimon.ai/ |url-status=live }}</ref> have emerged to aid in the detection of hallucination in offline experimentation and real-time production scenarios.\n\n==See also==\n{{Div col}}\n* [[AI alignment]]\n* [[AI effect]]\n* [[AI safety]]\n* [[Artifact (error)|Artifact]]\n* [[Artificial stupidity]]\n* [[Turing test]]\n* [[Uncanny valley]]\n{{div col end}}\n{{Clear|right}}\n\n==References==\n{{Reflist}}\n\n{{Artificial intelligence navbox}}\n{{Authority control}}\n\n[[Category:Communication of falsehoods]]\n[[Category:Computational linguistics]]\n[[Category:Computational neuroscience]]\n[[Category:Deep learning]]\n[[Category:Disinformation]]\n[[Category:Language modeling]]\n[[Category:Machine learning]]\n[[Category:Misinformation]]\n[[Category:Software bugs]]\n[[Category:Generative artificial intelligence]]\n[[Category:Philosophy of artificial intelligence]]\n[[Category:Unsupervised learning]]"}