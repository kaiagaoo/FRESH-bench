{"title": "Text-to-image model", "page_id": 71701751, "revision_id": 1192246711, "revision_timestamp": "2023-12-28T11:45:50Z", "content": "{{Short description|Machine learning model}}\n[[File:An astronaut riding a horse (Hiroshige) 2022-08-30.png|thumb|An image conditioned on the prompt \"an astronaut riding a horse, by [[Hiroshige]]\", generated by [[Stable Diffusion]], a large-scale text-to-image model released in 2022]]\nA '''text-to-image model''' is a [[machine learning]] model which takes an input [[natural language]] description and produces an image matching that description. \n\nSuch models began to be developed in the mid-2010s during the beginnings of the [[AI spring]], as a result of advances in [[deep learning|deep neural networks]]. In 2022, the output of state of the art text-to-image models, such as OpenAI's [[DALL-E 3]], Google Brain's [[Imagen (Google Brain)|Imagen]], StabilityAI's [[Stable Diffusion]], and [[Midjourney]] began to approach the quality of real photographs and human-drawn art.\n\nText-to-image models generally combine a [[language model]], which transforms the input text into a latent representation, and a [[generative model|generative]] image model, which produces an image conditioned on that representation. The most effective models have generally been trained on massive amounts of image and text data scraped from the web.{{r|imagen-verge}}\n\n==History==\nBefore the rise of [[deep learning]], attempts to build text-to-image models were limited to [[collage]]s by arranging existing component images, such as from a database of [[clip art]].{{r|agnese|zhu-2007}}\n\nThe inverse task, [[image captioning]], was more tractable and a number of image captioning deep learning models came prior to the first text-to-image models.{{r|mansimov-2015}}\n\nThe first modern text-to-image model, alignDRAW, was introduced in 2015 by researchers from the [[University of Toronto]]. alignDRAW extended the previously-introduced DRAW architecture (which used a [[recurrent neural network|recurrent]] [[variational autoencoder]] with an [[attention mechanism]]) to be conditioned on text sequences.{{r|mansimov-2015}} Images generated by alignDRAW were blurry and not photorealistic, but the model was able to generalize to objects not represented in the training data (such as a red school bus), and appropriately handled novel prompts such as \"a stop sign is flying in blue skies\", showing that it was not merely \"memorizing\" data from the training set.{{r|mansimov-2015|reed-2016}}\n\n{{image frame\n|width=268\n|align=right\n|content=[[File:AlignDRAW - Flying stop sign.png|268px]]\n|caption=Eight images generated from the text prompt \"A stop sign is flying in blue skies.\" by AlignDRAW (2015). Enlarged to show detail.<ref>{{Cite journal |last1=Mansimov |first1=Elman |last2=Parisotto |first2=Emilio |last3=Ba |first3=Jimmy Lei |last4=Salakhutdinov |first4=Ruslan |date=February 29, 2016 |title=Generating Images from Captions with Attention |journal=International Conference on Learning Representations|arxiv=1511.02793 }}</ref>\n}}\nIn 2016, Reed, Akata, Yan et al. became the first to use [[generative adversarial network]]s for the text-to-image task.{{r|reed-2016|frolov}} With models trained on narrow, domain-specific datasets, they were able to generate \"visually plausible\" images of birds and flowers from text captions like \"an all black bird with a distinct thick, rounded bill\". A model trained on the more diverse [[COCO (dataset)|COCO]] dataset produced images which were \"from a distance... encouraging\", but which lacked coherence in their details.{{r|reed-2016}} Later systems include VQGAN+CLIP,<ref>{{Cite web |last=Rodriguez |first=Jesus |title=\ud83c\udf05 Edge#229: VQGAN + CLIP |url=https://thesequence.substack.com/p/edge229 |access-date=2022-10-10 |website=thesequence.substack.com |language=en}}</ref> XMC-GAN, and GauGAN2.<ref>{{Cite web |last=Rodriguez |first=Jesus |title=\ud83c\udf86\ud83c\udf06 Edge#231: Text-to-Image Synthesis with GANs |url=https://thesequence.substack.com/p/edge231 |access-date=2022-10-10 |website=thesequence.substack.com |language=en}}</ref>\n\nOne of the first text-to-image models to capture widespread public attention was [[OpenAI]]'s [[DALL-E]], a [[Transformer (machine learning model)|transformer]] system announced in January 2021.{{r|tc-dalle}} A successor capable of generating more complex and realistic images, DALL-E 2, was unveiled in April 2022,{{r|tc-dalle-2}} followed by [[Stable Diffusion]] publicly released in August 2022.<ref>{{Cite web |title=Stable Diffusion Public Release |url=https://stability.ai/blog/stable-diffusion-public-release |access-date=2022-10-27 |website=Stability.Ai |language=en-GB}}</ref> \n\nFollowing other text-to-image models, [[language model]]-powered [[text-to-video]] platforms such as Runway, Make-A-Video,<ref>{{Cite web |last=Kumar |first=Ashish |date=2022-10-03 |title=Meta AI Introduces 'Make-A-Video': An Artificial Intelligence System That Generates Videos From Text |url=https://www.marktechpost.com/2022/10/03/meta-ai-introduces-make-a-video-an-artificial-intelligence-system-that-generates-videos-from-text/ |access-date=2022-10-03 |website=MarkTechPost |language=en-US}}</ref> Imagen Video,<ref>{{Cite web |last=Edwards |first=Benj |date=2022-10-05 |title=Google's newest AI generator creates HD video from text prompts |url=https://arstechnica.com/information-technology/2022/10/googles-newest-ai-generator-creates-hd-video-from-text-prompts/ |access-date=2022-10-25 |website=Ars Technica |language=en-us}}</ref> Midjourney,<ref>{{Cite web |last=Rodriguez |first=Jesus |title=\ud83c\udfa8 Edge#237: What is Midjourney? |url=https://thesequence.substack.com/p/edge237 |access-date=2022-10-26 |website=thesequence.substack.com |language=en}}</ref> and Phenaki<ref>{{Cite web |title=Phenaki |url=https://phenaki.video/?mc_cid=9fee7eeb9d&mc_eid=6e7303fddd#interactive |access-date=2022-10-03 |website=phenaki.video}}</ref> can generate video from text and/or text/image prompts.<ref>{{cite news |last1=Edwards |first1=Benj |title=Runway teases AI-powered text-to-video editing using written prompts |url=https://arstechnica.com/information-technology/2022/09/runway-teases-ai-powered-text-to-video-editing-using-written-prompts/ |access-date=12 September 2022 |publisher=Ars Technica |date=9 September 2022}}</ref>\n\nIn August 2022, it was further shown how a large text-to-image foundation models can be \"personalized\". [[Text-to-image_personalization|Text-to-Image personalization]] allows to teach the model a new concept using a small set of images of a new object that was not included in the training set of the text-to-image foundation model. This is achieved by [[Textual inversion]], namely, finding a new text term that correspond to these images.<gallery widths=\"148\" heights=\"148\" perrow=\"4\" caption=\"Dall-e2 (April 2022) and Dall-3 (September 2023) interpretations of &amp;quot;A stop sign is flying in blue skies&amp;quot;.\">\nFile:Image generator-A stop sign is flying in blue skies-Dall-e2-03.png|link=|Dall-e2 (April 2022)\nFile:Image generator-A stop sign is flying in blue skies-Dall-e2-01.png|link=|Dall-e2 (April 2022)\nFile:Image generator-A stop sign is flying in blue skies-Dall-e3-01.png|link=|Dall-e3 (September 2023)\nFile:Image generator-A stop sign is flying in blue skies-Dall-e3-02.png|link=|Dall-e3 (September 2023)\n</gallery>\n\n==Architecture and training==\n[[Image:State of AI Art Machine Learning Models.svg|thumb|High level architecture showing the state of AI art machine learning models, the larger or more notable models and applications in the AI art landscape, and pertinent relationships and dependencies as a clickable SVG image map]]\n\nText-to-image models have been built using a variety of architectures. The text encoding step may be performed with a [[recurrent neural network]] such as a [[long short-term memory]] (LSTM) network, though [[transformer (machine learning model)|transformer]] models have since become a more popular option. For the image generation step, conditional [[generative adversarial network]]s have been commonly used, with [[diffusion model]]s also becoming a popular option in recent years. Rather than directly training a model to output a high-resolution image conditioned on a text embedding, a popular technique is to train a model to generate low-resolution images, and use one or more auxiliary deep learning models to upscale it, filling in finer details.\n\nText-to-image models are trained on large datasets of (text, image) pairs, often scraped from the web. With their 2022 Imagen model, Google Brain reported positive results from using a [[large language model]] trained separately on a text-only corpus (with its weights subsequently frozen), a departure from the theretofore standard approach.{{r|imagen-paper}}\n\n==Datasets==\n[[File:Captioned image dataset examples.jpg|thumb|Examples of images and captions from three public datasets which are commonly used to train text-to-image models]]\n\nTraining a text-to-image model requires a dataset of images paired with text captions. One dataset commonly used for this purpose is [[COCO (dataset)|COCO]] (Common Objects in Context). Released by Microsoft in 2014, COCO consists of around 123,000 images depicting a diversity of objects, with five captions per image, generated by human annotators. Oxford-120 Flowers and CUB-200 Birds are smaller datasets of around 10,000 images each, restricted to flowers and birds, respectively. It is considered less difficult to train a high-quality text-to-image model with these datasets, because of their narrow range of subject matter.{{r|frolov}}\n\n==Evaluation==\nEvaluating and comparing the quality of text-to-image models is a challenging problem, and involves assessing multiple desirable properties. As with any generative image model, it is desirable that the generated images be realistic (in the sense of appearing as if they could plausibly have come from the training set), and diverse in their style. A desideratum specific to text-to-image models is that generated images semantically align with the text captions used to generate them. A number of schemes have been devised for assessing these qualities, some automated and others based on human judgement.{{r|frolov}}\n\nA common algorithmic metric for assessing image quality and diversity is [[Inception score]] (IS), which is based on the distribution of labels predicted by a pretrained [[Inceptionv3]] image classification model when applied to a sample of images generated by the text-to-image model. The score is increased when the image classification model predicts a single label with high probability, a scheme intended to favour \"distinct\" generated images. Another popular metric is the related [[Fr\u00e9chet inception distance]], which compares the distribution of generated images and real training images, according to features extracted by one of the final layers of a pretrained image classification model.{{r|frolov}}\n\n==Impact and applications==\n{{Excerpt|Artificial intelligence art|Impact and applications|subsections=yes}}\n\n== List of text-to-image models ==\n{| class=\"wikitable sortable\"\n|+\n!Name\n!Release date\n!Developer\n!License\n|-\n|[[DALL-E|DALL-E 3]]\n|September 2023\n|[[OpenAI]]\n|Proprietary\n|-\n|[[DALL-E|DALL-E 2]]\n|April 2022\n|[[OpenAI]]\n|Proprietary\n|-\n|[[DALL-E]] \n|January 2021\n|[[OpenAI]]\n|Proprietary\n|}\n\n==See also==\n* [[Artificial intelligence art]]\n\n==References==\n{{reflist|refs=\n<ref name=\"imagen-paper\">{{cite arXiv\n|title=Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n|date=23 May 2022\n|last1=Saharia|first1=Chitwan |last2=Chan|first2=William |last3=Saxena|first3=Saurabh |last4=Li|first4=Lala |last5=Whang|first5=Jay |last6=Denton|first6=Emily |last7=Kamyar Seyed Ghasemipour|first7=Seyed |last8=Karagol Ayan|first8=Burcu |last9=Sara Mahdavi|first9=S. |last10=Gontijo Lopes|first10=Rapha |last11=Salimans|first11=Tim |last12=Ho|first12=Jonathan |last13=J Fleet|first13=David |last14=Norouzi|first14=Mohammad\n|class=cs.CV\n|eprint=2205.11487\n}}</ref>\n<ref name=\"imagen-verge\">{{cite news |last1=Vincent |first1=James |title=All these images were generated by Google's latest text-to-image AI |url=https://www.theverge.com/2022/5/24/23139297/google-imagen-text-to-image-ai-system-examples-paper |access-date=May 28, 2022 |work=The Verge |publisher=Vox Media |date=May 24, 2022}}</ref>\n<ref name=\"agnese\">{{citation \n|title=A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis\n|date=October 2019\n|last1=Agnese|first1=Jorge|last2=Herrera|first2=Jonathan|last3=Tao|first3=Haicheng|last4=Zhu|first4=Xingquan\n|arxiv=1910.09399\n}}</ref>\n<ref name=\"zhu-2007\">{{cite journal\n|title=A text-to-picture synthesis system for augmenting communication\n|year=2007\n|last1=Zhu|first1=Xiaojin|last2=Goldberg|first2=Andrew B.|last3=Eldawy|first3=Mohamed|last4=Dyer|first4=Charles R.|last5=Strock|first5=Bradley\n|url=https://www.aaai.org/Papers/AAAI/2007/AAAI07-252.pdf\n|journal=AAAI\n|volume=7\n|pages=1590\u20131595\n}}</ref>\n<ref name=\"frolov\">{{cite journal\n|title=Adversarial text-to-image synthesis: A review\n|date=December 2021\n|journal=Neural Networks|volume=144|pages=187\u2013209\n|last1=Frolov|first1=Stanislav|last2=Hinz|first2=Tobias|last3=Raue|first3=Federico|last4=Hees|first4=J\u00f6rn|last5=Dengel|first5=Andreas\n|doi=10.1016/j.neunet.2021.07.019\n|pmid=34500257\n|s2cid=231698782\n|doi-access=free|arxiv=2101.09983}}</ref>\n<ref name=\"reed-2016\">{{cite journal\n|title=Generative Adversarial Text to Image Synthesis\n|date=June 2016\n|last1=Reed|first1=Scott|last2=Akata|first2=Zeynep|last3=Logeswaran|first3=Lajanugen|last4=Schiele|first4=Bernt|last5=Lee|first5=Honglak\n|journal=International Conference on Machine Learning\n|url=http://proceedings.mlr.press/v48/reed16.pdf\n}}</ref>\n<ref name=\"mansimov-2015\">{{cite journal\n|title=Generating Images from Captions with Attention\n|last1=Mansimov|first1=Elman|last2=Parisotto|first2=Emilio|last3=Lei Ba|first3=Jimmy|last4=Salakhutdinov|first4=Ruslan\n|date=November 2015\n|journal=ICLR\n|arxiv=1511.02793 }}</ref>\n<ref name=\"tc-dalle\">{{cite web\n|work=TechCrunch\n|last=Coldewey|first=Devin\n|date=5 January 2021\n|title=OpenAI's DALL-E creates plausible images of literally anything you ask it to\n|url=https://techcrunch.com/2021/01/05/openais-dall-e-creates-plausible-images-of-literally-anything-you-ask-it-to/\n}}</ref>\n<ref name=\"tc-dalle-2\">{{cite web\n|work=TechCrunch\n|last=Coldewey|first=Devin\n|date=6 April 2022\n|title=OpenAI's new DALL-E model draws anything \u2014 but bigger, better and faster than before\n|url=https://techcrunch.com/2022/04/06/openais-new-dall-e-model-draws-anything-but-bigger-better-and-faster-than-before/\n}}</ref>\n}}\n\n[[Category:Text-to-image generation| ]]"}