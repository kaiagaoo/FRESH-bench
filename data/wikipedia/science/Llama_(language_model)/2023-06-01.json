{"title": "Llama (language model)", "page_id": 73306787, "revision_id": 1153093615, "revision_timestamp": "2023-05-04T05:35:26Z", "content": "{{short description|Large language model by Meta AI}}\n{{Distinguish|LaMDA}}\n{{other uses|Llama (disambiguation)}}\n'''LLaMA''' ('''Large Language Model Meta AI''') is a [[large language model]] (LLM) released by [[Meta AI]] in February 2023. A variety of model sizes were trained ranging from 7 billion to 65 billion parameters. LLaMA's developers reported that the 13 billion parameter model's performance on most [[natural language processing|NLP]] benchmarks exceeded that of the much larger [[GPT-3]] (with 175 billion parameters) and that the largest model was competitive with state of the art models such as PaLM and [[Chinchilla AI|Chinchilla]].<ref name=paper/> Whereas the most powerful LLMs have generally been accessible only through limited [[API]]s (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.<ref name=blog/> Within a week of LLaMA's release, its weights were [[internet leak|leaked]] to the public on 4chan via [[BitTorrent]].<ref name=verge-leak/>\n\n==Architecture and training==\n{{Quote box\n| title = \"A conversation between the sun and pluto\"\n| quote = Sun: Hey Pluto, what\u2019s up?<br/>\nPluto: Not much. Just hanging out here at the edge of the solar system.<br/>\nSun: Yeah, I\u2019ve been meaning to talk to you about that. You\u2019re not a planet anymore.<br/>\nPluto: What? Come on, man. That\u2019s not fair.<br/>\nSun: I\u2019m sorry, but it\u2019s true. You just don\u2019t meet the criteria anymore.<br/>\nPluto: This is bulls**t! I\u2019ve been a planet for over 70 years!<br/>\nSun: Things change, Pluto. You\u2019ve just got to accept it.<br/>\nPluto: F**k you, Sun! F**k you and your stupid solar system!<br/>\n| source = \u2013 Output of 65 billion parameter LLaMA model after [[instruction tuning]] given the prompt \"Write a conversation between the sun and pluto\"<ref name=paper/>\n| align = right\n| width = 350px\n}}\nLLaMA uses the [[transformer (machine learning)|transformer]] architecture, the standard architecture for language modelling since 2018. LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process. LLaMA was trained on 1.4 trillion tokens, drawn from publicly available data sources, including:<ref name=paper/>\n* Webpages scraped by [[CommonCrawl]]\n* Open source repositories of source code from [[GitHub]]\n* [[Wikipedia]] in 20 different languages\n* [[Public domain]] books from [[Project Gutenberg]]\n* The [[LaTeX]] source code for scientific papers uploaded to [[ArXiv]]\n* Questions and answers from [[Stack Exchange]] websites\n\n==Release and leak==\nLLaMA was announced on February 23, 2023, via a blog post and a paper describing the [[Training, validation, and test data sets|model's training]], architecture, and performance.<ref name=paper/><ref name=blog/> The code used to train the model was publicly released under the open-source [[GPL 3]] license.<ref name=repo/> Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".<ref name=blog/>\n\nOn March 2, 2023,<ref>{{cite web |title=/g/ - /aicg/ - AI Chatbot General - Technology - 4chan |url=https://archive.is/uPGfc#p91850335 |date=5 Mar 2023}}</ref> a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the [[4chan]] imageboard and subsequently spreading through online AI communities.<ref name=verge-leak/> That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation.<ref name=India-leak>{{cite news |last1=VK |first1=Anirudh |title=Meta's LLaMA Leaked to the Public, Thanks To 4chan |url=https://analyticsindiamag.com/metas-llama-leaked-to-the-public-thanks-to-4chan/ |access-date=17 March 2023 |work=Analytics India Magazine |date=6 March 2023}}</ref><ref name=\"CKing\"/> On March 4, a pull request was opened to add links to [[HuggingFace]] repositories containing the model.<ref>{{cite web |title=Download weights from huggingface to help us save bandwith by Jainam213 \u00b7 Pull Request #109 \u00b7 facebookresearch/llama |url=https://github.com/facebookresearch/llama/pull/109 |website=GitHub |access-date=17 March 2023 |language=en}}</ref><ref name=India-leak/> On March 6, Meta filed [[takedown request]]s to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests.<ref>{{cite news |last1=Cox |first1=Joseph |title=Facebook's Powerful Large Language Model Leaks Online |url=https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama |access-date=17 March 2023 |work=Vice |date=7 March 2023 |language=en}}</ref> On March 20, Meta filed a [[DMCA]] takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.<ref>{{cite web |author1=OpSec Online LLC |title=github/dmca - Notice of Claimed Infringement via Email |url=https://github.com/github/dmca/blob/master/2023/03/2023-03-21-meta.md |publisher=GitHub |access-date=25 March 2023 |date=21 March 2023}}</ref> As of March 25, Facebook has not responded to the pull request containing the magnet link.<ref name=\"CKing\">{{cite web |title=Save bandwidth by using a torrent to distribute more efficiently by ChristopherKing42 \u00b7 Pull Request #73 \u00b7 facebookresearch/llama |url=https://github.com/facebookresearch/llama/pull/73 |website=GitHub |access-date=25 March 2023 |language=en}}</ref>\n\nReactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated [[spamming|spam]]. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments.<ref name=verge-leak/> Multiple commentators, such as [[Simon Willison]], compared LLaMA to [[Stable Diffusion]], a [[text-to-image model]] which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.<ref name=verge-leak/><ref name=willison/>\n==Open Sourcing/Reproduction==\nOn April 17, 2023, Together launched a project named RedPajama to reproduce and distribute an [[Open_source|open source]] version of the LLaMA dataset.<ref name=red-pajama/> The dataset has approximately 1.2 trillion tokens and is publicly available for download.<ref name=red-pajama-download/>\n\n==Applications==\nThe [[Stanford University]] Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of [[instruction tuning]] to acquire capabilities comparable to the OpenAI GPT-3.5 series text-davinci-003 model at a modest cost.<ref>{{cite web |url=https://crfm.stanford.edu/2023/03/13/alpaca.html |title=Alpaca: A Strong, Replicable Instruction-Following Model |date=13 March 2023 |first1=Rohan |last1=Taori |first2=Ishaan |last2=Gulrajani |first3=Tianyi |last3=Zhang |first4=Yann |last4=Dubois |first5=Xuechen |last5=Li |first6=Carlos |last6=Guestrin |first7=Percy |last7=Liang |first8=Tatsunori B. |last8=Hashimoto |website= |publisher=Stanford Center for Research on Foundation Models |access-date=}}</ref><ref>{{Cite Q |Q117202254}}</ref> Multiple open source projects are continuing this work of finetuning LLaMA with Alpaca dataset.<ref name=repo-alpaca/>\n\n==References==\n{{reflist|refs=\n<ref name=blog>{{cite web\n|work=Meta AI\n|title=Introducing LLaMA: A foundational, 65-billion-parameter large language model\n|date=24 February 2023\n|url=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n}}</ref>\n<ref name=paper>{{cite arXiv\n|eprint=2302.13971\n|last1=Touvron\n|first1=Hugo\n|last2=Lavril\n|first2=Thibaut\n|last3=Izacard\n|first3=Gautier\n|last4=Martinet\n|first4=Xavier\n|last5=Lachaux\n|first5=Marie-Anne\n|last6=Lacroix\n|first6=Timoth\u00e9e\n|last7=Rozi\u00e8re\n|first7=Baptiste\n|last8=Goyal\n|first8=Naman\n|last9=Hambro\n|first9=Eric\n|last10=Azhar\n|first10=Faisal\n|last11=Rodriguez\n|first11=Aurelien\n|last12=Joulin\n|first12=Armand\n|last13=Grave\n|first13=Edouard\n|last14=Lample\n|first14=Guillaume\n|title=LLaMA: Open and Efficient Foundation Language Models\n|year=2023\n|class=cs.CL\n}}</ref>\n<ref name=verge-leak>{{cite web\n|work=The Verge\n|title=Meta's powerful AI language model has leaked online \u2014 what happens now?\n|last=Vincent|first=James\n|date=8 March 2023\n|url=https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse\n}}</ref>\n<ref name=repo>{{cite web\n|title=llama\n|work=GitHub\n|access-date=16 March 2023\n|url=https://github.com/facebookresearch/llama\n}}</ref>\n<ref name=willison>{{cite web\n|work=Simon Willison's Weblog\n|last=Willison|first=Simon\n|title=Large language models are having their Stable Diffusion moment\n|date=11 March 2023\n|url=https://simonwillison.net/2023/Mar/11/llama/\n}}</ref> <!-- [[WP:SPS]] by established subject matter expert -->\n<ref name=repo-alpaca>{{cite web\n|title=alpaca-lora\n|work=GitHub\n|access-date=5 April 2023\n|url=https://github.com/tloen/alpaca-lora\n}}</ref>\n<ref name=red-pajama>{{cite web \n|title=RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset\n|url=https://github.com/togethercomputer/RedPajama-Data\n|website=GitHub\n|publisher=Together \n|access-date=4 May 2023}}</ref>\n<ref name=red-pajama-download>{{cite web \n|title=RedPajama-Data-1T\n|url=https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\n|website=Hugging Face\n|publisher=Together \n|access-date=4 May 2023}}</ref>\n}}\n\n[[Category:Large language models]]\n[[Category:Internet leaks]]"}