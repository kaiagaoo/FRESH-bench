{"title": "Llama (language model)", "page_id": 73306787, "revision_id": 1226642154, "revision_timestamp": "2024-05-31T23:47:19Z", "content": "{{short description|Large language model by Meta AI}}\n{{For2|the animal|Llama||Llama (disambiguation)}}\n{{Distinguish|LaMDA}}\n{{Infobox software\n| title = Llama\n| developer = [[Meta AI]]\n| released = {{start date and age|2023|2|24}}\n| latest release version = Llama 3\n| latest release date = {{start date and age|2024|4|18}}\n| repo = {{URL|https://github.com/meta-llama/llama3}}\n| genre = {{ indented plainlist |\n*[[Large language model]]\n*[[Generative pre-trained transformer|GPT]]\n*[[Foundation model]]\n}}\n| programming language = [[Python_(programming_language)|Python]]\n| license = Meta Llama 3 Community License<ref>{{cite web |title=llama3/LICENSE at main \u00b7 meta-llama/llama3 |url=https://github.com/meta-llama/llama3/blob/main/LICENSE |website=GitHub |language=en}}</ref>\n| website = {{URL|https://llama.meta.com}}\n}}\n'''Llama''' (acronym for '''Large Language Model Meta AI''', and formerly stylized as '''LLaMA''') is a family of autoregressive [[large language model|large language models]] released by [[Meta AI]] starting in February 2023.<ref name=\"l1arxiv\" /><ref name=\"blog\" /> The latest version is Llama 3 released in April 2024.<ref name=\"llama3blog\" />\n\nModel weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis.<ref>{{cite web |last1=Malik |first1=Yuvraj |last2=Paul |first2=Katie |title=Meta heats up Big Tech's AI arms race with new language model |url=https://www.reuters.com/technology/meta-launch-ai-language-model-llama-2023-02-24/ |date=25 February 2023 |publisher=Reuters}}</ref><ref name=\"blog\" /> Unauthorized copies of the model were shared via [[BitTorrent]], in response, Meta AI issued [[DMCA]] takedown requests against repositories sharing the link on [[GitHub]].<ref name=verge-leak /><ref name=githubdcma /> Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.<ref>{{cite web |last1=David |first1=Emilia |title=Meta's AI research head wants open source licensing to change |url=https://www.theverge.com/2023/10/30/23935587/meta-generative-ai-models-open-source |website=The Verge |language=en |date=30 October 2023}}</ref><ref name=\"llama2blog\" /> Llama models are trained at different parameter sizes, typically ranging between 7B and 70B.<ref name=\"llama3blog\" /> Originally, Llama was only available as a [[foundation model]].<ref>{{cite web |last1=Peters |first1=Jay |last2=Vincent |first2=James |title=Meta has a new machine learning language model to remind you it does AI too |url=https://www.theverge.com/2023/2/24/23613512/meta-llama-ai-research-large-language-model |website=The Verge |language=en |date=24 February 2023}}</ref> Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.<ref name=\"llama2blog\" />\n\nAlongside the release of Llama 3, [[Meta Platforms|Meta]] added [[virtual assistant]] features to [[Facebook]] and [[WhatsApp]] in select regions, and a standalone website. Both services use a Llama 3 model.<ref>{{cite web |title=Meet Your New Assistant: Meta AI, Built With Llama 3 |url=https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-llama-3/ |website=Meta |date=18 April 2024}}</ref>\n\n== Background ==\nAfter the release of large language model's such as [[GPT-3]], a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities.<ref>{{cite web |title=Examining Emergent Abilities in Large Language Models |url=https://hai.stanford.edu/news/examining-emergent-abilities-large-language-models |website=hai.stanford.edu |language=en |date=13 September 2022}}</ref> The release of [[ChatGPT]], and it's surprise success caused an increase in attention to large language models.<ref>{{cite web |title=The inside story of how ChatGPT was built from the people who made it |url=https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/ |website=MIT Technology Review |language=en}}</ref>\n\nCompared with other responses to ChatGPT, Meta's Chief AI scientist [[Yann LeCun]] stated that large language models are best for aiding with writing.<ref>{{cite web |title=ChatGPT is 'not particularly innovative,' and 'nothing revolutionary', says Meta's chief AI scientist |url=https://www.zdnet.com/article/chatgpt-is-not-particularly-innovative-and-nothing-revolutionary-says-metas-chief-ai-scientist/ |website=ZDNET |language=en}}</ref><ref>{{cite web |last1=Badminton |first1=Nik |title=Meta's Yann LeCun on auto-regressive Large Language Models (LLMs) |url=https://futurist.com/2023/02/13/metas-yann-lecun-thoughts-large-language-models-llms/ |website=Futurist.com |date=13 February 2023}}</ref><ref>{{cite web |title=Yann LeCun on LinkedIn: My unwavering opinion on current (auto-regressive) LLMs |url=https://www.linkedin.com/feed/update/urn:li:activity:7030921081876029443/ |website=www.linkedin.com |language=en}}</ref>\n\n== Initial release ==\nLLaMA was announced on February 24, 2023, via a blog post and a paper describing the [[Training, validation, and test data sets|model's training]], architecture, and performance.<ref name=l1arxiv/><ref name=blog/> The inference code used to run the model was publicly released under the open-source [[GPLv3]] license.<ref name=repo/> Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".<ref name=blog/>\n\nLlama was trained on only publicly available information, and was trained at various different model sizes, with the intention to make it more accessible to different hardware.\n\nMeta AI reported the 13B parameter model performance on most [[natural language processing|NLP]] benchmarks exceeded that of the much larger [[GPT-3]] (with 175B parameters), and the largest 65B model was competitive with state of the art models such as [[PaLM]] and [[Chinchilla AI|Chinchilla]].<ref name=\"l1arxiv\" />\n\n=== Leak ===\nOn March 3, 2023, a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the [[4chan]] imageboard and subsequently spread through online AI communities.<ref name=verge-leak/> That same day, a pull request on the main LLaMA repository was opened, requesting to add the [[magnet link]] to the official documentation.<ref name=India-leak>{{cite news |last1=VK |first1=Anirudh |title=Meta's LLaMA Leaked to the Public, Thanks To 4chan |url=https://analyticsindiamag.com/metas-llama-leaked-to-the-public-thanks-to-4chan/ |access-date=17 March 2023 |work=Analytics India Magazine |date=6 March 2023}}</ref><ref name=\"CKing\">{{cite web |title=Save bandwidth by using a torrent to distribute more efficiently by ChristopherKing42 \u00b7 Pull Request #73 \u00b7 facebookresearch/llama |url=https://github.com/facebookresearch/llama/pull/73 |website=GitHub |access-date=25 March 2023 |language=en}}</ref> On March 4, a pull request was opened to add links to [[HuggingFace]] repositories containing the model.<ref>{{cite web |title=Download weights from hugging face to help us save bandwidth by Jainam213 \u00b7 Pull Request #109 \u00b7 facebookresearch/llama |url=https://github.com/facebookresearch/llama/pull/109 |website=GitHub |access-date=17 March 2023 |language=en}}</ref><ref name=India-leak/> On March 6, Meta filed [[takedown request]]s to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests.<ref>{{cite news |last1=Cox |first1=Joseph |title=Facebook's Powerful Large Language Model Leaks Online |url=https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama |access-date=17 March 2023 |work=Vice |date=7 March 2023 |language=en}}</ref> On March 20, Meta filed a [[DMCA]] takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.<ref name=\"githubdcma\">{{cite web |author1=OpSec Online LLC |title=github/dmca - Notice of Claimed Infringement via Email |url=https://github.com/github/dmca/blob/master/2023/03/2023-03-21-meta.md |publisher=GitHub |access-date=25 March 2023 |date=21 March 2023}}</ref>\n\nReactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated [[spamming|spam]]. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments.<ref name=verge-leak/> Multiple commentators, such as [[Simon Willison]], compared LLaMA to [[Stable Diffusion]], a [[text-to-image model]] which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.<ref name=verge-leak/><ref name=willison/>\n\n== Llama 2 ==\nOn July 18, 2023, in partnership with [[Microsoft]], Meta announced Llama 2, the next generation of Llama. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters.<ref name=\"llama2blog\">{{cite web |title=Meta and Microsoft Introduce the Next Generation of LLaMA |url=https://about.fb.com/news/2023/07/llama-2/ |website=Meta |access-date=21 July 2023 |date=18 July 2023}}</ref> The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.<ref name=\"l2arxiv\">{{cite arXiv|last1=Touvron |first1=Hugo|last2=Martin |first2=Louis |title=LLaMA-2: Open Foundation and Fine-Tuned Chat Models|date=18 Jul 2023|eprint=2307.09288|class=cs.CL|display-authors=etal}}</ref> The accompanying preprint<ref name=\"l2arxiv\"/> also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n\nLlama 2 includes foundation models and models fine-tuned for chat. In a further departure from LLaMA, all models are released with weights and are free for many commercial use cases. However, due to some remaining restrictions, Meta's description of LLaMA as [[open source]] has been disputed by the [[Open Source Initiative]] (known for maintaining the [[The Open Source Definition|''Open Source Definition'']]).<ref>{{Cite web |last=Edwards |first=Benj |date=2023-07-18 |title=Meta launches LLaMA-2, a source-available AI model that allows commercial applications [Updated] |url=https://arstechnica.com/information-technology/2023/07/meta-launches-llama-2-an-open-source-ai-model-that-allows-commercial-applications/ |access-date=2023-08-08 |website=Ars Technica |language=en-us}}</ref>\n\nCode Llama is a fine-tune of Llama 2 with code specific datasets. 7B, 13B, and 34B versions were released on August 24, 2023, with the 70B releasing on the January 29, 2024.<ref>{{cite web |title=Introducing Code Llama, a state-of-the-art large language model for coding |url=https://ai.meta.com/blog/code-llama-large-language-model-coding/ |website=ai.meta.com |language=en}}</ref> Starting with the foundation models from Llama 2, Meta AI would train an additional 500B tokens of code datasets, before an additional 20B token of long-context data, creating the Code Llama foundation models. This foundation model was further trained on 5B instruction following token to create the instruct fine-tune. Another foundation model was created for Python code, which trained on 100B tokens of Python-only code, before the long-context data.<ref>{{cite arXiv |last1=Rozi\u00e8re |first1=Baptiste |title=Code Llama: Open Foundation Models for Code |date=2024-01-31 |eprint=2308.12950 |last2=Gehring |first2=Jonas |last3=Gloeckle |first3=Fabian |last4=Sootla |first4=Sten |last5=Gat |first5=Itai |last6=Tan |first6=Xiaoqing Ellen |last7=Adi |first7=Yossi |last8=Liu |first8=Jingyu |last9=Sauvestre |first9=Romain|class=cs.CL }}</ref>\n\n== Llama 3 ==\nOn April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters. The models have been pre-trained on approximately 15 trillion tokens of text gathered from \u201cpublicly available sources\u201d with the instruct models fine-tuned on \u201cpublicly available instruction datasets, as well as over 10M human-annotated examples\". Meta plans on releasing multimodal models, models capable of conversing in multiple languages, and models with larger context windows. A version with 400B+ parameters is currently being trained.<ref name=\"llama3blog\">{{Cite web |date=April 18, 2024 |title=Introducing Meta Llama 3: The most capable openly available LLM to date |url=https://ai.meta.com/blog/meta-llama-3/ |access-date=2024-04-21 |website=ai.meta.com |language=en}}</ref>\n\nMeta AI's testing shows that Llama 3 70B beats [[Gemini_(chatbot)|Gemini]], and [[Claude_(language_model)|Claude]] in most benchmarks.<ref>{{cite web |last1=Wiggers |first1=Kyle |title=Meta releases Llama 3, claims it's among the best open models available |url=https://techcrunch.com/2024/04/18/meta-releases-llama-3-claims-its-among-the-best-open-models-available/ |website=TechCrunch |date=18 April 2024}}</ref><ref>{{cite web |last1=Mann |first1=Tobias |title=Meta debuts third-generation Llama large language model |url=https://www.theregister.com/2024/04/19/meta_debuts_llama3_llm/ |website=www.theregister.com |language=en}}</ref>\n\n== Comparison of models ==\n{| class=\"wikitable sortable\"\n|-\n! Name !! Release date !! Parameters \n!Training cost (petaFLOP-day)!! Context length !! Corpus size !! Commercial viability?\n|-\n|LLaMA\n|February 24, 2023\n|\n*6.7B\n*13B\n*32.5B\n*65.2B\n|6,300<ref name=\":5\">{{Cite web |title=The Falcon has landed in the Hugging Face ecosystem |url=https://huggingface.co/blog/falcon |access-date=2023-06-20 |website=huggingface.co}}</ref>\n|2048\n|1\u20131.4T\n|{{no}}\n|-\n|Llama 2\n|July 18, 2023\n|\n*6.7B\n*13B\n*69B\n|21,000<ref>{{Cite web |title=llama/MODEL_CARD.md at main \u00b7 meta-llama/llama |url=https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md |access-date=2024-05-28 |website=GitHub |language=en}}</ref>\n| rowspan=\"2\" |4096\n| rowspan=\"2\" |2T\n| rowspan=\"3\" {{yes}}\n|-\n|Code Llama\n|August 24, 2023\n|\n*6.7B\n*13B\n*33.7B\n*69B\n|\n|-\n|Llama 3\n|April 18, 2024\n|\n*8B\n*70.6B\n*400B+ (unreleased)\n|100,000<ref>[https://x.com/karpathy/status/1781047292486914189 Andrej Karpathy (Apr 18, 2024), ''The model card has some more interesting info too'']</ref><ref>{{Cite web |title=llama3/MODEL_CARD.md at main \u00b7 meta-llama/llama3 |url=https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md |access-date=2024-05-28 |website=GitHub |language=en}}</ref>\n|8912\n|15T\n|}\n\n== Architecture and training ==\n{{Quote box\n| title = \"A conversation between the sun and pluto\"\n| quote = Sun: Hey Pluto, what\u2019s up?<br/>\nPluto: Not much. Just hanging out here at the edge of the solar system.<br/>\nSun: Yeah, I\u2019ve been meaning to talk to you about that. You\u2019re not a planet anymore.<br/>\nPluto: What? Come on, man. That\u2019s not fair.<br/>\nSun: I\u2019m sorry, but it\u2019s true. You just don\u2019t meet the criteria anymore.<br/>\nPluto: This is bulls**t! I\u2019ve been a planet for over 70 years!<br/>\nSun: Things change, Pluto. You\u2019ve just got to accept it.<br/>\nPluto: F**k you, Sun! F**k you and your stupid solar system!<br/>\n| source = \u2013 Output of 65 billion parameter LLaMA model after [[instruction tuning]] given the prompt \"Write a conversation between the sun and pluto\"<ref name=l1arxiv/>\n| align = right\n| width = 350px\n}}\n\n=== Architecture ===\nLLaMA uses the [[transformer (machine learning)|transformer]] architecture, the standard architecture for language modeling since 2018. \n\nThere are minor architectural differences. Compared to GPT-3, LLaMA \n\n* uses SwiGLU<ref>{{Cite arXiv |last=Shazeer |first=Noam |date=2020-02-01 |title=GLU Variants Improve Transformer  |class=cs.CL |eprint=2104.09864}}</ref> [[activation function]] instead of GeLU;\n* uses rotary positional embeddings<ref>{{Cite arXiv |last1=Su |first1=Jianlin |last2=Lu |first2=Yu |last3=Pan |first3=Shengfeng |last4=Murtadha |first4=Ahmed |last5=Wen |first5=Bo |last6=Liu |first6=Yunfeng |date=2021-04-01 |title=RoFormer: Enhanced Transformer with Rotary Position Embedding  |class=cs.CL |eprint=2104.09864}}</ref> instead of absolute positional embedding;\n* uses root-mean-squared layer-normalization<ref>{{Cite arXiv|last1=Zhang |first1=Biao |last2=Sennrich |first2=Rico |date=2019-10-01 |title=Root Mean Square Layer Normalization |class=cs.LG |eprint=1910.07467}}</ref> instead of standard layer-normalization.<ref>{{Cite arXiv|last1=Lei Ba |first1=Jimmy |last2=Kiros |first2=Jamie Ryan |last3=Hinton |first3=Geoffrey E. |date=2016-07-01 |title=Layer Normalization |class=stat.ML |eprint=1607.06450}}</ref>\n* Increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n\n=== Training datasets ===\nLLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process. \n\n'''LLaMA 1''' foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:<ref name=\"l1arxiv\" />\n* Webpages scraped by [[CommonCrawl]]\n* Open source repositories of source code from [[GitHub]]\n* [[Wikipedia]] in 20 different languages\n* [[Public domain]] books from [[Project Gutenberg]]\n* [[The Pile (dataset)|Books3]] books dataset\n* The [[LaTeX]] source code for scientific papers uploaded to [[ArXiv]]\n* Questions and answers from [[Stack Exchange]] websites\n\nOn April 17, 2023, TogetherAI launched a project named RedPajama to reproduce and distribute an [[open source]] version of the LLaMA dataset.<ref name=red-pajama/> The dataset has approximately 1.2 trillion tokens and is publicly available for download.<ref name=red-pajama-download/>\n\n'''Llama 2''' foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.<ref name=\"l2arxiv\"/> Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n\n'''Llama 3''' consists of mainly English data, with over 5% in over 30 other languages. Its dataset was filtered by a text-quality classifier, and the classifier was trained by text synthesized by Llama 2.<ref name=\"llama3blog\" />\n\n=== Fine-tuning ===\nLlama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 \u2013 Chat models were derived from foundational Llama 2 models. Unlike [[GPT-4]] which increased context length during fine-tuning, Llama 2 and Llama 2 - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. The batch size was 64.\n\nFor [[AI alignment]], human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using [[Reinforcement learning from human feedback]] (RLHF). A major technical contribution is the departure from the exclusive use of [[Proximal Policy Optimization]] (PPO) for RLHF \u2013 a new technique based on [[Rejection sampling]] was used, followed by PPO.\n\nMulti-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).\n\n==Applications==\nThe [[Stanford University]] Institute for [[Human-centered computing|Human-Centered Artificial Intelligence]] (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of [[instruction tuning]] to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost.<ref>{{cite web |url=https://crfm.stanford.edu/2023/03/13/alpaca.html |title=Alpaca: A Strong, Replicable Instruction-Following Model |date=13 March 2023 |first1=Rohan |last1=Taori |first2=Ishaan |last2=Gulrajani |first3=Tianyi |last3=Zhang |first4=Yann |last4=Dubois |first5=Xuechen |last5=Li |first6=Carlos |last6=Guestrin |first7=Percy |last7=Liang |first8=Tatsunori B. |last8=Hashimoto |website= |publisher=Stanford Center for Research on Foundation Models |access-date=}}</ref><ref>{{cite arXiv | eprint=2212.10560 | last1=Wang | first1=Yizhong | last2=Kordi | first2=Yeganeh | last3=Mishra | first3=Swaroop | last4=Liu | first4=Alisa | last5=Smith | first5=Noah A. | last6=Khashabi | first6=Daniel | last7=Hajishirzi | first7=Hannaneh | title=Self-Instruct: Aligning Language Models with Self-Generated Instructions | year=2022 | class=cs.CL }}</ref><ref>{{cite web |title=Stanford CRFM |url=https://crfm.stanford.edu/2023/03/13/alpaca.html |website=crfm.stanford.edu}}</ref> The model files were officially removed on March 21st 2023 over hosting costs and safety concerns, though the code and paper remain online for reference.<ref>{{cite web |last1=Quach |first1=Katyanna |title=Stanford takes costly, risky Alpaca AI model offline |url=https://www.theregister.com/2023/03/21/stanford_ai_alpaca_taken_offline/ |website=www.theregister.com |language=en}}</ref><ref>{{cite web |title=Stanford Researchers Take Down Alpaca AI Over Cost and Hallucinations |url=https://gizmodo.com/stanford-ai-alpaca-llama-facebook-taken-down-chatgpt-1850247570 |website=Gizmodo |language=en |date=21 March 2023}}</ref><ref name=\"repo-alpaca\" />\n\nMeditron is a family of Llama-based finetuned on a corpus of clinical guidelines, [[PubMed]] papers, and articles. It was created by researchers at [[\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne]] School of Computer and Communication Sciences, and the [[Yale School of Medicine]]. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.<ref>{{cite web |title=Meditron: An LLM suite for low-resource medical settings leveraging Meta Llama |url=https://ai.meta.com/blog/llama-2-3-meditron-yale-medicine-epfl-open-source-llm/ |website=ai.meta.com |language=en}}</ref><ref>{{cite web |last1=Petersen |first1=Tanya |title=EPFL's new Large Language Model for Medical Knowledge |url=https://actu.epfl.ch/news/epfl-s-new-large-language-model-for-medical-knowle/ |language=en |date=28 November 2023}}</ref><ref>{{cite web |title=epfLLM/meditron |url=https://github.com/epfLLM/meditron |publisher=epfLLM |date=11 May 2024}}</ref>\n\n[[Zoom (software)|Zoom]] used Meta Llama 2 to create an AI Companion that can summarize meetings, provide helpful presentation tips, and assist with message responses. This AI Companion is powered by multiple models, including Meta Llama 2.<ref>{{cite web |title=How Companies Are Using Meta Llama |url=https://about.fb.com/news/2024/05/how-companies-are-using-meta-llama/ |website=Meta |date=7 May 2024}}</ref>\n\n===llama.cpp===\n{{Main|llama.cpp}}\nSoftware developer Georgi Gerganov released [[llama.cpp]] as open-source on March 10, 2023. It's a re-implementation of LLaMA in [[C++]], allowing systems without a powerful GPU to run the model locally.<ref>{{Cite web |last=Edwards |first=Benj |date=2023-03-13 |title=You can now run a GPT-3-level AI model on your laptop, phone, and Raspberry Pi |url=https://arstechnica.com/information-technology/2023/03/you-can-now-run-a-gpt-3-level-ai-model-on-your-laptop-phone-and-raspberry-pi/ |access-date=2024-01-04 |website=Ars Technica |language=en-us}}</ref> The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata.<ref>{{cite web |title=GGUF |url=https://huggingface.co/docs/hub/gguf |website=huggingface.co |access-date=9 May 2024}}</ref> The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.<ref>{{cite web |last1=Labonne |first1=Maxime |title=Quantize Llama models with GGUF and llama.cpp |url=https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172 |website=Medium |publisher=Towards Data Science |access-date=9 May 2024 |language=en |date=29 November 2023}}</ref>\n\nllamafile created by [[Justine Tunney]] is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et. al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for [[FP16]] and 8-bit quantized data types.<ref name=\"llamafileregister\">{{cite web |last1=Connatser |first1=Matthew |title=Llamafile LLM driver project boosts performance on CPU cores |url=https://www.theregister.com/2024/04/03/llamafile_performance_gains/ |website=www.theregister.com |access-date=10 May 2024 |language=en}}</ref>\n\n== Reception ==\n[[Wired_(magazine)|Wired]] describes the 8B parameter version of Llama 3 as being \"surprisingly capable\" given its size.<ref>{{cite magazine |last1=Knight |first1=Will |title=Meta's Open Source Llama 3 Is Already Nipping at OpenAI's Heels |url=https://www.wired.com/story/metas-open-source-llama-3-nipping-at-openais-heels/ |magazine=Wired}}</ref>\n\nThe response to Meta's integration of Llama into Facebook was mixed, with some users confused after Meta AI told a parental group that it had a child.<ref>{{cite web |title=Meta's amped-up AI agents confusing Facebook users |url=https://www.abc.net.au/news/2024-04-19/meta-releases-llama-3-ai-model/103744538 |website=ABC News |language=en-AU |date=19 April 2024}}</ref>\n\nAccording to the Q4 2023 Earnings transcript, Meta adopted the strategy of open weights to improve on model safety, iteration speed, increase adoption among developers and researchers, and to become the industry standard. Llama 5, 6, and 7 are planned for the future.<ref>https://s21.q4cdn.com/399680738/files/doc_financials/2023/q4/META-Q4-2023-Earnings-Call-Transcript.pdf</ref>\n\n== See also ==\n* [[Mistral AI]]\n* [[GPT-4o]]\n\n==References==\n{{reflist|refs=\n<ref name=blog>{{cite web\n|work=Meta AI\n|title=Introducing LLaMA: A foundational, 65-billion-parameter large language model\n|date=24 February 2023\n|url=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n}}</ref>\n<ref name=l1arxiv>{{cite arXiv\n|eprint=2302.13971\n|last1=Touvron\n|first1=Hugo\n|last2=Lavril\n|first2=Thibaut\n|last3=Izacard\n|first3=Gautier\n|last4=Martinet\n|first4=Xavier\n|last5=Lachaux\n|first5=Marie-Anne\n|last6=Lacroix\n|first6=Timoth\u00e9e\n|last7=Rozi\u00e8re\n|first7=Baptiste\n|last8=Goyal\n|first8=Naman\n|last9=Hambro\n|first9=Eric\n|last10=Azhar\n|first10=Faisal\n|last11=Rodriguez\n|first11=Aurelien\n|last12=Joulin\n|first12=Armand\n|last13=Grave\n|first13=Edouard\n|last14=Lample\n|first14=Guillaume\n|title=LLaMA: Open and Efficient Foundation Language Models\n|year=2023\n|class=cs.CL\n}}</ref>\n<ref name=verge-leak>{{cite web\n|work=The Verge\n|title=Meta's powerful AI language model has leaked online \u2014 what happens now?\n|last=Vincent|first=James\n|date=8 March 2023\n|url=https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse\n}}</ref>\n<ref name=repo>{{cite web\n|title=llama\n|work=GitHub\n|access-date=16 March 2023\n|url=https://github.com/facebookresearch/llama\n}}</ref>\n<ref name=willison>{{cite web\n|work=Simon Willison's Weblog\n|last=Willison|first=Simon\n|title=Large language models are having their Stable Diffusion moment\n|date=11 March 2023\n|url=https://simonwillison.net/2023/Mar/11/llama/\n}}</ref> <!-- [[WP:SPS]] by established subject matter expert -->\n<ref name=repo-alpaca>{{cite web\n|title=alpaca-lora\n|work=GitHub\n|access-date=5 April 2023\n|url=https://github.com/tloen/alpaca-lora\n}}</ref>\n<ref name=red-pajama>{{cite web \n|title=RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset\n|url=https://github.com/togethercomputer/RedPajama-Data\n|website=GitHub\n|publisher=Together \n|access-date=4 May 2023}}</ref>\n<ref name=red-pajama-download>{{cite web \n|title=RedPajama-Data-1T\n|url=https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\n|website=Hugging Face\n|publisher=Together \n|access-date=4 May 2023}}</ref>\n}}\n\n== Further reading ==\n{{refbegin}}\n* {{Cite web |last1=Huang |first1=Kalley |last2=O'Regan |first2=Sylvia Varnham |date=September 5, 2023 |title=Inside Meta's AI Drama: Internal Feuds Over Compute Power |url=https://www.theinformation.com/articles/inside-metas-ai-drama-internal-feuds-over-compute-power |url-access=limited |url-status=live |archive-url=https://web.archive.org/web/20230905174145/https://www.theinformation.com/articles/inside-metas-ai-drama-internal-feuds-over-compute-power |archive-date=September 5, 2023 |access-date=September 6, 2023 |website=[[The Information (website)|The Information]]}}\n{{refend}}\n\n[[Category:2023 software]]\n[[Category:Internet leaks]]\n[[Category:Large language models]]\n[[Category:Meta Platforms]]"}