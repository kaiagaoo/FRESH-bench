{"title": "Reinforcement learning from human feedback", "page_id": 73200355, "revision_id": 1263568223, "revision_timestamp": "2024-12-17T11:14:57Z", "content": "{{good article}}\n{{Short description|Machine learning technique}}\n{{Machine learning|Learning with humans}}\nIn [[machine learning]], '''reinforcement learning from human feedback''' ('''RLHF''') is a technique to [[AI alignment|align]] an [[intelligent agent]] with human preferences. It involves training a reward model to represent preferences, which can then be used to train other models through [[reinforcement learning]].\n\nIn classical reinforcement learning, an intelligent agent's goal is to learn a function that guides its behavior, called a [[reinforcement learning#Policy|policy]]. This function is iteratively updated to maximize rewards based on the agent's task performance.<ref>{{cite book |last1=Russell |first1=Stuart J. |last2=Norvig |first2=Peter |title=Artificial intelligence: a modern approach |date=2016 |publisher=Pearson |location=Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo |isbn=978-0-13-604259-4 |pages=830\u2013831 |edition=Third, Global}}</ref> However, explicitly defining a reward function that accurately approximates human preferences is challenging. Therefore, RLHF seeks to train a \"reward model\" directly from human [[feedback]].<ref name=\"ziegler\">{{cite arXiv |last1=Ziegler |first1=Daniel M. |last2=Stiennon |first2=Nisan |last3=Wu |first3=Jeffrey |last4=Brown |first4=Tom B. |last5=Radford |first5=Alec |last6=Amodei |first6=Dario |last7=Christiano |first7=Paul |last8=Irving |first8=Geoffrey |title=Fine-Tuning Language Models from Human Preferences |date=2019 |eprint=1909.08593 |class=cs.CL}}</ref> The reward model is first trained in a [[supervised learning|supervised]] manner to predict if a response to a given prompt is good (high reward) or bad (low reward) based on ranking data collected from human [[labeled data|annotator]]s. This model then serves as a reward function to improve an agent's policy through an [[optimization algorithm]] like [[proximal policy optimization]].<ref name=\"huggingface\">{{cite web |last1=Lambert |first1=Nathan |last2=Castricato |first2=Louis |last3=von Werra |first3=Leandro |last4=Havrilla |first4=Alex |title=Illustrating Reinforcement Learning from Human Feedback (RLHF) |url=https://huggingface.co/blog/rlhf |website=huggingface.co |access-date=4 March 2023}}</ref>\n<ref>{{cite arXiv |last1=Schulman |first1=John |last2=Wolski |first2=Filip |last3=Dhariwal |first3=Prafulla |last4=Radford |first4=Alec |last5=Klimov |first5=Oleg |title=Proximal Policy Optimization Algorithms |date=2017 |eprint=1707.06347 |class=cs.LG}}</ref>\n<ref>{{cite arXiv |last1=Tuan |first1=Yi-Lin |last2=Zhang |first2=Jinzhi |last3=Li |first3=Yujia |last4=Lee |first4=Hung-yi |title=Proximal Policy Optimization and its Dynamic Version for Sequence Generation |date=2018 |eprint=1808.07982 |class=cs.CL}}</ref>\n\nRLHF has applications in various domains in machine learning, including [[natural language processing]] tasks such as [[text summarization]] and [[conversational agents]], [[computer vision]] tasks like [[text-to-image model]]s, and the development of [[video game bot]]s. While RLHF is an effective method of training models to act better in accordance with human preferences, it also faces challenges due to the way the human preference data is collected. Though RLHF does not require massive amounts of data to improve performance, sourcing high-quality preference data is still an expensive process. Furthermore, if the data is not carefully collected from a representative [[sampling (statistics)|sample]], the resulting model may exhibit unwanted [[algorithmic bias|biases]].\n\n[[File:RLHF diagram.svg|thumb|350px|High-level overview of reinforcement learning from human feedback]]\n\n==Background and motivation==\nOptimizing a model based on human feedback is desirable when a task is difficult to specify yet easy to judge.<ref name=\"openai\"/> For example, one may want to train a model to generate [[AI safety|safe]] text that is both helpful and harmless (such as lacking [[algorithmic bias|bias]], toxicity, or otherwise harmful content). Asking humans to manually create examples of harmless and harmful text would be difficult and time-consuming. However, humans are adept at swiftly assessing and comparing the harmfulness of different AI-generated text. Therefore, a more practical objective would be to allow the model to use this type of human feedback to improve its text generation.<ref>{{cite arXiv |last1=Zheng |first1=Rui |last2=Dou |first2=Shihan |last3=Gao |first3=Songyang |last4=Hua |first4=Yuan |last5=Shen |first5=Wei |last6=Wang |first6=Binghai |last7=Liu |first7=Yan |last8=Jin |first8=Senjie |last9=Liu |first9=Qin |last10=Zhou |first10=Yuhao |last11=Xiong |first11=Limao |last12=Chen |first12=Lu |last13=Xi |first13=Zhiheng |last14=Xu |first14=Nuo |last15=Lai |first15=Wenbin |last16=Zhu |first16=Minghao |last17=Chang |first17=Cheng |last18=Yin |first18=Zhangyue |last19=Weng |first19=Rongxiang |last20=Cheng |first20=Wensen |last21=Huang |first21=Haoran |last22=Sun |first22=Tianxiang |last23=Yan |first23=Hang |last24=Gui |first24=Tao |last25=Zhang |first25=Qi |last26=Qiu |first26=Xipeng |last27=Huang |first27=Xuanjing |title=Secrets of RLHF in Large Language Models Part I: PPO |date=2023 |class=cs.CL |eprint=2307.04964}}</ref>\n\nDespite the clear benefits of incorporating human feedback in training models, prior efforts\u2014including some that leverage [[reinforcement learning]]\u2014have encountered significant challenges. Most attempts were either narrow and difficult to generalize, breaking down on more complex tasks,<ref>{{cite book |last1=Knox |first1=W. Bradley |last2=Stone |first2=Peter |last3=Breazeal |first3=Cynthia |chapter=Training a Robot via Human Feedback: A Case Study |title=Social Robotics |series=Lecture Notes in Computer Science |date=2013 |volume=8239 |pages=460\u2013470 |doi=10.1007/978-3-319-02675-6_46 |chapter-url=https://link.springer.com/chapter/10.1007/978-3-319-02675-6_46 |access-date=26 February 2024 |publisher=Springer International Publishing |isbn=978-3-319-02674-9 |language=en}}</ref><ref>{{cite book |last1=Akrour |first1=Riad |last2=Schoenauer |first2=Marc |last3=Sebag |first3=Mich\u00e8le |chapter=APRIL: Active Preference Learning-Based Reinforcement Learning |title=Machine Learning and Knowledge Discovery in Databases |series=Lecture Notes in Computer Science |date=2012 |volume=7524 |pages=116\u2013131 |doi=10.1007/978-3-642-33486-3_8 |chapter-url=https://link.springer.com/chapter/10.1007/978-3-642-33486-3_8 |access-date=26 February 2024 |publisher=Springer |language=en|arxiv=1208.0984 |isbn=978-3-642-33485-6 }}</ref><ref>{{cite journal |last1=Wilson |first1=Aaron |last2=Fern |first2=Alan |last3=Tadepalli |first3=Prasad |title=A Bayesian Approach for Policy Learning from Trajectory Preference Queries |journal=Advances in Neural Information Processing Systems |date=2012 |volume=25 |url=https://papers.nips.cc/paper_files/paper/2012/hash/16c222aa19898e5058938167c8ab6c57-Abstract.html |access-date=26 February 2024 |publisher=Curran Associates, Inc.}}</ref><ref>{{cite journal |last1=Schoenauer |first1=Marc |last2=Akrour |first2=Riad |last3=Sebag |first3=Michele |last4=Souplet |first4=Jean-Christophe |title=Programming by Feedback |journal=Proceedings of the 31st International Conference on Machine Learning |date=18 June 2014 |pages=1503\u20131511 |url=https://proceedings.mlr.press/v32/schoenauer14.html |access-date=26 February 2024 |publisher=PMLR |language=en}}</ref> or they faced difficulties learning from sparse (lacking specific information and relating to large amounts of text at a time) or noisy (inconsistently rewarding similar outputs) reward functions.<ref>{{cite journal |last1=Warnell |first1=Garrett |last2=Waytowich |first2=Nicholas |last3=Lawhern |first3=Vernon |last4=Stone |first4=Peter |title=Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces |journal=Proceedings of the AAAI Conference on Artificial Intelligence |date=25 April 2018 |volume=32 |issue=1 |doi=10.1609/aaai.v32i1.11485|s2cid=4130751 |arxiv=1709.10163 }}\n</ref><ref>{{cite journal |last1=MacGlashan |first1=James |last2=Ho |first2=Mark K. |last3=Loftin |first3=Robert |last4=Peng |first4=Bei |last5=Wang |first5=Guan |last6=Roberts |first6=David L. |last7=Taylor |first7=Matthew E. |last8=Littman |first8=Michael L. |title=Interactive learning from policy-dependent human feedback |journal=Proceedings of the 34th International Conference on Machine Learning - Volume 70 |date=6 August 2017 |pages=2285\u20132294 |url=https://dl.acm.org/doi/10.5555/3305890.3305917 |publisher=JMLR.org|arxiv=1701.06049 }}</ref>\n\nRLHF was not the first successful method of using human feedback for reinforcement learning, but it is one of the most widely used. The foundation for RLHF was introduced as an attempt to create a general algorithm for learning from a practical amount of human feedback.<ref name=\"openai\"/><ref name=\"huggingface\"/> The algorithm as used today was introduced by [[OpenAI]] in a paper on enhancing text continuation or summarization based on human feedback, and it began to gain popularity when the same method was reused in their paper on [[InstructGPT]].<ref name=\"ziegler\"/><ref name=\"summarizationpaper\"/><ref name=\"instructgptpaper\"/> RLHF has also been shown to improve the [[robust optimization|robustness]] of RL agents and their capacity for [[exploration (reinforcement learning)|exploration]], which results in an optimization process more adept at handling [[uncertainty]] and efficiently exploring its environment in search of the highest reward.<ref>{{cite arXiv |last1=Bai |first1=Yuntao |last2=Jones |first2=Andy |last3=Ndousse |first3=Kamal |last4=Askell |first4=Amanda |last5=Chen |first5=Anna |last6=DasSarma |first6=Nova |last7=Drain |first7=Dawn |last8=Fort |first8=Stanislav |last9=Ganguli |first9=Deep |last10=Henighan |first10=Tom |last11=Joseph |first11=Nicholas |last12=Kadavath |first12=Saurav |last13=Kernion |first13=Jackson |last14=Conerly |first14=Tom |last15=El-Showk |first15=Sheer |last16=Elhage |first16=Nelson |last17=Hatfield-Dodds |first17=Zac |last18=Hernandez |first18=Danny |last19=Hume |first19=Tristan |last20=Johnston |first20=Scott |last21=Kravec |first21=Shauna |last22=Lovitt |first22=Liane |last23=Nanda |first23=Neel |last24=Olsson |first24=Catherine |last25=Amodei |first25=Dario |last26=Brown |first26=Tom |last27=Clark |first27=Jack |last28=McCandlish |first28=Sam |last29=Olah |first29=Chris |last30=Mann |first30=Ben |last31=Kaplan |first31=Jared |title=Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback |date=2022 |eprint=2204.05862 |class=cs.CL}}</ref>\n\n==Collecting human feedback==\n\nHuman feedback is commonly collected by prompting humans to rank instances of the agent's behavior.<ref name=\"instructgptpaper\">{{cite conference |last1=Ouyang |first1=Long |last2=Wu |first2=Jeffrey |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Gray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |conference=Thirty-Sixth Conference on Neural Information Processing Systems: NeurIPS 2022 |date=31 October 2022 |arxiv=2203.02155 |url=https://openreview.net/forum?id=TG8KACxEON |language=en}}</ref><ref name=\"ars\">{{cite web |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |website=Ars Technica |access-date=4 March 2023 |language=en-us |date=1 December 2022}}</ref><ref>{{cite web |last1=Abhishek |first1=Gupta |title=Getting stakeholder engagement right in responsible AI |url=https://venturebeat.com/ai/getting-stakeholder-engagement-right-in-responsible-ai/ |website=VentureBeat |access-date=4 March 2023 |date=5 February 2023}}</ref> These rankings can then be used to score outputs, for example, using the [[Elo rating system]], which is an algorithm for calculating the relative skill levels of players in a game based only on the outcome of each game.<ref name=\"huggingface\"/> While ranking outputs is the most widely adopted form of feedback, recent research has explored other forms, such as numerical feedback, natural language feedback, and prompting for direct edits to the model's output.<ref>{{Cite arXiv |eprint=2305.00955 |last1=Fernandes |first1=Patrick |last2=Madaan |first2=Aman |last3=Liu |first3=Emmy |last4=Farinhas |first4=Ant\u00f3nio |author5=Pedro Henrique Martins |last6=Bertsch |first6=Amanda |last7=de Souza |first7=Jos\u00e9 G. C. |last8=Zhou |first8=Shuyan |last9=Wu |first9=Tongshuang |last10=Neubig |first10=Graham |last11=Martins |first11=Andr\u00e9 F. T. |title=Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation |date=2023 |class=cs.CL }}</ref>\n\nOne initial motivation of RLHF was that it requires relatively small amounts of comparison data to be effective.<ref name=\"openai\"/> It has been shown that a small amount of data can lead to comparable results to a larger amount. In addition, increasing the amount of data tends to be less effective than proportionally increasing the size of the reward model.<ref name=\"summarizationpaper\"/> Nevertheless, a larger and more diverse amount of data can be crucial for tasks where it is important to avoid [[algorithmic bias|bias]] from a partially [[representative sample|representative]] group of annotators.<ref name=\"instructgptpaper\"/>\n\nWhen learning from human feedback through [[pairwise comparison (psychology)|pairwise comparison]] under the [[Bradley\u2013Terry\u2013Luce]] model (or the [[Discrete choice|Plackett\u2013Luce]] model for K-wise comparisons over more than two comparisons), the [[maximum likelihood estimator]] (MLE) for linear reward functions has been shown to [[convergent series|converge]] if the comparison data is generated under a well-specified [[linear model]]. This implies that, under certain conditions, if a model is trained to decide which choices people would prefer between pairs (or groups) of choices, it will necessarily improve at predicting future preferences. This improvement is expected as long as the comparisons it learns from are based on a consistent and simple rule.<ref name=\"xiejiang\">{{cite journal |last1=Xie |first1=Tengyang |last2=Jiang |first2=Nan |last3=Wang |first3=Huan |last4=Xiong |first4=Caiming |last5=Bai |first5=Yu |title=Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning |journal=Advances in Neural Information Processing Systems |date=2021 |volume=34 |pages=27395\u201327407 |url=https://proceedings.neurips.cc/paper/2021/hash/e61eaa38aed621dd776d0e67cfeee366-Abstract.html |access-date=10 March 2024 |publisher=Curran Associates, Inc.|arxiv=2106.04895 }}</ref><ref name=\"pacchiano\">{{Cite journal |last1=Pacchiano |first1=Aldo |last2=Saha |first2=Aadirupa |last3=Lee |first3=Jonathan |date=2023-03-03 |title=Dueling RL: Reinforcement Learning with Trajectory Preferences |url=https://proceedings.mlr.press/v206/saha23a.html |journal=Proceedings of the 26th International Conference on Artificial Intelligence and Statistics |language=en |publisher=PMLR |pages= 6263\u20136289 |arxiv= 2111.04850 }}</ref>\n\nBoth offline data collection models, where the model is learning by interacting with a static dataset and updating its policy in batches, as well as online data collection models, where the model directly interacts with the dynamic environment and updates its policy immediately, have been mathematically studied proving sample complexity bounds for RLHF under different feedback models.<ref name=\"xiejiang\"/><ref name=\"zhujordan\">{{Cite journal |last1=Zhu |first1=Banghua |last2=Jordan |first2=Michael |last3=Jiao |first3=Jiantao |date=2023-07-03 |title=Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons |url=https://proceedings.mlr.press/v202/zhu23f.html |journal=Proceedings of the 40th International Conference on Machine Learning |language=en |publisher=PMLR |pages=43037\u201343067|arxiv=2301.11270 }}</ref>\n\nIn the offline data collection model, when the objective is policy training, a pessimistic MLE that incorporates a lower [[confidence bound]] as the reward estimate is most effective. Moreover, when applicable, it has been shown that considering K-wise comparisons directly is [[efficiency (statistics)#Asymptotic efficiency|asymptotically more efficient]] than converting them into pairwise comparisons for prediction purposes.<ref name=\"zhujordan\"/><ref>{{cite journal |last1=Li |first1=Zihao |last2=Yang |first2=Zhuoran |last3=Wang |first3=Mengdi |title=Reinforcement learning with Human Feedback: Learning Dynamic Choices via Pessimism |journal=ILHF Workshop ICML 2023 |date=20 June 2023 |arxiv=2305.18438 |url=https://openreview.net/forum?id=gxM2AUFMsK&referrer=%5Bthe%20profile%20of%20Zhuoran%20Yang%5D(%2Fprofile%3Fid%3D~Zhuoran_Yang1) |access-date=10 March 2024 |language=en}}</ref><ref name=\"instructgptpaper\"/>\n\nIn the online scenario, when human feedback is collected through pairwise comparisons under the Bradley\u2013Terry\u2013Luce model and the objective is to minimize the algorithm's [[regret (decision theory)|regret]] (the difference in performance compared to an optimal agent), it has been shown that an optimistic MLE that incorporates an upper [[confidence bound]] as the reward estimate can be used to design sample efficient algorithms (meaning that they require relatively little training data). A key challenge in RLHF when learning from pairwise (or dueling) comparisons is associated with the [[markov property|non-Markovian]] nature of its optimal policies. Unlike simpler scenarios where the optimal strategy does [[memoryless|not require memory]] of past actions, in RLHF, the best course of action often depends on previous events and decisions, making the strategy inherently memory-dependent.<ref name=\"pacchiano\"/>\n\n==Applications==\n\nRLHF has been applied to various domains of [[natural language processing]] (NLP), such as conversational agents, text summarization, and natural language understanding.<ref>{{cite arXiv |last1=Ouyang |first1=Long |last2=Wu |first2=Jeff |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |date=2022 |class=cs.CL |eprint=2203.02155}}\n</ref><ref name=\"summarizationpaper\">{{cite journal |author=Nisan Stiennon |author2=Long Ouyang |author3=Jeffrey Wu |author4=Daniel Ziegler |author5=Ryan Lowe |author6=Chelsea Voss |author7=Alec Radford |author8=Dario Amodei |author9=Paul F. Christiano |title=Learning to summarize with human feedback |journal=Advances in Neural Information Processing Systems |date=2020 |volume=33 |url=https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html |language=en}}</ref> Ordinary reinforcement learning, in which agents learn from their actions based on a predefined \"reward function\", is difficult to apply to NLP tasks because the rewards tend to be difficult to define or measure, especially when dealing with complex tasks that involve human values or preferences.<ref name=\"openai\"/> RLHF can steer NLP models, in particular [[language model]]s, to provide answers that [[AI alignment|align]] with human preferences with regard to such tasks by capturing their preferences beforehand in the reward model. This results in a model capable of generating more relevant responses and rejecting inappropriate or irrelevant queries.<ref name=\"instructgptpaper\"/><ref>{{cite web |last1=Wiggers |first1=Kyle |title=Can AI really be protected from text-based attacks? |url=https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/ |website=TechCrunch |access-date=4 March 2023 |date=24 February 2023}}</ref> Some notable examples of RLHF-trained language models are [[OpenAI]]'s [[ChatGPT]] (and its predecessor [[InstructGPT]]),<ref name=\"ars\"/><ref>{{cite web |last1=Heikkil\u00e4 |first1=Melissa |title=How OpenAI is trying to make ChatGPT safer and less biased |date=21 February 2023 |url=https://www.technologyreview.com/2023/02/21/1068893/how-openai-is-trying-to-make-chatgpt-safer-and-less-biased/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}\n</ref><ref>{{cite web |last1=Douglas Heaven |first1=Will |date=30 November 2022 |title=ChatGPT is OpenAI's latest fix for GPT-3. It's slick but still spews nonsense |url=https://www.technologyreview.com/2022/11/30/1063878/openai-still-fixing-gpt3-ai-large-language-model/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}</ref> [[DeepMind]]'s [[Sparrow (bot)|Sparrow]],<ref>{{cite arXiv |last1=Glaese |first1=Amelia |last2=McAleese |first2=Nat |last3=Tr\u0119bacz |first3=Maja |last4=Aslanides |first4=John |last5=Firoiu |first5=Vlad |last6=Ewalds |first6=Timo |last7=Rauh |first7=Maribeth |last8=Weidinger |first8=Laura |last9=Chadwick |first9=Martin |last10=Thacker |first10=Phoebe |last11=Campbell-Gillingham |first11=Lucy |last12=Uesato |first12=Jonathan |last13=Huang |first13=Po-Sen |last14=Comanescu |first14=Ramona |last15=Yang |first15=Fan |last16=See |first16=Abigail |last17=Dathathri |first17=Sumanth |last18=Greig |first18=Rory |last19=Chen |first19=Charlie |last20=Fritz |first20=Doug |last21=Elias |first21=Jaume Sanchez |last22=Green |first22=Richard |last23=Mokr\u00e1 |first23=So\u0148a |last24=Fernando |first24=Nicholas |last25=Wu |first25=Boxi |last26=Foley |first26=Rachel |last27=Young |first27=Susannah |last28=Gabriel |first28=Iason |last29=Isaac |first29=William |last30=Mellor |first30=John |last31=Hassabis |first31=Demis |last32=Kavukcuoglu |first32=Koray |last33=Hendricks |first33=Lisa Anne |last34=Irving |first34=Geoffrey |title=Improving alignment of dialogue agents via targeted human judgements |date=2022 |eprint=2209.14375 |class=cs.LG}}\n</ref><ref>{{cite web |last=Goldman |first=Sharon |title=Why DeepMind isn't deploying its new AI chatbot \u2014 and what it means for responsible AI |url=https://venturebeat.com/ai/why-deepmind-isnt-deploying-its-new-ai-chatbot |website=VentureBeat |access-date=4 March 2023 |date=23 September 2022}}\n</ref><ref>{{cite web |title=Building safer dialogue agents |author=The Sparrow team |url=https://www.deepmind.com/blog/building-safer-dialogue-agents |website=www.deepmind.com |date=22 September 2022 |access-date=4 March 2023 |language=en}}</ref> [[Google]]'s [[Gemini (language model)|Gemini]],<ref>{{cite web |last1=Pinchai |first1=Sundar |last2=Hassabis |first2=Demis |title=Introducing Gemini: our largest and most capable AI model |url=https://blog.google/technology/ai/google-gemini-ai/ |website=Google |access-date=29 February 2024 |language=en-us |date=6 December 2023}}</ref> and [[Anthropic]]'s [[Claude (language model)|Claude]].<ref>{{cite magazine |last1=Henshall |first1=Will |title=What to Know About Claude 2, Anthropic's Rival to ChatGPT |url=https://time.com/6295523/claude-2-anthropic-chatgpt/ |magazine=TIME |access-date=6 March 2024 |language=en |date=18 July 2023}}</ref>\n\nIn computer vision, RLHF has also been used to align [[text-to-image model]]s. Studies that successfully used RLHF for this goal have noted that the use of [[KL divergence|KL regularization]] in RLHF, which aims to prevent the learned policy from straying too far from the unaligned model, helped to stabilize the training process by reducing overfitting to the reward model. The final image outputs from models trained with KL regularization were noted to be of significantly higher quality than those trained without.<ref>{{cite journal |journal=NeurIPS 2023 |last1=Fan |first1=Ying |last2=Watkins |first2=Olivia |last3=Du |first3=Yuqing |last4=Liu |first4=Hao |last5=Ryu |first5=Moonkyung |last6=Boutilier |first6=Craig |last7=Abbeel |first7=Pieter |last8=Ghavamzadeh |first8=Mohammad |last9=Lee |first9=Kangwook |last10=Lee |first10=Kimin |title=DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models |date=2 November 2023 |arxiv=2305.16381 |url=https://openreview.net/forum?id=8OTPepXzeh&referrer=%5Bthe%20profile%20of%20Moonkyung%20Ryu%5D(%2Fprofile%3Fid%3D~Moonkyung_Ryu1) |access-date=1 March 2024 |language=en}}</ref><ref>{{cite journal |last1=Xu |first1=Jiazheng |last2=Liu |first2=Xiao |last3=Wu |first3=Yuchen |last4=Tong |first4=Yuxuan |last5=Li |first5=Qinkai |last6=Ding |first6=Ming |last7=Tang |first7=Jie |last8=Dong |first8=Yuxiao |title=ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation |journal=Advances in Neural Information Processing Systems |date=15 December 2023 |volume=36 |pages=15903\u201315935 |arxiv=2304.05977 |url=https://proceedings.neurips.cc/paper_files/paper/2023/hash/33646ef0ed554145eab65f6250fab0c9-Abstract-Conference.html |access-date=1 March 2024 |language=en}}</ref> Other methods tried to incorporate the feedback through more direct training\u2014based on maximizing the reward without the use of reinforcement learning\u2014but conceded that an RLHF-based approach would likely perform better due to the online sample generation used in RLHF during updates as well as the aforementioned KL regularization over the prior model, which mitigates [[overfitting]] to the reward function.<ref>{{cite arXiv |last1=Lee |first1=Kimin |last2=Liu |first2=Hao |last3=Ryu |first3=Moonkyung |last4=Watkins |first4=Olivia |last5=Du |first5=Yuqing |last6=Boutilier |first6=Craig |last7=Abbeel |first7=Pieter |last8=Ghavamzadeh |first8=Mohammad |last9=Gu |first9=Shixiang Shane |title=Aligning Text-to-Image Models using Human Feedback |date=2023 |class=cs.LG |eprint=2302.12192}}</ref>\n\nRLHF was initially applied to other areas, such as the development of [[video game bot]]s and tasks in [[robotics simulator|simulated robotics]]. For example, OpenAI and DeepMind trained agents to play [[Atari]] games based on human preferences. In classical RL-based training of such bots, the reward function is simply correlated to how well the agent is performing in the game, usually using metrics like the in-game [[score (game)|score]]. In comparison, in RLHF, a human is periodically presented with two clips of the agent's behavior in the game and must decide which one ''looks'' better. This approach can teach agents to perform at a competitive level without ever having access to their score. In fact, it was shown that RLHF can sometimes lead to superior performance over RL with score metrics because the human's preferences can contain more useful information than performance-based metrics.<ref name=\"openai\">{{cite web |title=Learning from human preferences |date=13 June 2017 |last1=Amodei |first1=Dario |last2=Christiano |first2=Paul |last3=Ray |first3=Alex |url=https://openai.com/research/learning-from-human-preferences |website=openai.com |access-date=4 March 2023}}</ref><ref>{{cite web |first1=Jan |last1=Leike |first2=Miljan |last2=Martic |first3=Shane |last3=Legg |title=Learning through human feedback |url=https://www.deepmind.com/blog/learning-through-human-feedback |website=www.deepmind.com |date=12 June 2017 |access-date=4 March 2023 |language=en}}</ref> The agents achieved strong performance in many of the environments tested, often surpassing human performance.<ref>{{cite journal |last1=Christiano |first1=Paul F |last2=Leike |first2=Jan |last3=Brown |first3=Tom |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |title=Deep Reinforcement Learning from Human Preferences |journal=Advances in Neural Information Processing Systems |date=2017 |volume=30 |url=https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html |access-date=4 March 2023 |publisher=Curran Associates, Inc.|arxiv=1706.03741 }}</ref>\n\n==Training==\nIn RLHF, two different models are trained: a reward model and a [[reinforcement learning]] (RL) policy. The reward model learns to determine what behavior is desirable based on human feedback, while the policy is guided by the reward model to determine the agent's actions. Both models are commonly initialized using a pre-trained [[autoregressive]] [[language model]]. This model is then customarily trained in a [[supervised learning|supervised]] manner on a relatively small dataset of pairs of prompts to an assistant and their accompanying responses, written by human annotators. The reward model benefits from starting with a pre-trained model, as this initializes it with an understanding of language and focuses training explicitly on learning human preferences, speeding up the process. In addition to being used to initialize the reward model and the RL policy, the model is then also used to sample data to be compared by annotators.<ref name=\"instructgptpaper\"/><ref name=\"summarizationpaper\"/>\n\nThe reward model is then trained by replacing the final layer of the previous model with a randomly initialized [[regression analysis|regression]] head. This change shifts the model from its original [[statistical classification|classification]] task over its vocabulary to simply outputting a number corresponding to the score of any given prompt and response. This model is trained on the human preference comparison data collected earlier from the supervised model. In particular, it is trained to [[mathematical optimization|minimize]] the following [[cross-entropy]] loss function, which incentivizes it to make predictions that are closer to the actual human ratings:\n\n<math>\\mathcal{L}(\\theta)=-\\frac{1}{K\\choose 2}E_{(x,y_w,y_l)}[\\log(\\sigma(r_\\theta(x,y_w)-r_\\theta(x,y_l)))]</math>\n\nwhere <math>K</math> is the number of responses the labelers ranked, <math>r_\\theta(x,y)</math> is the output of the reward model for prompt <math>x</math> and completion <math>y</math>, <math>y_w</math> is the preferred completion over <math>y_l</math>, <math>\\sigma(x)</math> denotes the [[sigmoid function]], and <math>E[X]</math> denotes the [[expected value]].<ref name=\"instructgptpaper\"/> This loss function essentially measures the difference between the reward model's predictions and the decisions made by humans. The goal is to make the model's guesses as close as possible to the humans' preferences by minimizing the difference measured by this equation. In the case of only pairwise comparisons, the factor of <math>1/{\\tbinom K2}</math> is omitted.<ref name=\"summarizationpaper\"/> Otherwise, all <math>{\\tbinom K2}</math> comparisons from each prompt are used for training as a single [[batch learning|batch]].<ref name=\"instructgptpaper\"/> After training, the outputs of the model are normalized such that the reference completions have a mean score of 0.<ref name=\"summarizationpaper\"/>\n\nSimilarly to the reward model, the human feedback policy is also [[fine-tuning (deep learning)|fine-tuned]] over the pre-trained model. The objective of this fine-tuning step is to adapt the pre-existing, unaligned model (initially trained in a supervised manner) to better align with human preferences by adjusting its parameters based on the rewards derived from human feedback. The output of the reward model can be used as the reward to be maximized using RL for the prompt-response pairs.<ref name=\"summarizationpaper\"/> The environment randomly presents the policy with prompts from the dataset and expects responses to them, simulating real-world scenarios where the agent must understand diverse prompts and generate appropriate responses. Denoting the learned RL policy with parameters <math>\\phi</math> as <math>\\pi_\\phi^\\text{RL}</math>, we can define the following objective function:\n\n<math>\\text{objective}(\\phi)=E_{(x,y)\\sim D_{\\pi_\\phi^\\text{RL}}}\\left[r_\\theta(x,y)-\\beta\\log\\left(\\frac{\\pi^\\text{RL}_\\phi(y|x)}{\\pi^\\text{SFT}(y|x)}\\right)\\right]</math>\n\nwhere <math>D_{\\pi_\\phi^\\text{RL}}</math> is the training [[probability distribution|distribution]] we are drawing from and <math>\\pi^\\text{SFT}</math> is the previously trained, unaligned, model. The constant <math>\\beta</math> is used to adjust the intensity of the KL penalty term. This penalty is applied on a per-[[lexical token|token]] basis between the policy and the unaligned models' outputs. Its purpose is to avoid excessively fine-tuning the policy, ensuring that the training process does not [[overfitting|overly specialize]] the model on the new training data.<ref name=\"instructgptpaper\"/><ref name=\"summarizationpaper\"/> This KL term works by penalizing the [[KL divergence]] (a measure of [[statistical distance]] between distributions) between the model being fine-tuned and the initial supervised model. By choosing an appropriate <math>\\beta</math>, the training can balance learning from new data while retaining useful information from the initial model, increasing [[generalization (learning)|generalization]] by avoiding [[overfitting|fitting too closely]] to the new data. Aside from preventing the new model from producing outputs too dissimilar those of the initial model, a second motivation of including the KL term is to allow the policy to further explore the environment by encouraging additional [[statistical entropy|entropy]], which can prevent the model from collapsing to a single [[mode (statistics)|mode]].<ref name=\"summarizationpaper\"/>\n\nIn simpler terms, the objective function calculates how well the policy's responses are expected to align with human feedback. The policy generates responses to prompts, and each response is evaluated both on how well it matches human preferences (as measured by the reward model) and how similar it is to responses the model would naturally generate. The goal is to balance improving alignment with human preferences while ensuring the model's responses remain diverse and not too far removed from what it has learned during its initial training. This helps the model not only to provide answers that people find useful or agreeable but also to maintain a broad understanding and avoid overly narrow or repetitive responses.\n\nA second term is commonly added to the objective function that allows the policy to incorporate the pre-training gradients. This term keeps the model from losing its initial language understanding ability while it learns new tasks based on human feedback by incorporating its original pre-training task of text completion. The final objective function is written as:\n\n<math>\\text{objective}(\\phi)=E_{(x,y)\\sim D_{\\pi_\\phi^\\text{RL}}}\\left[r_\\theta(x,y)-\\beta\\log\\left(\\frac{\\pi^\\text{RL}_\\phi(y|x)}{\\pi^\\text{SFT}(y|x)}\\right)\\right]+\\gamma E_{x\\sim D_\\text{pretrain}}[\\log(\\pi_\\phi^\\text{RL}(x))]</math>\n\nwhere <math>\\gamma</math> controls the strength of this additional term and <math>D_\\text{pretrain}</math> is the original pre-training text distribution.<ref name=\"instructgptpaper\"/> This objective function can then be directly used to train the policy using the [[proximal policy optimization]] algorithm.<ref name=\"instructgptpaper\"/><ref name=\"summarizationpaper\"/>\n\nIn total, this objective function defines the method for adjusting the RL policy, blending the aim of aligning with human feedback and maintaining the model's original language understanding.\n\n==Limitations==\n\nRLHF suffers from challenges with collecting human feedback, learning a reward model, and optimizing the policy.<ref name=\"openproblems\">{{cite journal |last1=Casper |first1=Stephen |last2=Davies |first2=Xander |last3=Shi |first3=Claudia |last4=Gilbert |first4=Thomas Krendl |last5=Scheurer |first5=J\u00e9r\u00e9my |last6=Rando |first6=Javier |last7=Freedman |first7=Rachel |last8=Korbak |first8=Tomasz |last9=Lindner |first9=David |last10=Freire |first10=Pedro |last11=Wang |first11=Tony Tong |last12=Marks |first12=Samuel |last13=Segerie |first13=Charbel-Raphael |last14=Carroll |first14=Micah |last15=Peng |first15=Andi |last16=Christoffersen |first16=Phillip |last17=Damani |first17=Mehul |last18=Slocum |first18=Stewart |last19=Anwar |first19=Usman |last20=Siththaranjan |first20=Anand |last21=Nadeau |first21=Max |last22=Michaud |first22=Eric J. |last23=Pfau |first23=Jacob |last24=Krasheninnikov |first24=Dmitrii |last25=Chen |first25=Xin |last26=Langosco |first26=Lauro |last27=Hase |first27=Peter |last28=Biyik |first28=Erdem |last29=Dragan |first29=Anca |last30=Krueger |first30=David |last31=Sadigh |first31=Dorsa |last32=Hadfield-Menell |first32=Dylan |title=Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback |journal=Transactions on Machine Learning Research |date=18 September 2023 |arxiv=2307.15217 |url=https://openreview.net/forum?id=bx24KpJ4Eb}}</ref> Compared to data collection for techniques like [[unsupervised learning|unsupervised]] or [[self-supervised learning]], collecting data for RLHF is less scalable and more expensive. Its quality and consistency may vary depending on the task, interface, and the preferences and biases of individual humans.<ref name=\"instructgptpaper\"/><ref>{{cite web |last1=Christiano |first1=Paul |title=Thoughts on the impact of RLHF research |date=25 January 2023 |url=https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research |access-date=4 March 2023 |language=en}}</ref>\n\nThe effectiveness of RLHF depends on the quality of human feedback. For instance, the model may become [[algorithmic bias|biased]], favoring certain groups over others, if the feedback lacks impartiality, is inconsistent, or is incorrect.<ref name=\"huggingface\"/><ref>{{cite journal |last1=Belenguer |first1=Lorenzo |title=AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry |journal=AI and Ethics |publisher=AI Ethics |date=2022|volume=2 |issue=4 |pages=771\u2013787 |doi=10.1007/s43681-022-00138-8 |pmid=35194591 |pmc=8830968 }}</ref> There is a risk of [[overfit]]ting, where the model memorizes specific feedback examples instead of learning to [[generalization (learning)|generalize]]. For instance, feedback predominantly from a specific demographic might lead the model to learn peculiarities or noise, along with the intended alignment. Excessive alignment to the specific feedback it received (that is, to the bias therein) can lead to the model performing sub-optimally in new contexts or when used by different groups.<ref>{{cite web |author=Zhang, Chiyuan |author2=Bengio, Samy |author3=Hardt, Moritz |author4=Recht, Benjamin |author5=Vinyals, Oriol |title=Understanding deep learning requires rethinking generalization |date=4 November 2016 |url=https://openreview.net/forum?id=Sy8gdB9xx |publisher=International Conference on Learning Representations}}</ref> A single reward function cannot always represent the opinions of diverse groups of people. Even with a representative sample, conflicting views and preferences may result in the reward model favoring the majority's opinion, potentially disadvantaging underrepresented groups.<ref name=\"openproblems\"/>\n\nIn some cases, as is possible in regular [[reinforcement learning]], there may be a risk of the model learning to manipulate the feedback process or [[game the system]] to achieve higher rewards rather than genuinely improving its performance.<ref>{{cite web |last1=Clark |first1=Jack |last2=Amodei |first2=Dario |title=Faulty reward functions in the wild |date=21 December 2016 |url=https://openai.com/research/faulty-reward-functions |publisher=OpenAI}}</ref> In the case of RLHF, a model may learn to exploit the fact that it is rewarded for what is evaluated positively and not necessarily for what is actually good, which can lead to it learning to persuade and manipulate. For example, models might learn that apparent confidence, even if inaccurate, garners higher rewards. Such behavior, if unchecked, is not just incentivized but can cause significant deployment issues due to the model's potential to mislead. Studies have found that humans are not skilled at identifying mistakes in LLM outputs in complex tasks; therefore, models learning to generate confident-sounding yet incorrect text can lead to significant issues when deployed.<ref name=\"openproblems\"/>\n\n==Alternatives==\n\n=== Reinforcement learning from AI feedback ===\nSimilarly to RLHF, ''reinforcement learning from AI feedback'' (RLAIF) relies on training a preference model, except that the feedback is automatically generated.<ref>{{Cite web |last=Ansari |first=Tasmia |date=2023-10-06 |title=Reinforcement Learning Craves Less Human, More AI |url=https://analyticsindiamag.com/reinforcement-learning-craves-for-less-human-more-ai/ |access-date=2024-04-27 |website=Analytics India Magazine |language=en-US}}</ref> This is notably used in [[Anthropic]]'s [[constitutional AI]], where the AI feedback is based on the conformance to the principles of a constitution.<ref>{{Cite web |last=Edwards |first=Benj |date=2023-05-09 |title=AI gains \"values\" with Anthropic's new Constitutional AI chatbot approach |url=https://arstechnica.com/information-technology/2023/05/ai-with-a-moral-compass-anthropic-outlines-constitutional-ai-in-its-claude-chatbot/ |access-date=2024-04-27 |website=Ars Technica |language=en-us}}</ref>\n\n=== Direct preference optimization ===\nAnother alternative to RLHF called Direct Preference Optimization (DPO) has been proposed to learn human preferences. Like RLHF, it has been applied to [[AI alignment|align]] pre-trained [[large language model]]s using human-generated preference data. Unlike RLHF, however, which first trains a separate intermediate model to understand what good outcomes look like and then teaches the main model how to achieve those outcomes, DPO simplifies the process by directly adjusting the main model according to people's preferences. It uses a [[change of variables]] to define the \"preference [[loss function|loss]]\" directly as a function of the policy and uses this loss to [[fine-tuning (deep learning)|fine-tune]] the model, helping it understand and prioritize human preferences without needing a separate step. Essentially, this approach directly shapes the model's decisions based on positive or negative human feedback.\n\nDPO is simpler to implement and train than RLHF and has been shown to produce comparable and sometimes superior results.<ref>{{cite arXiv|eprint=2305.18290 |last1=Rafailov |first1=Rafael |last2=Sharma |first2=Archit |last3=Mitchell |first3=Eric |last4=Ermon |first4=Stefano |last5=Manning |first5=Christopher D. |last6=Finn |first6=Chelsea |title=Direct Preference Optimization: Your Language Model is Secretly a Reward Model |date=2023 |class=cs.LG }}</ref> Nevertheless, RLHF has also been shown to beat DPO on some datasets, for example, on benchmarks that attempt to measure truthfulness. Therefore, the choice of method may vary depending on the features of the human preference data and the nature of the task.<ref>{{cite arXiv |last1=Wang |first1=Zhilin |last2=Dong |first2=Yi |last3=Zeng |first3=Jiaqi |last4=Adams |first4=Virginia |last5=Sreedhar |first5=Makesh Narsimhan |last6=Egert |first6=Daniel |last7=Delalleau |first7=Olivier |last8=Scowcroft |first8=Jane Polak |last9=Kant |first9=Neel |last10=Swope |first10=Aidan |last11=Kuchaiev |first11=Oleksii |title=HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM |date=2023 |class=cs.CL |eprint=2311.09528}}</ref>\n\n==See also==\n* [[Human-in-the-loop]]\n* [[Reward-based selection]]\n\n==References==\n{{reflist}}\n\n{{Artificial intelligence navbox}}\n\n[[Category:Reinforcement learning]]\n[[Category:Language modeling]]"}