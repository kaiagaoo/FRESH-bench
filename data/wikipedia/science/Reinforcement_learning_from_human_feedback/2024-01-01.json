{"title": "Reinforcement learning from human feedback", "page_id": 73200355, "revision_id": 1192206711, "revision_timestamp": "2023-12-28T05:28:41Z", "content": "{{Short description|Machine learning technique}}\nIn [[Machine learning]], '''reinforcement learning from human feedback''' ('''RLHF'''), including '''reinforcement learning from human preferences''',  is a technique that trains a \"reward model\" directly from human [[feedback]] and uses the model as a reward function to optimize an [[intelligent agent|agent]]'s [[Reinforcement learning#Policy|policy]] using [[reinforcement learning]] (RL) through an optimization algorithm like [[Proximal Policy Optimization]].<ref>{{cite arXiv |last1=Ziegler |first1=Daniel M. |last2=Stiennon |first2=Nisan |last3=Wu |first3=Jeffrey |last4=Brown |first4=Tom B. |last5=Radford |first5=Alec |last6=Amodei |first6=Dario |last7=Christiano |first7=Paul |last8=Irving |first8=Geoffrey |title=Fine-Tuning Language Models from Human Preferences |date=2019 |eprint=1909.08593 |class=cs.CL}}</ref><ref name=\"huggingface\">{{cite web |last1=Lambert |first1=Nathan |last2=Castricato |first2=Louis |last3=von Werra |first3=Leandro |last4=Havrilla |first4=Alex |title=Illustrating Reinforcement Learning from Human Feedback (RLHF) |url=https://huggingface.co/blog/rlhf |website=huggingface.co |access-date=4 March 2023}}</ref> The reward model is trained in advance to the policy being optimized to predict if a given output is good (high reward) or bad (low reward). RLHF can improve the [[robust optimisation|robustness]] and exploration of reinforcement-learning agents, especially when the reward function is sparse or noisy.<ref>{{cite journal |last1=MacGlashan |first1=James |last2=Ho |first2=Mark K |last3=Loftin |first3=Robert |last4=Peng |first4=Bei |last5=Wang |first5=Guan |last6=Roberts |first6=David L. |last7=Taylor |first7=Matthew E. |last8=Littman |first8=Michael L. |title=Interactive learning from policy-dependent human feedback |journal=Proceedings of the 34th International Conference on Machine Learning - Volume 70 |date=6 August 2017 |pages=2285\u20132294 |url=https://dl.acm.org/doi/10.5555/3305890.3305917 |publisher=JMLR.org|arxiv=1701.06049 }}\n* {{cite journal |last1=Warnell |first1=Garrett |last2=Waytowich |first2=Nicholas |last3=Lawhern |first3=Vernon |last4=Stone |first4=Peter |title=Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces |journal=Proceedings of the AAAI Conference on Artificial Intelligence |date=25 April 2018 |volume=32 |issue=1 |doi=10.1609/aaai.v32i1.11485|s2cid=4130751 |arxiv=1709.10163 }}\n* {{cite arXiv |last1=Bai |first1=Yuntao |last2=Jones |first2=Andy |last3=Ndousse |first3=Kamal |last4=Askell |first4=Amanda |last5=Chen |first5=Anna |last6=DasSarma |first6=Nova |last7=Drain |first7=Dawn |last8=Fort |first8=Stanislav |last9=Ganguli |first9=Deep |last10=Henighan |first10=Tom |last11=Joseph |first11=Nicholas |last12=Kadavath |first12=Saurav |last13=Kernion |first13=Jackson |last14=Conerly |first14=Tom |last15=El-Showk |first15=Sheer |last16=Elhage |first16=Nelson |last17=Hatfield-Dodds |first17=Zac |last18=Hernandez |first18=Danny |last19=Hume |first19=Tristan |last20=Johnston |first20=Scott |last21=Kravec |first21=Shauna |last22=Lovitt |first22=Liane |last23=Nanda |first23=Neel |last24=Olsson |first24=Catherine |last25=Amodei |first25=Dario |last26=Brown |first26=Tom |last27=Clark |first27=Jack |last28=McCandlish |first28=Sam |last29=Olah |first29=Chris |last30=Mann |first30=Ben |last31=Kaplan |first31=Jared |title=Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback |date=2022 |eprint=2204.05862 |class=cs.CL}}</ref>\n\nHuman feedback is most commonly collected by asking humans to rank instances of the agent's behavior.<ref>{{cite conference |last1=Ouyang |first1=Long |last2=Wu |first2=Jeffrey |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Gray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |conference=Thirty-Sixth Conference on Neural Information Processing Systems: NeurIPS 2022 |date=31 October 2022 |arxiv=2203.02155 |url=https://openreview.net/forum?id=TG8KACxEON |language=en}}</ref><ref name=\"ars\">{{cite web |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |website=Ars Technica |access-date=4 March 2023 |language=en-us |date=1 December 2022}}</ref><ref>{{cite web |last1=Abhishek |first1=Gupta |title=Getting stakeholder engagement right in responsible AI |url=https://venturebeat.com/ai/getting-stakeholder-engagement-right-in-responsible-ai/ |website=VentureBeat |access-date=4 March 2023 |date=5 February 2023}}</ref> These rankings can then be used to score outputs, for example with the [[Elo rating system]].<ref name=\"huggingface\"/> While the preference judgement is widely adopted, there are other types of human feedbacks that provide richer information, such as numerical feedback, natural language feedback, and edit rate.<ref>{{Cite arXiv |eprint=2305.00955 |last1=Fernandes |first1=Patrick |last2=Madaan |first2=Aman |last3=Liu |first3=Emmy |last4=Farinhas |first4=Ant\u00f3nio |author5=Pedro Henrique Martins |last6=Bertsch |first6=Amanda |last7=de Souza |first7=Jos\u00e9 G. C. |last8=Zhou |first8=Shuyan |last9=Wu |first9=Tongshuang |last10=Neubig |first10=Graham |last11=Martins |first11=Andr\u00e9 F. T. |title=Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation |date=2023 |class=cs.CL }}</ref> \n\nThe standard RLHF assumes the human preferences follow a [[Bradley\u2013Terry model|Bradley-Terry model]] for pairwise comparisons (or [[Discrete choice|Plackket-Luce]] for multi-wise comparisons) and minimizes the cross entropy loss to learn a reward model.<ref>{{Cite journal |last1=Zhu |first1=Banghua |last2=Jordan |first2=Michael |last3=Jiao |first3=Jiantao |date=2023-07-03 |title=Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons |url=https://proceedings.mlr.press/v202/zhu23f.html |journal=Proceedings of the 40th International Conference on Machine Learning |language=en |publisher=PMLR |pages=43037\u201343067}}</ref> After learning the reward model, RLHF further fine-tunes the language model according to the learned reward model, aligning the model with human preferences.\n\nRLHF is used in tasks where it's difficult to define a clear, algorithmic solution but where humans can easily judge the quality of the model's output. For example, if the task is to generate a compelling story, humans can rate different AI-generated stories on their quality, and the model can use their feedback to improve its story generation skills.\n\nRLHF has been applied to various domains of natural language processing, such as conversational agents, text summarization, and natural language understanding.<ref>{{cite arXiv |last1=Ouyang |first1=Long |last2=Wu |first2=Jeff |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |date=2022 |class=cs.CL |eprint=2203.02155}}\n* {{cite journal |author=Nisan Stiennon |author2=Long Ouyang |author3=Jeffrey Wu |author4=Daniel Ziegler |author5=Ryan Lowe |author6=Chelsea Voss |author7=Alec Radford |author8=Dario Amodei |author9=Paul F. Christiano |title=Learning to summarize with human feedback |journal=Advances in Neural Information Processing Systems |date=2020 |volume=33 |url=https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html |language=en}}</ref> Ordinary reinforcement learning, where agents learn from their own actions based on a \"reward function\", is difficult to apply to [[natural language processing]] tasks because the rewards are often not easy to define or measure, especially when dealing with complex tasks that involve human values or preferences. RLHF can enable language models to provide answers that align with these complex values, to generate more verbose responses, and to reject questions that are either inappropriate or outside the knowledge space of the model.<ref>{{cite web |last1=Wiggers |first1=Kyle |title=Can AI really be protected from text-based attacks? |url=https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/ |website=TechCrunch |access-date=4 March 2023 |date=24 February 2023}}</ref> Some examples of RLHF-trained language models are OpenAI's [[ChatGPT]] and its predecessor InstructGPT,<ref name=\"ars\"/><ref>{{cite web |last1=Farseev |first1=Aleks |title=Council Post: Is Bigger Better? Why The ChatGPT Vs. GPT-3 Vs. GPT-4 'Battle' Is Just A Family Chat |url=https://www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat/?sh=40a3577a5b65 |website=Forbes |access-date=4 March 2023 |language=en}}\n* {{cite web |last1=Heikkil\u00e4 |first1=Melissa |title=How OpenAI is trying to make ChatGPT safer and less biased |url=https://www.technologyreview.com/2023/02/21/1068893/how-openai-is-trying-to-make-chatgpt-safer-and-less-biased/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}\n* {{cite web |last1=Douglas Heaven |first1=Will |title=ChatGPT is OpenAI's latest fix for GPT-3. It's slick but still spews nonsense |url=https://www.technologyreview.com/2022/11/30/1063878/openai-still-fixing-gpt3-ai-large-language-model/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}</ref> as well as [[DeepMind]]'s [[Sparrow (bot)|Sparrow]].<ref>{{cite arXiv |last1=Glaese |first1=Amelia |last2=McAleese |first2=Nat |last3=Tr\u0119bacz |first3=Maja |last4=Aslanides |first4=John |last5=Firoiu |first5=Vlad |last6=Ewalds |first6=Timo |last7=Rauh |first7=Maribeth |last8=Weidinger |first8=Laura |last9=Chadwick |first9=Martin |last10=Thacker |first10=Phoebe |last11=Campbell-Gillingham |first11=Lucy |last12=Uesato |first12=Jonathan |last13=Huang |first13=Po-Sen |last14=Comanescu |first14=Ramona |last15=Yang |first15=Fan |last16=See |first16=Abigail |last17=Dathathri |first17=Sumanth |last18=Greig |first18=Rory |last19=Chen |first19=Charlie |last20=Fritz |first20=Doug |last21=Elias |first21=Jaume Sanchez |last22=Green |first22=Richard |last23=Mokr\u00e1 |first23=So\u0148a |last24=Fernando |first24=Nicholas |last25=Wu |first25=Boxi |last26=Foley |first26=Rachel |last27=Young |first27=Susannah |last28=Gabriel |first28=Iason |last29=Isaac |first29=William |last30=Mellor |first30=John |last31=Hassabis |first31=Demis |last32=Kavukcuoglu |first32=Koray |last33=Hendricks |first33=Lisa Anne |last34=Irving |first34=Geoffrey |title=Improving alignment of dialogue agents via targeted human judgements |date=2022 |eprint=2209.14375 |class=cs.LG}}\n* {{cite web |title=Why DeepMind isn't deploying its new AI chatbot \u2014 and what it means for responsible AI |url=https://venturebeat.com/ai/why-deepmind-isnt-deploying-its-new-ai-chatbot |website=VentureBeat |access-date=4 March 2023 |date=23 September 2022}}\n* {{cite web |title=Building safer dialogue agents |url=https://www.deepmind.com/blog/building-safer-dialogue-agents |website=www.deepmind.com |access-date=4 March 2023 |language=en}}</ref>\n\nRLHF has also been applied to other areas, such as the development of [[video game bot]]s. For example, OpenAI and DeepMind trained agents to play [[Atari]] games based on human preferences.<ref>{{cite web |title=Learning from human preferences |url=https://openai.com/research/learning-from-human-preferences |website=openai.com |access-date=4 March 2023}}</ref><ref>{{cite web |title=Learning through human feedback |url=https://www.deepmind.com/blog/learning-through-human-feedback |website=www.deepmind.com |access-date=4 March 2023 |language=en}}</ref> The agents achieved strong performance in many of the environments tested, often surpassing human performance.<ref>{{cite journal |last1=Christiano |first1=Paul F |last2=Leike |first2=Jan |last3=Brown |first3=Tom |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |title=Deep Reinforcement Learning from Human Preferences |journal=Advances in Neural Information Processing Systems |date=2017 |volume=30 |url=https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html |access-date=4 March 2023 |publisher=Curran Associates, Inc.}}</ref>\n\n==Challenges and limitations==\n\nRLHF suffers from a number of challenges that can be broken down into problems with human feedback, problems with learning a reward model, and problems with optimizing the policy.<ref>{{cite arXiv |author=Casper, Stephen |author2=Davies, Xander |author3=Shi, Claudia |author4=Gilbert, Thomas Krendl |author5=Scheurer, J\u00e9r\u00e9my |author6=Rando, Javier |author7=Freedman, Rachel |author8=Korbak, Tomasz |author9=Lindner, David |author10=Freire, Pedro |author11=Wang, Tony |author12=Marks, Samuel |author13=Segerie, Charbel-Rapha\u00ebl |author14=Carroll, Micah |author15=Peng, Andi |author16=Christoffersen, Phillip |author17=Damani, Mehul |author18=Slocum, Stewart |author19=Anwar, Usman |author20=Siththaranjan, Anand |author21=Nadeau, Max |author22=Michaud, Eric J. |author23=Pfau, Jacob |author24=Krasheninnikov, Dmitrii |author25=Chen, Xin |author26=Langosco, Lauro |author27=Hase, Peter |author28=B\u0131y\u0131k, Erdem |author29=Dragan, Anca |author30=Krueger, David |author31=Sadigh, Dorsa |author32=Hadfield-Menell, Dylan |title=Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback |date=2023 |class=cs.AI |eprint=2307.15217 |language=en}}</ref>\n\nOne major challenge is the scalability and cost of human feedback, which can be slow and expensive compared to unsupervised learning. The quality and consistency of human feedback can also vary depending on the task, the interface, and the individual preferences of the humans. Even when human feedback is feasible, RLHF models may still exhibit undesirable behaviors that are not captured by human feedback or exploit loopholes in the reward model, which brings to light the challenges of [[AI alignment|alignment]] and [[Robust optimization|robustness]].<ref>{{cite web |last1=Christiano |first1=Paul |title=Thoughts on the impact of RLHF research |url=https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research |access-date=4 March 2023 |language=en}}</ref>\n\nThe effectiveness of RLHF is dependent on the quality of human feedback.<ref>{{cite web |title=Illustrating Reinforcement Learning from Human Feedback (RLHF) |url=https://huggingface.co/blog/rlhf#whats-next-for-rlhf:~:text=RLHF%20performance%20is%20only%20as%20good%20as%20the%20quality%20of%20its%20human%20annotations%2C%20which%20takes%20on%20two%20varieties%3A%20human%2Dgenerated%20text%2C%20such%20as%20fine%2Dtuning%20the%20initial%20LM%20in%20InstructGPT%2C%20and%20labels%20of%20human%20preferences%20between%20model%20outputs |website=Hugging Face}}</ref> If the feedback lacks impartiality or is inconsistent or incorrect, the model may become [[Algorithmic_bias|biased]].<ref>{{cite journal |last1=Belenguer |first1=Lorenzo |title=AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry |journal=AI and Ethics |publisher=AI Ethics |date=2022|volume=2 |issue=4 |pages=771\u2013787 |doi=10.1007/s43681-022-00138-8 |pmid=35194591 |pmc=8830968 }}</ref> There is also a risk that the model may [[overfit]] to the feedback it receives. For instance, if feedback comes predominantly from a specific demographic or if it reflects specific biases, the model may learn not only the general alignment intended in the feedback, but also any peculiarities or noise present therein.<ref>{{cite web |last1=Wang |first1=Austin |title=Training Language Models to Follow Instructions with Human Feedback |url=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec18.pdf |publisher=Princeton}}</ref><ref>{{cite web |author=Zhang, Chiyuan |author2=Bengio, Samy |author3=Hardt, Moritz |author4=Recht, Benjamin |author5=Vinyals, Oriol |title=Understanding deep learning requires rethinking generalization |date=4 November 2016 |url=https://openreview.net/forum?id=Sy8gdB9xx |publisher=International Conference on Learning Representations}}</ref> This excessive alignment to the specific feedback it received (or to the biases of the specific demographic that provided it) can lead to the model performing suboptimally in new contexts or when used by different groups.\n\nAdditionally, in some cases, there may be a risk of the model learning to manipulate the feedback process or [[Gaming_the_system | game the system]] to achieve higher rewards, rather than genuinely improving its performance, which indicates a fault in the reward function.<ref>{{cite web |title=Faulty reward functions in the wild |url=https://openai.com/research/faulty-reward-functions |publisher=OpenAI}}</ref>\n\nResearchers have surveyed a number of additional limitations to RLHF.<ref>{{Cite web |date=2023-07-31 |title=Paper page - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback |url=https://huggingface.co/papers/2307.15217 |access-date=2023-07-31 |website=huggingface.co}}</ref>\n\n== Alternatives ==\nAn alternative to RLHF called [[Direct Preference Optimization]] (DPO) was described in 2023.<ref>{{cite arXiv|eprint=2305.18290 |last1=Rafailov |first1=Rafael |last2=Sharma |first2=Archit |last3=Mitchell |first3=Eric |last4=Ermon |first4=Stefano |last5=Manning |first5=Christopher D. |last6=Finn |first6=Chelsea |title=Direct Preference Optimization: Your Language Model is Secretly a Reward Model |date=2023 |class=cs.LG }}</ref> Like RLHF, it is used to improve pre-trained large language models using human-generated preference data. Unlike RLHF, it does not train an intermediate reward model and does not use reinforcement learning; instead, it formulates a reward function based on the human preferences and directly trains the large language model to maximize this reward.\n\n==See also==\n* [[Reinforcement learning]]\n* [[ChatGPT]]\n* [[Reward-based selection]]\n\n==References==\n{{reflist}}\n\n[[Category:Machine learning]]\n[[Category:Reinforcement learning]]\n[[Category:Language modeling]]\n[[Category:Artificial intelligence]]"}