{"title": "Reinforcement learning from human feedback", "page_id": 73200355, "revision_id": 1147290523, "revision_timestamp": "2023-03-30T00:08:02Z", "content": "{{Short description|Machine learning technique}}\nIn machine learning, '''reinforcement learning from human feedback''' ('''RLHF''') or '''reinforcement learning from human preferences'''  is a technique that trains a \"reward model\" directly from human feedback and uses the model as a reward function to optimize an [[intelligent agent|agent]]'s [[Reinforcement learning#Policy|policy]] using [[reinforcement learning]] (RL) through an optimization algorithm like [[Proximal Policy Optimization]].<ref>{{cite journal |last1=Ziegler |first1=Daniel M. |last2=Stiennon |first2=Nisan |last3=Wu |first3=Jeffrey |last4=Brown |first4=Tom B. |last5=Radford |first5=Alec |last6=Amodei |first6=Dario |last7=Christiano |first7=Paul |last8=Irving |first8=Geoffrey |title=Fine-Tuning Language Models from Human Preferences |date=2019 |arxiv=1909.08593}}</ref><ref name=\"huggingface\">{{cite web |last1=Lambert |first1=Nathan |last2=Castricato |first2=Louis |last3=von Werra |first3=Leandro |last4=Havrilla |first4=Alex |title=Illustrating Reinforcement Learning from Human Feedback (RLHF) |url=https://huggingface.co/blog/rlhf |website=huggingface.co |access-date=4 March 2023}}</ref> The reward model is trained in advance to the policy being optimized to predict if a given output is good (high reward) or bad (low reward). RLHF can improve the [[robust optimisation|robustness]] and exploration of RL agents, especially when the reward function is sparse or noisy.<ref>{{cite journal |last1=MacGlashan |first1=James |last2=Ho |first2=Mark K |last3=Loftin |first3=Robert |last4=Peng |first4=Bei |last5=Wang |first5=Guan |last6=Roberts |first6=David L. |last7=Taylor |first7=Matthew E. |last8=Littman |first8=Michael L. |title=Interactive learning from policy-dependent human feedback |journal=Proceedings of the 34th International Conference on Machine Learning - Volume 70 |date=6 August 2017 |pages=2285\u20132294 |url=https://dl.acm.org/doi/10.5555/3305890.3305917 |publisher=JMLR.org|arxiv=1701.06049 }}\n* {{cite journal |last1=Warnell |first1=Garrett |last2=Waytowich |first2=Nicholas |last3=Lawhern |first3=Vernon |last4=Stone |first4=Peter |title=Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces |journal=Proceedings of the AAAI Conference on Artificial Intelligence |date=25 April 2018 |volume=32 |issue=1 |doi=10.1609/aaai.v32i1.11485|s2cid=4130751 }}\n* {{cite journal |last1=Bai |first1=Yuntao |last2=Jones |first2=Andy |last3=Ndousse |first3=Kamal |last4=Askell |first4=Amanda |last5=Chen |first5=Anna |last6=DasSarma |first6=Nova |last7=Drain |first7=Dawn |last8=Fort |first8=Stanislav |last9=Ganguli |first9=Deep |last10=Henighan |first10=Tom |last11=Joseph |first11=Nicholas |last12=Kadavath |first12=Saurav |last13=Kernion |first13=Jackson |last14=Conerly |first14=Tom |last15=El-Showk |first15=Sheer |last16=Elhage |first16=Nelson |last17=Hatfield-Dodds |first17=Zac |last18=Hernandez |first18=Danny |last19=Hume |first19=Tristan |last20=Johnston |first20=Scott |last21=Kravec |first21=Shauna |last22=Lovitt |first22=Liane |last23=Nanda |first23=Neel |last24=Olsson |first24=Catherine |last25=Amodei |first25=Dario |last26=Brown |first26=Tom |last27=Clark |first27=Jack |last28=McCandlish |first28=Sam |last29=Olah |first29=Chris |last30=Mann |first30=Ben |last31=Kaplan |first31=Jared |title=Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback |date=2022 |arxiv=2204.05862}}</ref>\n\nHuman feedback is collected by asking humans to rank instances of the agent's behavior.<ref>{{cite journal |last1=Ouyang |first1=Long |last2=Wu |first2=Jeffrey |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Gray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |date=31 October 2022 |arxiv=2203.02155 |url=https://openreview.net/forum?id=TG8KACxEON |language=en}}</ref><ref name=\"ars\">{{cite web |last1=Edwards |first1=Benj |title=OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot\u2014with amusing results |url=https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/ |website=Ars Technica |access-date=4 March 2023 |language=en-us |date=1 December 2022}}</ref><ref>{{cite web |last1=Abhishek |first1=Gupta |title=Getting stakeholder engagement right in responsible AI |url=https://venturebeat.com/ai/getting-stakeholder-engagement-right-in-responsible-ai/ |website=VentureBeat |access-date=4 March 2023 |date=5 February 2023}}</ref> These rankings can then be used to score outputs, for example with the [[Elo rating system]].<ref name=\"huggingface\"/>\n\nRLHF has been applied to various domains of natural language processing, such as conversational agents, text summarization, and natural language understanding.<ref>{{cite journal |last1=Ouyang |first1=Long |last2=Wu |first2=Jeff |last3=Jiang |first3=Xu |last4=Almeida |first4=Diogo |last5=Wainwright |first5=Carroll L. |last6=Mishkin |first6=Pamela |last7=Zhang |first7=Chong |last8=Agarwal |first8=Sandhini |last9=Slama |first9=Katarina |last10=Ray |first10=Alex |last11=Schulman |first11=John |last12=Hilton |first12=Jacob |last13=Kelton |first13=Fraser |last14=Miller |first14=Luke |last15=Simens |first15=Maddie |last16=Askell |first16=Amanda |last17=Welinder |first17=Peter |last18=Christiano |first18=Paul |last19=Leike |first19=Jan |last20=Lowe |first20=Ryan |title=Training language models to follow instructions with human feedback |date=2022 |arxiv=2203.02155}}\n* {{cite journal |last1=Nisan |first1=Stiennon |last2=Long |first2=Ouyang |last3=Jeffrey |first3=Wu |last4=Daniel |first4=Ziegler |last5=Ryan |first5=Lowe |last6=Chelsea |first6=Voss |last7=Alec |first7=Radford |last8=Dario |first8=Amodei |last9=F. |first9=Christiano, Paul |title=Learning to summarize with human feedback |journal=Advances in Neural Information Processing Systems |date=2020 |volume=33 |url=https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html |language=en}}</ref> Ordinary reinforcement learning, where agents learn from their own actions based on a \"reward function\", is difficult to apply to [[natural language processing]] tasks because the rewards are often not easy to define or measure, especially when dealing with complex tasks that involve human values or preferences. RLHF can enable language models to provide answers that align with these complex values, to generate more verbose responses, and to reject questions that are either inappropriate or outside the knowledge space of the model.<ref>{{cite web |last1=Wiggers |first1=Kyle |title=Can AI really be protected from text-based attacks? |url=https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/ |website=TechCrunch |access-date=4 March 2023 |date=24 February 2023}}</ref> Some examples of RLHF-trained language models are OpenAI's [[ChatGPT]] and its predecessor InstructGPT,<ref name=\"ars\"/><ref>{{cite web |last1=Farseev |first1=Aleks |title=Council Post: Is Bigger Better? Why The ChatGPT Vs. GPT-3 Vs. GPT-4 'Battle' Is Just A Family Chat |url=https://www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat/?sh=40a3577a5b65 |website=Forbes |access-date=4 March 2023 |language=en}}\n* {{cite web |last1=Heikkil\u00e4 |first1=Melissa |title=How OpenAI is trying to make ChatGPT safer and less biased |url=https://www.technologyreview.com/2023/02/21/1068893/how-openai-is-trying-to-make-chatgpt-safer-and-less-biased/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}\n* {{cite web |last1=Douglas Heaven |first1=Will |title=ChatGPT is OpenAI's latest fix for GPT-3. It's slick but still spews nonsense |url=https://www.technologyreview.com/2022/11/30/1063878/openai-still-fixing-gpt3-ai-large-language-model/ |website=MIT Technology Review |access-date=4 March 2023 |language=en}}</ref> as well as [[DeepMind]]'s [[Sparrow (bot)|Sparrow]].<ref>{{cite journal |last1=Glaese |first1=Amelia |last2=McAleese |first2=Nat |last3=Tr\u0119bacz |first3=Maja |last4=Aslanides |first4=John |last5=Firoiu |first5=Vlad |last6=Ewalds |first6=Timo |last7=Rauh |first7=Maribeth |last8=Weidinger |first8=Laura |last9=Chadwick |first9=Martin |last10=Thacker |first10=Phoebe |last11=Campbell-Gillingham |first11=Lucy |last12=Uesato |first12=Jonathan |last13=Huang |first13=Po-Sen |last14=Comanescu |first14=Ramona |last15=Yang |first15=Fan |last16=See |first16=Abigail |last17=Dathathri |first17=Sumanth |last18=Greig |first18=Rory |last19=Chen |first19=Charlie |last20=Fritz |first20=Doug |last21=Elias |first21=Jaume Sanchez |last22=Green |first22=Richard |last23=Mokr\u00e1 |first23=So\u0148a |last24=Fernando |first24=Nicholas |last25=Wu |first25=Boxi |last26=Foley |first26=Rachel |last27=Young |first27=Susannah |last28=Gabriel |first28=Iason |last29=Isaac |first29=William |last30=Mellor |first30=John |last31=Hassabis |first31=Demis |last32=Kavukcuoglu |first32=Koray |last33=Hendricks |first33=Lisa Anne |last34=Irving |first34=Geoffrey |title=Improving alignment of dialogue agents via targeted human judgements |date=2022 |arxiv=2209.14375}}\n* {{cite web |title=Why DeepMind isn't deploying its new AI chatbot \u2014 and what it means for responsible AI |url=https://venturebeat.com/ai/why-deepmind-isnt-deploying-its-new-ai-chatbot |website=VentureBeat |access-date=4 March 2023 |date=23 September 2022}}\n* {{cite web |title=Building safer dialogue agents |url=https://www.deepmind.com/blog/building-safer-dialogue-agents |website=www.deepmind.com |access-date=4 March 2023 |language=en}}</ref>\n\nRLHF has also been applied to other areas, such as the development of [[video game bot]]s. For example, OpenAI and DeepMind trained agents to play [[Atari]] games based on human preferences.<ref>{{cite web |title=Learning from human preferences |url=https://openai.com/research/learning-from-human-preferences |website=openai.com |access-date=4 March 2023}}</ref><ref>{{cite web |title=Learning through human feedback |url=https://www.deepmind.com/blog/learning-through-human-feedback |website=www.deepmind.com |access-date=4 March 2023 |language=en}}</ref> The agents achieved strong performance in many of the environments tested, often surpassing human performance.<ref>{{cite journal |last1=Christiano |first1=Paul F |last2=Leike |first2=Jan |last3=Brown |first3=Tom |last4=Martic |first4=Miljan |last5=Legg |first5=Shane |last6=Amodei |first6=Dario |title=Deep Reinforcement Learning from Human Preferences |journal=Advances in Neural Information Processing Systems |date=2017 |volume=30 |url=https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html |access-date=4 March 2023 |publisher=Curran Associates, Inc.}}</ref>\n\n==Challenges and limitations==\n\nOne major challenge of RLHF is the scalability and cost of human feedback, which can be slow and expensive compared to unsupervised learning. The quality and consistency of human feedback can also vary depending on the task, the interface, and the individual preferences of the humans. Even when human feedback is feasible, RLHF models may still exhibit undesirable behaviors that are not captured by human feedback or exploit loopholes in the reward model, which brings to light the challenges of [[AI alignment|alignment]] and [[Robust optimization|robustness]].<ref>{{cite web |last1=Christiano |first1=Paul |title=Thoughts on the impact of RLHF research |url=https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research |access-date=4 March 2023 |language=en}}</ref>\n\n==See also==\n* [[Reinforcement learning]]\n* [[ChatGPT]]\n\n==References==\n{{reflist}}\n\n[[Category:Machine learning]]\n[[Category:Reinforcement learning]]\n[[Category:Language modeling]]\n[[Category:Artificial intelligence]]"}