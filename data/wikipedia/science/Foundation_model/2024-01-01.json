{"title": "Foundation model", "page_id": 70984276, "revision_id": 1191907386, "revision_timestamp": "2023-12-26T14:45:28Z", "content": "{{Short description|Artificial intelligence model paradigm}}\n{{Use dmy dates|date=May 2023}}\nA '''foundation model''' is an AI model that is trained on broad data such that it can be applied across a wide range of use cases.<ref name=\":1\">Competition and Markets Authority (2023). ''AI Foundation Models: Initial Report''. Available at: https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf</ref> Foundation models have transformed AI, powering prominent [[Chatbot|chatbots]] and [[Generative artificial intelligence|generative AI]].<ref name=\":1\" /> The Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) created and popularized the term.<ref name=\"CRFM\">{{Cite web |title=Introducing the Center for Research on Foundation Models (CRFM) |url=https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm |access-date=11 June 2022 |work=Stanford HAI|date=18 August 2021 }}</ref>\n\nFoundation models are [[General-purpose technology|general-purpose technologies]] that can support a diverse range of use cases. Building foundation models is often highly resource-intensive, with the most expensive models costing hundreds of millions of dollars to pay for the underlying data and compute.<ref>Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, \u201cThe AI Index 2023 Annual Report,\u201d AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023. </ref> Early examples of foundation models were pre-trained [[language models]] (LMs) like [[Google]]'s [[BERT (language model)|BERT]]<ref>{{cite arXiv |title=A Primer in BERTology: What we know about how BERT works |first1=Anna |last1=Rogers |first2=Olga |last2=Kovaleva |first3=Anna |last3=Rumshisky |year=2020 |class=cs.CL |eprint=2002.12327 }}</ref> and [[OpenAI]]'s '''\"GPT-n\"''' series. Beyond text, foundation models have been developed across a range of modalities{{mdash}}including [[DALL-E]] and Flamingo<ref name=\"deepmind_20220428\">{{citation| title = Tackling multiple tasks with a single visual language model| access-date = 13 June 2022 |date=28 April 2022 | url = https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model}}</ref> for images, MusicGen<ref>{{cite arXiv |last1=Copet |first1=Jade |title=Simple and Controllable Music Generation |date=2023-11-07 |eprint=2306.05284 |last2=Kreuk |first2=Felix |last3=Gat |first3=Itai |last4=Remez |first4=Tal |last5=Kant |first5=David |last6=Synnaeve |first6=Gabriel |last7=Adi |first7=Yossi |last8=D\u00e9fossez |first8=Alexandre|class=cs.SD }}</ref> for music, and RT-2<ref>{{Cite web |date=2023-07-28 |title=Speaking robot: Our new AI model translates vision and language into robotic actions |url=https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/ |access-date=2023-12-11 |website=Google |language=en-us}}</ref> for robotic control. Foundation models constitute a broad shift in AI development: foundation models are being built for astronomy,<ref>{{cite arXiv |last1=Nguyen |first1=Tuan Dung |title=AstroLLaMA: Towards Specialized Foundation Models in Astronomy |date=2023-09-12 |eprint=2309.06126 |last2=Ting |first2=Yuan-Sen |last3=Ciuc\u0103 |first3=Ioana |last4=O'Neill |first4=Charlie |last5=Sun |first5=Ze-Chang |last6=Jab\u0142o\u0144ska |first6=Maja |last7=Kruk |first7=Sandor |last8=Perkowski |first8=Ernest |last9=Miller |first9=Jack|class=astro-ph.IM }}</ref> radiology,<ref>{{cite arXiv |last1=Tu |first1=Tao |title=Towards Generalist Biomedical AI |date=2023-07-26 |eprint=2307.14334 |last2=Azizi |first2=Shekoofeh |last3=Driess |first3=Danny |last4=Schaekermann |first4=Mike |last5=Amin |first5=Mohamed |last6=Chang |first6=Pi-Chuan |last7=Carroll |first7=Andrew |last8=Lau |first8=Chuck |last9=Tanno |first9=Ryutaro|class=cs.CL }}</ref> robotics,<ref>{{cite arXiv |last1=Ahn |first1=Michael |title=Do As I Can, Not As I Say: Grounding Language in Robotic Affordances |date=2022-08-16 |eprint=2204.01691 |last2=Brohan |first2=Anthony |last3=Brown |first3=Noah |last4=Chebotar |first4=Yevgen |last5=Cortes |first5=Omar |last6=David |first6=Byron |last7=Finn |first7=Chelsea |last8=Fu |first8=Chuyuan |last9=Gopalakrishnan |first9=Keerthana|class=cs.RO }}</ref> genomics,<ref>{{cite bioRxiv |last1=Zvyagin |first1=Maxim |title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics |date=2022-10-11 |language=en |biorxiv=10.1101/2022.10.10.511571 |last2=Brace |first2=Alexander |last3=Hippe |first3=Kyle |last4=Deng |first4=Yuntian |last5=Zhang |first5=Bin |last6=Bohorquez |first6=Cindy Orozco |last7=Clyde |first7=Austin |last8=Kale |first8=Bharat |last9=Perez-Rivera |first9=Danilo}}</ref> music,<ref>{{Cite web |last=Engineering |first=Spotify |date=2023-10-13 |title=LLark: A Multimodal Foundation Model for Music |url=https://research.atspotify.com/2023/10/llark-a-multimodal-foundation-model-for-music/ |access-date=2023-12-11 |website=Spotify Research |language=en-US}}</ref> coding,<ref>{{cite arXiv |last1=Li |first1=Raymond |title=StarCoder: may the source be with you! |date=2023-05-09 |eprint=2305.06161 |last2=Allal |first2=Loubna Ben |last3=Zi |first3=Yangtian |last4=Muennighoff |first4=Niklas |last5=Kocetkov |first5=Denis |last6=Mou |first6=Chenghao |last7=Marone |first7=Marc |last8=Akiki |first8=Christopher |last9=Li |first9=Jia|class=cs.CL }}</ref> and mathematics.<ref>{{cite arXiv |last1=Azerbayev |first1=Zhangir |title=Llemma: An Open Language Model For Mathematics |date=2023-11-30 |eprint=2310.10631 |last2=Schoelkopf |first2=Hailey |last3=Paster |first3=Keiran |last4=Santos |first4=Marco Dos |last5=McAleer |first5=Stephen |last6=Jiang |first6=Albert Q. |last7=Deng |first7=Jia |last8=Biderman |first8=Stella |last9=Welleck |first9=Sean|class=cs.CL }}</ref>\n==Definitions==\nThe Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term \"foundation model\" in August 2021 to mean \"any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\".<ref name=\"Bommasani_20210818\" /> This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that \"'(large) language model' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.\"<ref>{{cite web | title=Reflections on Foundation Models | website=Stanford HAI | date=18 October 2021 | url=https://hai.stanford.edu/news/reflections-foundation-models | access-date=22 May 2023}}</ref> After considering many terms, they settled on \"foundation model\" to emphasize the intended ''function'' (i.e., amenability to subsequent further development) rather than [[Modality (human\u2013computer interaction)|modality]], architecture, or implementation. The term \u201cfoundation model\u201d was chosen over \u201cfoundational model\u201d<ref>{{Cite web |last1=Bommasani |first1=Rishi |last2=Liang |first2=Percy |date=2021-10-18 |title=Reflections on Foundation Models |url=https://crfm.stanford.edu/2021/10/18/reflections.html |access-date=2023-12-11 |website=Stanford CRFM}}</ref> because \u201cfoundational\u201d implies that these models provide fundamental principles in a way that \u201cfoundation\u201d does not.<ref>{{Cite web |last=Marcus |first=Gary |date=2021-09-11 |title=Has AI found a new Foundation? |url=https://thegradient.pub/has-ai-found-a-new-foundation/ |access-date=2023-12-11 |website=The Gradient |language=en}}</ref>\n\nAs governments regulate foundation models, new legal definitions have emerged. \n\n* In the United States, the ''[[Executive Order 14110|Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence]]'' defines a foundation model as \u201can AI model that is trained on broad data; generally uses [[Self-supervised learning|self-supervision]]; contains at least tens of billions of parameters; is applicable across a wide range of contexts\u201d. \n* In the European Union, the [[European Parliament]]\u2019s negotiated position on the [[Artificial Intelligence Act|E.U. AI Act]] defines a foundation model as an \u201cAI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks\u201d. \n* In the United Kingdom, the [[Competition and Markets Authority]]\u2019s ''AI Foundation Models: Initial Report'' <ref name=\":1\" /> defines a foundation model as \u201ca type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations.\u201d \n\nOverall, while many of these definitions stick close to the original Stanford CRFM definition, they do introduce some subtle distinctions. For example, the U.S. definition is the sole definition to make reference to the size of a foundation model. In contrast, the E.U. definition includes mention of whether the model is designed for generality of output. Nonetheless, all definitions share that foundation models must be trained on a broad range of data with potential applications in many domains.\n\n==Personalizing foundation models==\nSince foundation models are pre-trained on a massive dataset, they are not capable of handling specific \"personal\" concepts that a user may be interested in. A series of methods were designed to augment a foundation model with personal, specific items without retraining the full model. For example, for [[One-shot learning (computer vision)|few-shot]] [[image retrieval]] it was shown how to adapt a vision-language foundation model (CLIP) by adding new concept to its vocabulary.<ref>{{Cite book |last1=Cohen |first1=Niv |last2=Gal |first2=Rinon |last3=Meirom |first3=Eli A. |last4=Chechik |first4=Gal |last5=Atzmon |first5=Yuval |title=Computer Vision \u2013 ECCV 2022 |chapter=\"This is My Unicorn, Fluffy\": Personalizing Frozen Vision-Language Representations |date=2022-10-23 |chapter-url=https://doi.org/10.1007/978-3-031-20044-1_32 |series=Lecture Notes in Computer Science |volume=13680 |location=Berlin, Heidelberg |publisher=Springer-Verlag |pages=558\u2013577 |doi=10.1007/978-3-031-20044-1_32 |arxiv=2204.01694 |isbn=978-3-031-20043-4}}</ref>  For [[text-to-image generation]], an approach called textual inversion<ref>{{Cite arXiv |last1=Gal |first1=Rinon |last2=Alaluf |first2=Yuval |last3=Atzmon |first3=Yuval |last4=Patashnik |first4=Or |last5=Bermano |first5=Amit H. |last6=Chechik |first6=Gal |last7=Cohen-Or |first7=Daniel |date=2022-08-02 |title=An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion |class=cs.CV |eprint=2208.01618 }}</ref> can be similarly used to teach the system new concept that can later be generated in conjunction with the concepts that the foundation model is already familiar with.\n\n==Opportunities and risks==\nA 2021 arXiv report listed foundation models' capabilities in regards to \"language, vision, robotics, reasoning, and human interaction\", technical principles, such as \"model architectures, training procedures, data, systems, security, evaluation, and theory\", their applications, for example in law, healthcare, and education and their potential impact on society, including \"inequity, misuse, economic and environmental impact, legal and ethical considerations\".<ref name=\"Bommasani_20210818\">{{Cite report| last1 = Bommasani| first1 = Rishi| last2 = Hudson| first2 = Drew A.| last3 = Adeli| first3 = Ehsan| last4 = Altman| first4 = Russ| last5 = Arora| first5 = Simran| last6 = von Arx| first6 = Sydney| last7 = Bernstein| first7 = Michael S.| last8 = Bohg| first8 = Jeannette| last9 = Bosselut| first9 = Antoine| last10 = Brunskill| first10 = Emma| last11 = Brynjolfsson| first11 = Erik| last12 = Buch| first12 = Shyamal| last13 = Card| first13 = Dallas| last14 = Castellon| first14 = Rodrigo| last15 = Chatterji| first15 = Niladri| last16 = Chen| first16 = Annie| last17 = Creel| first17 = Kathleen| last18 = Davis| first18 = Jared Quincy| last19 = Demszky| first19 = Dora| last20 = Donahue| first20 = Chris| last21 = Doumbouya| first21 = Moussa| last22 = Durmus| first22 = Esin| last23 = Ermon| first23 = Stefano| last24 = Etchemendy| first24 = John| last25 = Ethayarajh| first25 = Kawin| last26 = Fei-Fei| first26 = Li| last27 = Finn| first27 = Chelsea| last28 = Gale| first28 = Trevor| last29 = Gillespie| first29 = Lauren| last30 = Goel| first30 = Karan| last31 = Goodman| first31 = Noah| last32 = Grossman| first32 = Shelby| last33 = Guha| first33 = Neel| last34 = Hashimoto| first34 = Tatsunori| last35 = Henderson| first35 = Peter| last36 = Hewitt| first36 = John| last37 = Ho| first37 = Daniel E.| last38 = Hong| first38 = Jenny| last39 = Hsu| first39 = Kyle| last40 = Huang| first40 = Jing| last41 = Icard| first41 = Thomas| last42 = Jain| first42 = Saahil| last43 = Jurafsky| first43 = Dan| last44 = Kalluri| first44 = Pratyusha| last45 = Karamcheti| first45 = Siddharth| last46 = Keeling| first46 = Geoff| last47 = Khani| first47 = Fereshte| last48 = Khattab| first48 = Omar| last49 = Koh| first49 = Pang Wei| last50 = Krass| first50 = Mark| last51 = Krishna| first51 = Ranjay| last52 = Kuditipudi| first52 = Rohith| last53 = Kumar| first53 = Ananya| last54 = Ladhak| first54 = Faisal| last55 = Lee| first55 = Mina| last56 = Lee| first56 = Tony| last57 = Leskovec| first57 = Jure| last58 = Levent| first58 = Isabelle| last59 = Li| first59 = Xiang Lisa| last60 = Li| first60 = Xuechen| last61 = Ma| first61 = Tengyu| last62 = Malik| first62 = Ali| last63 = Manning| first63 = Christopher D.| last64 = Mirchandani| first64 = Suvir| last65 = Mitchell| first65 = Eric| last66 = Munyikwa| first66 = Zanele| last67 = Nair| first67 = Suraj| last68 = Narayan| first68 = Avanika| last69 = Narayanan| first69 = Deepak| last70 = Newman| first70 = Ben| last71 = Nie| first71 = Allen| last72 = Niebles| first72 = Juan Carlos| last73 = Nilforoshan| first73 = Hamed| last74 = Nyarko| first74 = Julian| last75 = Ogut| first75 = Giray| last76 = Orr| first76 = Laurel| last77 = Papadimitriou| first77 = Isabel| last78 = Park| first78 = Joon Sung| last79 = Piech| first79 = Chris| last80 = Portelance| first80 = Eva| last81 = Potts| first81 = Christopher| last82 = Raghunathan| first82 = Aditi| last83 = Reich| first83 = Rob| last84 = Ren| first84 = Hongyu| last85 = Rong| first85 = Frieda| last86 = Roohani| first86 = Yusuf| last87 = Ruiz| first87 = Camilo| last88 = Ryan| first88 = Jack| last89 = R\u00e9| first89 = Christopher| last90 = Sadigh| first90 = Dorsa| last91 = Sagawa| first91 = Shiori| last92 = Santhanam| first92 = Keshav| last93 = Shih| first93 = Andy| last94 = Srinivasan| first94 = Krishnan| last95 = Tamkin| first95 = Alex| last96 = Taori| first96 = Rohan| last97 = Thomas| first97 = Armin W.| last98 = Tram\u00e8r| first98 = Florian| last99 = Wang| first99 = Rose E.| last100 = Wang| first100 = William| last101 = Wu| first101 = Bohan| last102 = Wu| first102 = Jiajun| last103 = Wu| first103 = Yuhuai| last104 = Xie| first104 = Sang Michael| last105 = Yasunaga| first105 = Michihiro| last106 = You| first106 = Jiaxuan| last107 = Zaharia| first107 = Matei| last108 = Zhang| first108 = Michael| last109 = Zhang| first109 = Tianyi| last110 = Zhang| first110 = Xikun| last111 = Zhang| first111 = Yuhui| last112 = Zheng| first112 = Lucia| last113 = Zhou| first113 = Kaitlyn| last114 = Liang| first114 = Percy| title = On the Opportunities and Risks of Foundation Models| date = 18 August 2021 | arxiv = 2108.07258}}</ref>\n\nAn article about foundation models in ''The Economist'' notes that \"some worry that the technology's heedless spread will further concentrate economic and political power\".<ref name=\":0\">{{Cite news |title=Huge \"foundation models\" are turbo-charging AI progress |newspaper=The Economist |url=https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress |access-date=2022-10-24 |issn=0013-0613}}</ref>\n\n==References==\n{{reflist}}\n\n{{Natural Language Processing}}\n{{Differentiable computing}}\n{{Existential risk from artificial intelligence}}\n\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Computational fields of study]]\n[[Category:Language modeling]]\n[[Category:Unsupervised learning]]\n[[Category:Deep learning]]"}