{"title": "Foundation model", "page_id": 70984276, "revision_id": 1265815220, "revision_timestamp": "2024-12-28T20:16:57Z", "content": "{{Short description|Artificial intelligence model paradigm}}\n{{Use dmy dates|date=May 2023}}\nA '''foundation model''', also known as '''large X model (LxM)''', is a [[machine learning]] or [[deep learning]] model that is trained on vast datasets so it can be applied across a wide range of use cases.<ref name=\":1\">Competition and Markets Authority (2023). ''AI Foundation Models: Initial Report''. Available at: https://assets.publishing.service.gov.uk/media/65081d3aa41cc300145612c0/Full_report_.pdf</ref> [[Generative artificial intelligence|Generative AI]] applications like [[Large language model|Large Language Models]] are often examples of foundation models.<ref name=\":1\" />\n\nBuilding foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training.<ref>Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, \"The AI Index 2023 Annual Report,\" AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023.</ref> These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs. In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets.\n\nEarly examples of foundation models are [[language models]] (LMs) like [[Generative pre-trained transformer|OpenAI's GPT]] series and [[Google]]'s [[BERT (language model)|BERT]].<ref>{{cite arXiv |eprint=2002.12327 |class=cs.CL |first1=Anna |last1=Rogers |first2=Olga |last2=Kovaleva |title=A Primer in BERTology: What we know about how BERT works |first3=Anna |last3=Rumshisky |year=2020}}</ref><ref>{{Cite web |last=Haddad |first=Mohammed |title=How does GPT-4 work and how can you start using it in ChatGPT? |url=https://www.aljazeera.com/news/2023/3/15/how-do-ai-models-like-gpt-4-work-and-how-can-you-start-using-it |access-date=2024-10-20 |website=Al Jazeera |language=en}}</ref> Beyond text, foundation models have been developed across a range of modalities{{mdash}}including [[DALL-E]] and Flamingo<ref name=\"deepmind_20220428\">{{citation| title = Tackling multiple tasks with a single visual language model| access-date = 13 June 2022 |date=28 April 2022 | url = https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model}}</ref> for images, MusicGen<ref>{{cite arXiv |last1=Copet |first1=Jade |title=Simple and Controllable Music Generation |date=2023-11-07 |eprint=2306.05284 |last2=Kreuk |first2=Felix |last3=Gat |first3=Itai |last4=Remez |first4=Tal |last5=Kant |first5=David |last6=Synnaeve |first6=Gabriel |last7=Adi |first7=Yossi |last8=D\u00e9fossez |first8=Alexandre|class=cs.SD }}</ref> for music, and RT-2<ref>{{Cite web |date=2023-07-28 |title=Speaking robot: Our new AI model translates vision and language into robotic actions |url=https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/ |access-date=2023-12-11 |website=Google |language=en-us}}</ref> for robotic control. Foundation models are also being developed for fields like astronomy,<ref>{{cite arXiv |last1=Nguyen |first1=Tuan Dung |title=AstroLLaMA: Towards Specialized Foundation Models in Astronomy |date=2023-09-12 |eprint=2309.06126 |last2=Ting |first2=Yuan-Sen |last3=Ciuc\u0103 |first3=Ioana |last4=O'Neill |first4=Charlie |last5=Sun |first5=Ze-Chang |last6=Jab\u0142o\u0144ska |first6=Maja |last7=Kruk |first7=Sandor |last8=Perkowski |first8=Ernest |last9=Miller |first9=Jack|class=astro-ph.IM }}</ref> radiology,<ref>{{cite arXiv |last1=Tu |first1=Tao |title=Towards Generalist Biomedical AI |date=2023-07-26 |eprint=2307.14334 |last2=Azizi |first2=Shekoofeh |last3=Driess |first3=Danny |last4=Schaekermann |first4=Mike |last5=Amin |first5=Mohamed |last6=Chang |first6=Pi-Chuan |last7=Carroll |first7=Andrew |last8=Lau |first8=Chuck |last9=Tanno |first9=Ryutaro|class=cs.CL }}</ref> genomics,<ref>{{cite bioRxiv |last1=Zvyagin |first1=Maxim |title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics |date=2022-10-11 |language=en |biorxiv=10.1101/2022.10.10.511571 |last2=Brace |first2=Alexander |last3=Hippe |first3=Kyle |last4=Deng |first4=Yuntian |last5=Zhang |first5=Bin |last6=Bohorquez |first6=Cindy Orozco |last7=Clyde |first7=Austin |last8=Kale |first8=Bharat |last9=Perez-Rivera |first9=Danilo}}</ref> music,<ref>{{Cite web |last=Engineering |first=Spotify |date=2023-10-13 |title=LLark: A Multimodal Foundation Model for Music |url=https://research.atspotify.com/2023/10/llark-a-multimodal-foundation-model-for-music/ |access-date=2023-12-11 |website=Spotify Research |language=en-US}}</ref> coding,<ref>{{cite arXiv |last1=Li |first1=Raymond |title=StarCoder: may the source be with you! |date=2023-05-09 |eprint=2305.06161 |last2=Allal |first2=Loubna Ben |last3=Zi |first3=Yangtian |last4=Muennighoff |first4=Niklas |last5=Kocetkov |first5=Denis |last6=Mou |first6=Chenghao |last7=Marone |first7=Marc |last8=Akiki |first8=Christopher |last9=Li |first9=Jia|class=cs.CL }}</ref> [[Time series|times-series]] forecasting,<ref>{{Cite web |last1=Se |first1=Ksenia |last2=Spektor |first2=Ian |date=April 5, 2024 |title=Revolutionizing Time Series Forecasting: Interview with TimeGPT's creators |url=https://www.turingpost.com/p/timegpt |access-date=2024-04-11 |website=Turing Post |language=en}}</ref> mathematics,<ref>{{cite arXiv |last1=Azerbayev |first1=Zhangir |title=Llemma: An Open Language Model For Mathematics |date=2023-11-30 |eprint=2310.10631 |last2=Schoelkopf |first2=Hailey |last3=Paster |first3=Keiran |last4=Santos |first4=Marco Dos |last5=McAleer |first5=Stephen |last6=Jiang |first6=Albert Q. |last7=Deng |first7=Jia |last8=Biderman |first8=Stella |last9=Welleck |first9=Sean|class=cs.CL }}</ref> and chemistry.<ref>{{cite web | url=https://www.orbitalmaterials.com/post/technical-blog-introducing-the-orb-ai-based-interatomic-potential | title=Orbital }}</ref>\n\n==Definitions==\nThe Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term \"foundation model\" in August 2021<ref name=\"CRFM\">{{Cite web |date=18 August 2021 |title=Introducing the Center for Research on Foundation Models (CRFM) |url=https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm |access-date=11 June 2022 |work=Stanford HAI}}</ref> to mean \"any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\".<ref name=\"Bommasani_20210818\">{{Cite report |title=On the Opportunities and Risks of Foundation Models |last1=Bommasani |first1=Rishi |last2=Hudson |first2=Drew A. |date=18 August 2021 |arxiv=2108.07258 |last3=Adeli |first3=Ehsan |last4=Altman |first4=Russ |last5=Arora |first5=Simran |last6=von Arx |first6=Sydney |last7=Bernstein |first7=Michael S. |last8=Bohg |first8=Jeannette |last9=Bosselut |first9=Antoine |display-authors=1 |last10=Brunskill |first10=Emma |last11=Brynjolfsson |first11=Erik |last12=Buch |first12=Shyamal |last13=Card |first13=Dallas |last14=Castellon |first14=Rodrigo |last15=Chatterji |first15=Niladri |last16=Chen |first16=Annie |last17=Creel |first17=Kathleen |last18=Davis |first18=Jared Quincy |last19=Demszky |first19=Dora |last20=Donahue |first20=Chris |last21=Doumbouya |first21=Moussa |last22=Durmus |first22=Esin |last23=Ermon |first23=Stefano |last24=Etchemendy |first24=John |last25=Ethayarajh |first25=Kawin |last26=Fei-Fei |first26=Li |last27=Finn |first27=Chelsea |last28=Gale |first28=Trevor |last29=Gillespie |first29=Lauren |last30=Goel |first30=Karan |last31=Goodman |first31=Noah |last32=Grossman |first32=Shelby |last33=Guha |first33=Neel |last34=Hashimoto |first34=Tatsunori |last35=Henderson |first35=Peter |last36=Hewitt |first36=John |last37=Ho |first37=Daniel E. |last38=Hong |first38=Jenny |last39=Hsu |first39=Kyle |last40=Huang |first40=Jing |last41=Icard |first41=Thomas |last42=Jain |first42=Saahil |last43=Jurafsky |first43=Dan |last44=Kalluri |first44=Pratyusha |last45=Karamcheti |first45=Siddharth |last46=Keeling |first46=Geoff |last47=Khani |first47=Fereshte |last48=Khattab |first48=Omar |last49=Koh |first49=Pang Wei |last50=Krass |first50=Mark |last51=Krishna |first51=Ranjay |last52=Kuditipudi |first52=Rohith |last53=Kumar |first53=Ananya |last54=Ladhak |first54=Faisal |last55=Lee |first55=Mina |last56=Lee |first56=Tony |last57=Leskovec |first57=Jure |last58=Levent |first58=Isabelle |last59=Li |first59=Xiang Lisa |last60=Li |first60=Xuechen |last61=Ma |first61=Tengyu |last62=Malik |first62=Ali |last63=Manning |first63=Christopher D. |last64=Mirchandani |first64=Suvir |last65=Mitchell |first65=Eric |last66=Munyikwa |first66=Zanele |last67=Nair |first67=Suraj |last68=Narayan |first68=Avanika |last69=Narayanan |first69=Deepak |last70=Newman |first70=Ben |last71=Nie |first71=Allen |last72=Niebles |first72=Juan Carlos |last73=Nilforoshan |first73=Hamed |last74=Nyarko |first74=Julian |last75=Ogut |first75=Giray |last76=Orr |first76=Laurel |last77=Papadimitriou |first77=Isabel |last78=Park |first78=Joon Sung |last79=Piech |first79=Chris |last80=Portelance |first80=Eva |last81=Potts |first81=Christopher |last82=Raghunathan |first82=Aditi |last83=Reich |first83=Rob |last84=Ren |first84=Hongyu |last85=Rong |first85=Frieda |last86=Roohani |first86=Yusuf |last87=Ruiz |first87=Camilo |last88=Ryan |first88=Jack |last89=R\u00e9 |first89=Christopher |last90=Sadigh |first90=Dorsa |last91=Sagawa |first91=Shiori |last92=Santhanam |first92=Keshav |last93=Shih |first93=Andy |last94=Srinivasan |first94=Krishnan |last95=Tamkin |first95=Alex |last96=Taori |first96=Rohan |last97=Thomas |first97=Armin W. |last98=Tram\u00e8r |first98=Florian |last99=Wang |first99=Rose E. |last100=Wang |first100=William |last101=Wu |first101=Bohan |last102=Wu |first102=Jiajun |last103=Wu |first103=Yuhuai |last104=Xie |first104=Sang Michael |last105=Yasunaga |first105=Michihiro |last106=You |first106=Jiaxuan |last107=Zaharia |first107=Matei |last108=Zhang |first108=Michael |last109=Zhang |first109=Tianyi |last110=Zhang |first110=Xikun |last111=Zhang |first111=Yuhui |last112=Zheng |first112=Lucia |last113=Zhou |first113=Kaitlyn |last114=Liang |first114=Percy}}</ref> This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that \"'[[Large language model|(large) language model]]' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.\"<ref>{{cite web | title=Reflections on Foundation Models | website=Stanford HAI | date=18 October 2021 | url=https://hai.stanford.edu/news/reflections-foundation-models | access-date=22 May 2023}}</ref> The term \"foundation model\" was chosen over \"foundational model\"<ref>{{Cite web |last1=Bommasani |first1=Rishi |last2=Liang |first2=Percy |date=2021-10-18 |title=Reflections on Foundation Models |url=https://crfm.stanford.edu/2021/10/18/reflections.html |access-date=2023-12-11 |website=Stanford CRFM}}</ref> because \"foundational\" implies that these models provide fundamental principles in a way that \"foundation\" does not.<ref>{{Cite web |last=Marcus |first=Gary |date=2021-09-11 |title=Has AI found a new Foundation? |url=https://thegradient.pub/has-ai-found-a-new-foundation/ |access-date=2023-12-11 |website=The Gradient |language=en}}</ref>\n\nAs governments regulate foundation models, new legal definitions have emerged.\n* In the United States, the ''[[Executive Order 14110|Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence]]'' defines a foundation model as \"an AI model that is trained on broad data; generally uses [[Self-supervised learning|self-supervision]]; contains at least tens of billions of parameters; is applicable across a wide range of contexts\".<ref>{{Cite web |last=House |first=The White |date=2023-10-30 |title=Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence |url=https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ |access-date=2024-02-12 |website=The White House |language=en-US}}</ref>\n* In the United States, the proposed AI Foundation Model Transparency Act of 2023<ref>{{Cite web |title=AI Foundation Model Transparency Act |url=https://beyer.house.gov/uploadedfiles/one-pager_ai_foundation_model_transparency_act_.pdf}}</ref> by House Representatives [[Don Beyer]] (D, VA) and [[Anna Eshoo]] (D, CA) defines a foundation model as \"an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters.\"\n* In the European Union, the [[European Parliament]]'s negotiated position on the [[Artificial Intelligence Act|E.U. AI Act]] defines a foundation model as an \"AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks\".\n* In the United Kingdom, the [[Competition and Markets Authority]]'s ''AI Foundation Models: Initial Report'' <ref name=\":1\" /> defines foundations model as \"a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations.\"\n\nThe United States's definitions are the only ones to make reference to the size of a foundation model, and differ on magnitude. Beyer and Eshoo's definition also specifies that foundation models must achieve a level of performance as to be a potential danger. In contrast, the E.U. definition requires the model to be designed for generality of output. All definitions agree that foundation models must be trained on a broad range of data with potential applications in many domains.\n\n==History==\nTechnologically, foundation models are built using established machine learning techniques like [[deep neural networks]], [[transfer learning]], and [[self-supervised learning]]. Foundation models differ from previous techniques as they are general purpose models function as a reusable infrastructure, instead of bespoke and one-off task-specific models.\n\nAdvances in computer parallelism (e.g., [[CUDA|CUDA GPUs]]) and new developments in neural network architecture (e.g., [[Transformer (machine learning model)|Transformers]]), and the increased use of training data with minimal supervision all contributed to the rise of foundation models. Foundation models began to materialize as the latest wave of deep learning models in the late 2010s.<ref name=\":2\">{{Citation |last1=Liang |first1=Percy |title=Holistic Evaluation of Language Models |date=2023-10-01 |journal=Annals of the New York Academy of Sciences |volume=1525 |issue=1 |pages=140\u2013146 |arxiv=2211.09110 |bibcode=2023NYASA1525..140B |doi=10.1111/nyas.15007 |pmid=37230490 |last2=Bommasani |first2=Rishi |last3=Lee |first3=Tony |last4=Tsipras |first4=Dimitris |last5=Soylu |first5=Dilara |last6=Yasunaga |first6=Michihiro |last7=Zhang |first7=Yian |last8=Narayanan |first8=Deepak |last9=Wu |first9=Yuhuai}}</ref> Relative to most prior work on deep learning, these language models demonstrated the potential of training on much larger web-sourced datasets using self-supervised objectives (e.g. predicting the next word in a large corpus of text). These approaches, which draw upon earlier works like [[word2vec]] and [[GloVe]], deviated from prior supervised approaches that required annotated data (e.g. crowd-sourced labels).\n\nThe 2022 releases of [[Stable Diffusion]] and [[ChatGPT]] (initially powered by the GPT-3.5 model) led to foundation models and generative AI entering widespread public discourse. Further, releases of [[LLaMA]], Llama 2, and [[Mistral AI|Mistral]] in 2023 contributed to a greater emphasis placed on how foundation models are released with open foundation models garnering a lot of support<ref>{{Cite web |date=2023-10-31 |title=Joint Statement on AI Safety and Openness |url=https://open.mozilla.org/letter/ |access-date=2024-02-12 |website=Mozilla |language=en}}</ref> and scrutiny.<ref>{{Cite web |date=2023-06-06 |title=Hawley and Blumenthal Demand Answers from Meta, Warn of Misuse After 'Leak' of Meta's AI Model |url=https://www.hawley.senate.gov/hawley-and-blumenthal-demand-answers-meta-warn-misuse-after-leak-metas-ai-model |access-date=2024-02-12 |website=Senator Josh Hawley |language=en}}</ref>\n\n==Related concepts==\n\n=== Frontier models ===\nCertain highly advanced foundation models are termed \"frontier models,\" which have the potential to \"possess dangerous capabilities sufficient to pose severe risks to public safety.\"<ref name=\":0\">{{Citation |last1=Anderljung |first1=Markus |title=Frontier AI Regulation: Managing Emerging Risks to Public Safety |date=2023-11-07 |arxiv=2307.03718 |last2=Barnhart |first2=Joslyn |last3=Korinek |first3=Anton |last4=Leung |first4=Jade |last5=O'Keefe |first5=Cullen |last6=Whittlestone |first6=Jess |last7=Avin |first7=Shahar |last8=Brundage |first8=Miles |last9=Bullock |first9=Justin |author-link4=Jade Leung (engineer)}}</ref> These \"dangerous capabilities\" stem from the accidental or intentional misuse of such models, which in conjunction with their powerful nature can lead to severe harms. As foundation models continue to improve, some AI researchers speculate that almost all next-generation foundation models will be considered frontier models.\n\nSince the concept of dangerous capabilities is inherently subjective, there is no strict designation for what foundation models qualify as frontier models. However, some generally held ideas for sufficiently dangerous capabilities include:\n* Designing and synthesizing new biological or chemical weapons<ref>{{Cite journal |last1=Singhal |first1=Karan |last2=Azizi |first2=Shekoofeh |last3=Tu |first3=Tao |last4=Mahdavi |first4=S. Sara |last5=Wei |first5=Jason |last6=Chung |first6=Hyung Won |last7=Scales |first7=Nathan |last8=Tanwani |first8=Ajay |last9=Cole-Lewis |first9=Heather |last10=Pfohl |first10=Stephen |last11=Payne |first11=Perry |last12=Seneviratne |first12=Martin |last13=Gamble |first13=Paul |last14=Kelly |first14=Chris |last15=Babiker |first15=Abubakr |date=August 2023 |title=Large language models encode clinical knowledge |journal=Nature |language=en |volume=620 |issue=7972 |pages=172\u2013180 |doi=10.1038/s41586-023-06291-2 |pmid=37438534 |pmc=10396962 |issn=1476-4687|arxiv=2212.13138 |bibcode=2023Natur.620..172S }}</ref>\n* Producing and propagating convincing, tailored disinformation with minimal user instruction<ref>{{Citation |last1=Nori |first1=Harsha |title=Capabilities of GPT-4 on Medical Challenge Problems |date=2023-04-12 |arxiv=2303.13375 |last2=King |first2=Nicholas |last3=McKinney |first3=Scott Mayer |last4=Carignan |first4=Dean |last5=Horvitz |first5=Eric}}</ref>\n* Harnessing unprecedented offensive cyber capabilities<ref>{{Cite journal |last=Simshaw |first=Drew |date=April 22, 2022 |title=Access to A.I. Justice: Avoiding an Inequitable Two-Tiered System of Legal Services |url=https://ssrn.com/abstract=4090984 |journal=SSRN Electronic Journal}}</ref>\n* Evading human control through deceptive means<ref>{{Cite journal |last1=Arbel |first1=Yonathan A. |last2=Becher |first2=Shmuel I. |date=2020 |title=Contracts in the Age of Smart Readers |journal=Geo. Wash. L. Rev. |volume=90 |page=83 |doi=10.2139/ssrn.3740356 |s2cid=229386991 |url=https://scholarship.law.ua.edu/fac_articles/695 }}</ref>\n\nDue to frontier models' unique capabilities, it is difficult to effectively regulate their development and deployment. Because of their emergent nature, new dangerous capabilities can appear on their own in frontier models, both in the development stage and after being deployed.<ref name=\":0\" /> Additionally, since frontier models continue to adapt after deployment, it remains difficult to mitigate all harms that arise from already-deployed models. If a frontier model happens to be open-source or is released online, the model can also disseminate rapidly, further hampering regulators by creating a lack of accountability.\n\n===General-purpose AI===\nDue to their adaptability to a wide range of use-cases, foundation models are sometimes considered to be examples of general-purpose AI. In designing the EU AI Act, the European Parliament has stated that a new wave of general-purpose AI technologies shapes the overall AI ecosystem.<ref>{{Cite web |title=General-purpose artificial intelligence {{!}} Think Tank {{!}} European Parliament |url=https://www.europarl.europa.eu/thinktank/en/document/EPRS_ATA(2023)745708 |access-date=2024-02-12 |website=www.europarl.europa.eu |language=en}}</ref> The fuller structure of the ecosystem, in addition to the properties of specific general-purpose AI systems, influences the design of AI policy and research.<ref>{{Citation |last1=Bommasani |first1=Rishi |title=Ecosystem Graphs: The Social Footprint of Foundation Models |date=2023-03-28 |arxiv=2303.15772 |last2=Soylu |first2=Dilara |last3=Liao |first3=Thomas I. |last4=Creel |first4=Kathleen A. |last5=Liang |first5=Percy}}</ref> General-purpose AI systems also often appear in people's everyday lives through applications and tools like [[ChatGPT]] or [[DALL-E]].\n\nGovernment agencies like EU Parliament have identified regulation general-purpose AI, such as foundation models, to be a high priority. General-purpose AI systems are often characterized by large size, opacity, and potential for emergence, all of which can create unintended harms. Such systems also heavily influence downstream applications, which further exacerbates the need for regulation. In regards to prominent legislation, a number of stakeholders have pushed for the [[Artificial Intelligence Act|EU AI Act]] to include restrictions on general-purpose AI systems, all of which would also apply to foundation models.\n\n== Technical details ==\n\n=== Modeling ===\nFor a foundation model to effectively generalize, it must acquire rich representations of the training data. As a result, expressive model architectures that efficiently process large-scale data are often preferred in building foundation models.<ref name=\"Bommasani_20210818\" /> Currently, the [[Transformer (deep learning architecture)|Transformer]] architecture is the de facto choice for building foundation models across a range of modalities.<ref>{{Citation |last1=Bommasani |first1=Rishi |title=The Foundation Model Transparency Index |date=2023-10-19 |arxiv=2310.12941 |last2=Klyman |first2=Kevin |last3=Longpre |first3=Shayne |last4=Kapoor |first4=Sayash |last5=Maslej |first5=Nestor |last6=Xiong |first6=Betty |last7=Zhang |first7=Daniel |last8=Liang |first8=Percy}}</ref>\n\n=== Training ===\nFoundation models are built by optimizing a training objective(s), which is a mathematical function that determines how model parameters are updated based on model predictions on training data.<ref>{{Cite journal |last=Claude Elwood |first=Shannon |date=July 1948 |title=A Mathematical Theory of Communication |url=https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf |journal=Bell System Technical Journal}}</ref> Language models are often trained with a next-tokens prediction objective, which refers to the extent at which the model is able to predict the next token in a sequence. Image models are commonly trained with contrastive learning or diffusion training objectives. For contrastive learning, images are randomly augmented before being evaluated on the resulting similarity of the model's representations. For diffusion models, images are noised and the model learns to gradually de-noise via the objective. Multimodal training objectives also exist, with some separating images and text during training, while others examine them concurrently.<ref>{{Citation |last1=Radford |first1=Alec |title=Learning Transferable Visual Models From Natural Language Supervision |date=2021-02-26 |arxiv=2103.00020 |last2=Kim |first2=Jong Wook |last3=Hallacy |first3=Chris |last4=Ramesh |first4=Aditya |last5=Goh |first5=Gabriel |last6=Agarwal |first6=Sandhini |last7=Sastry |first7=Girish |last8=Askell |first8=Amanda |last9=Mishkin |first9=Pamela}}</ref> In general, the training objectives for foundation models promote the learning of broadly useful representations of data.\n\nWith the rise of foundation models and the larger datasets that power them, a training objective must be able to parse through internet-scale data for meaningful data points. Additionally, since foundation models are designed to solve a general range of tasks, training objectives ought to be ''domain complete'', or able to solve a broad set of downstream capabilities within the given domain. Lastly, foundation model training objectives should seek to scale well and be computationally efficient. With model size and compute power both being relevant constraints, a training objective must be able to overcome such bottlenecks.\n\n=== Data ===\nFoundation models are trained on a large quantity of data, working under the maxim \"the more data, the better.\"<ref>{{Citation |last1=Kaplan |first1=Jared |title=Scaling Laws for Neural Language Models |date=2020-01-22 |arxiv=2001.08361 |last2=McCandlish |first2=Sam |last3=Henighan |first3=Tom |last4=Brown |first4=Tom B. |last5=Chess |first5=Benjamin |last6=Child |first6=Rewon |last7=Gray |first7=Scott |last8=Radford |first8=Alec |last9=Wu |first9=Jeffrey}}</ref> Performance evaluation does show that more data generally leads to better performance, but other issues arise as data quantity grows. Tasks like managing the dataset, integrating data across new applications, ensuring adherence to data licenses, and maintaining data quality all become more difficult as data size grows. The specific demands of foundation models have only exacerbated such issues, as it remains the norm for large foundation models to use public web-scraped data. Foundation models include also search engines data and SEO meta tags data. Public web data remains a plentiful resource, but it also demands stringent moderation and data processing from foundation model developers before it can be successfully integrated into the training pipeline.<ref>{{Cite book |last1=Jo |first1=Eun Seo |last2=Gebru |first2=Timnit |chapter=Lessons from archives: Strategies for collecting sociocultural data in machine learning |date=2020-01-27 |title=Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency |pages=306\u2013316 |doi=10.1145/3351095.3372829|arxiv=1912.10389 |isbn=978-1-4503-6936-7 }}</ref>\n\nTraining foundation models often runs the risk of violating user privacy, as private data can be disclosed, collected, or used in ways beyond the stated scope. Even if no private data is leaked, models can still inadvertently compromise security through learned behavior in the resulting foundation model.<ref>{{Cite book |last1=Bender |first1=Emily M. |last2=Gebru |first2=Timnit |last3=McMillan-Major |first3=Angelina |last4=Shmitchell |first4=Shmargaret |chapter=On the Dangers of Stochastic Parrots: Can Language Models be Too Big? \ud83e\udd9c |date=2021-03-01 |title=Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency |chapter-url=https://dl.acm.org/doi/10.1145/3442188.3445922 |series=FAccT '21 |location=New York, NY, USA |publisher=Association for Computing Machinery |pages=610\u2013623 |doi=10.1145/3442188.3445922 |isbn=978-1-4503-8309-7}}</ref> Data quality is another key point, as web-scraped data frequently contains biased, duplicate, and toxic material. Once foundation models are deployed, ensuring high-quality data is still an issue, as undesirable behavior can still emerge from small subsets of data.\n\n=== Systems ===\nThe size of foundation models also brings about issues with the computer systems they run on. The average foundation model is too large to be run within a single accelerator's memory and the initial training process requires an expensive amount of resources.<ref>{{Citation |last1=Brown |first1=Tom B. |title=Language Models are Few-Shot Learners |date=2020-07-22 |arxiv=2005.14165 |last2=Mann |first2=Benjamin |last3=Ryder |first3=Nick |last4=Subbiah |first4=Melanie |last5=Kaplan |first5=Jared |last6=Dhariwal |first6=Prafulla |last7=Neelakantan |first7=Arvind |last8=Shyam |first8=Pranav |last9=Sastry |first9=Girish}}</ref> Such issues are predicted to further exacerbate in future as foundation models grow to new heights. Due to this constraint, researchers have begun looking into compressing model size through tight model inference.\n\nGPUs are the most common choice of compute hardware for machine learning, due to high memory storage and strong power. Typical foundation model training requires many GPUs, all connected in parallel with fast interconnects. Acquiring a sufficient amount of GPUs of requisite compute efficiency is a challenge for many foundation model developers, one that has led to an increasing dilemma in the field. Larger models require greater compute power, but often at the cost of improved compute efficiency. Since training remains time-consuming and expensive, the tradeoff between compute power and compute efficiency has led only a few select companies to afford the production costs for large, state of the art foundation models. Some techniques like compression and distillation can make inference more affordable, but they fail to completely shore up this weakness.\n\n===Scaling===\nThe accuracy and capabilities of foundation models often scale predictably with the size of the model and the amount of the training data. Specifically, scaling laws have been discovered, which are data-based empirical trends that relate resources (data, model size, compute usage) to model capabilities. Particularly, a model's scale is defined by compute, dataset size, and the number of parameters, all of which exhibit a power-law relationship with end performance.\n\nHowever, [[Broken Neural Scaling Law|broken scaling laws]]<ref>Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). [[arxiv:2210.14891|\"Broken Neural Scaling Laws\"]]. International Conference on Learning Representations (ICLR), 2023.</ref> have been discovered in which this relationship smoothly transitions (at points referred to as [[Broken Neural Scaling Law|break(s)]]) from a power law with one exponent to a power law with another (different) exponent. When one does not collect any points near (or after) the break(s), it can be difficult to obtain an accurate extrapolation.\n\n=== Adaptation ===\nFoundation models are inherently multi-purpose: to use these model for a specific use case requires some form of adaptation. At a minimum, models need to be adapted to perform the task of interest (task specification), but often better performance can be achieved by more extensive adaptation to the domain of interest (domain specialization).\n\nA variety of methods (e.g. [[Prompt engineering|prompting]], [[in-context learning]], [[Fine-tuning (deep learning)|fine-tuning]], [[LoRA]]) provide different tradeoffs between the costs of adaptation and the extent to which models are specialized. Some major facets to consider when adapting a foundation model are compute budget and data availability. Foundation models can be very large, up to trillions of parameters in size, so adapting the entirety of a foundation model can be computationally expensive. Therefore, developers sometimes adapt only the last neural layer or only the bias vectors to save time and space.<ref>{{Citation |last1=Zaken |first1=Elad Ben |title=BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models |date=2022-09-05 |arxiv=2106.10199 |last2=Ravfogel |first2=Shauli |last3=Goldberg |first3=Yoav}}</ref> For particularly niche applications, specific data may also not be available to adapt the foundation model sufficiently. In such circumstances, data must be manually labeled, which is costly and can demand expert knowledge.\n\n=== Evaluation ===\nEvaluation is a key part of developing foundation models. Not only does evaluation allow for tracking progress of high-performance models, it also creates benchmarks for future model development. Stakeholders rely on evaluations to understand model behaviors and gain insight into their various attributes. Traditionally, foundation models are evaluated relative to each other through standardized task benchmarks like [[MMLU]],<ref>{{Cite web |title=Papers with Code - MMLU Benchmark (Multi-task Language Understanding) |url=https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu |access-date=2024-04-21 |website=paperswithcode.com |language=en}}</ref> MMMU,<ref>{{Citation |last1=Yue |first1=Xiang |title=MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI |date=2023-12-20 |arxiv=2311.16502 |last2=Ni |first2=Yuansheng |last3=Zhang |first3=Kai |last4=Zheng |first4=Tianyu |last5=Liu |first5=Ruoqi |last6=Zhang |first6=Ge |last7=Stevens |first7=Samuel |last8=Jiang |first8=Dongfu |last9=Ren |first9=Weiming}}</ref> HumanEval,<ref>{{Cite web |title=Papers with Code - HumanEval Benchmark (Code Generation) |url=https://paperswithcode.com/sota/code-generation-on-humaneval |access-date=2024-04-21 |website=paperswithcode.com |language=en}}</ref> and GSM8K.<ref>{{Cite web |title=Papers with Code - GSM8K Benchmark (Arithmetic Reasoning) |url=https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k |access-date=2024-04-21 |website=paperswithcode.com |language=en}}</ref> Given that foundation models are multi-purpose, increasingly meta-benchmarks are developed that aggregate different underlying benchmarks. Examples include LM-Harness,<ref>{{Citation |title=EleutherAI/lm-evaluation-harness |date=2024-04-21 |url=https://github.com/EleutherAI/lm-evaluation-harness |access-date=2024-04-21 |publisher=EleutherAI}}</ref> BIG-Bench,<ref>{{Citation |last1=Srivastava |first1=Aarohi |title=Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models |date=2023-06-12 |arxiv=2206.04615 |last2=Rastogi |first2=Abhinav |last3=Rao |first3=Abhishek |last4=Shoeb |first4=Abu Awal Md |last5=Abid |first5=Abubakar |last6=Fisch |first6=Adam |last7=Brown |first7=Adam R. |last8=Santoro |first8=Adam |last9=Gupta |first9=Aditya}}</ref> HELM,<ref>{{Cite web |title=Holistic Evaluation of Language Models (HELM) |url=https://crfm.stanford.edu/helm/lite/latest/ |access-date=2024-04-21 |website=crfm.stanford.edu}}</ref> OpenLLM Leaderboard,<ref>{{Cite web |date=2023-11-09 |title=open-llm-leaderboard (Open LLM Leaderboard) |url=https://huggingface.co/open-llm-leaderboard |access-date=2024-04-21 |website=huggingface.co}}</ref> DecodingTrust,<ref>{{Cite web |title=DecodingTrust Benchmark |url=https://decodingtrust.github.io/ |access-date=2024-04-21 |website=decodingtrust.github.io}}</ref> and HEIM.<ref>{{Cite web |title=Holistic Evaluation of Image Models (HEIM) |url=https://crfm.stanford.edu/heim/latest/ |access-date=2024-04-21 |website=crfm.stanford.edu}}</ref>\n\nSince foundation models' utility depends on their own general capabilities and the performance of fine-tuned applications, evaluation must cover both metrics. Proper evaluation examines both a foundation model's downstream applications in aggregate and the direct properties the foundation model holds. To ensure further equity in evaluation, certain existing evaluation frameworks account for all adaptation resources, which leads to more informed analyses for the benefit of all stakeholders.<ref>{{Cite journal |last=Linzen |first=Tal |date=July 2020 |editor-last=Jurafsky |editor-first=Dan |editor2-last=Chai |editor2-first=Joyce |editor3-last=Schluter |editor3-first=Natalie |editor4-last=Tetreault |editor4-first=Joel |title=How Can We Accelerate Progress Towards Human-like Linguistic Generalization? |url=https://aclanthology.org/2020.acl-main.465 |journal=Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics |location=Online |publisher=Association for Computational Linguistics |pages=5210\u20135217 |doi=10.18653/v1/2020.acl-main.465|arxiv=2005.00955 }}</ref>\n\n== Supply chain ==\nFoundation models' general capabilities allow them to fulfill a unique role in the AI ecosystem,<ref>{{Cite web |title=Ecosystem Graphs for Foundation Models |url=https://crfm.stanford.edu/ecosystem-graphs/ |access-date=2024-02-13 |website=crfm.stanford.edu}}</ref> fueled by many upstream and downstream technologies.<ref name=\":1\" /> Training a foundation model requires several resources (e.g. data, compute, labor, hardware, code), with foundation models often involving immense amounts of data and compute (also referred to as computational power). Due to foundation models' large development costs and inexpensive adaptation requirements, the AI landscape has shifted to a small subset of AI companies making foundation models for downstream adaptation.<ref>{{Citation |last1=Vipra |first1=Jai |title=Market Concentration Implications of Foundation Models |date=2023-11-02 |arxiv=2311.01550 |last2=Korinek |first2=Anton}}</ref> Thus, most foundation model companies outsource this step to specialized data providers (e.g. Scale AI,<ref>{{Cite web |title=Accelerate the Development of AI Applications {{!}} Scale AI |url=https://scale.com/ |access-date=2024-04-21 |website=scale.com |language=en}}</ref> Surge<ref>{{Cite web |title=Surge AI {{!}} World's Most Powerful Data Labeling Platform |url=https://www.surgehq.ai// |access-date=2024-04-21 |website=www.surgehq.ai |language=en}}</ref>) and compute providers (e.g. [[Amazon Web Services]], [[Google Cloud]], [[Microsoft Azure]]).\n[[File:Estimated training cost of some AI models - 2024 AI index.jpg|upright=1.5|thumb|Investment in computing capabilities to train larger AI models has rapidly increased.<ref>{{Cite web |date=April 15, 2024 |title=2024 AI Index - chapter 1 |url=https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter1.pdf |pages=37\u201339}}</ref>]]\nThe foundation model developer itself will then take the data and use the supplied compute to actually train the foundation model. After the foundation model is completely built, much of the data and labor requirements abate. In this development process, hardware and compute are the most necessary, and also the most exclusive resources. To train larger and more complex AI, a sufficient amount of compute is key. However, compute is consolidated in the hands of a few, select entities, which most foundation model developers depend on. As such, the foundation model pipeline is concentrated heavily around these providers. Compute is also costly; in 2023, AI companies spent more than 80% of total capital on compute resources.<ref>{{Cite web |last=pnp |date=2023-09-27 |title=Computational Power and AI |url=https://ainowinstitute.org/publication/policy/compute-and-ai |access-date=2024-02-13 |website=AI Now Institute |language=en-US}}</ref>\n\nFoundation models require a large amount of general data to power their capabilities. Early foundation models scraped from subsets of the internet to provide this data information. As the size and scope of foundation models grows, larger quantities of internet scraping becomes necessary, resulting in higher likelihoods of biased or toxic data. This toxic or biased data can disproportionately harm marginalized groups and exacerbate existing prejudices.<ref>{{Cite news |last1=Tiku |first1=Nitasha |last2=Schaul |first2=Kevin |last3=Chen |first3=Szu Yu |title=These fake images reveal how AI amplifies our worst stereotypes |url=https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/ |access-date=2024-02-13 |newspaper=Washington Post |language=en}}</ref>\n\nTo address this issue of low-quality data that arose with unsupervised training, some foundation model developers have turned to manual filtering. This practice, known as data labor, comes with its own host of issues.<ref>{{Cite web |title=How the AI industry profits from catastrophe |url=https://www.technologyreview.com/2022/04/20/1050392/ai-industry-appen-scale-data-labels/ |access-date=2024-02-13 |website=MIT Technology Review |language=en}}</ref> Such manual data detoxification is often outsourced to reduce labor costs, with some workers making less than $2 per hour.<ref>{{Cite magazine |date=2023-01-18 |title=Exclusive: The $2 Per Hour Workers Who Made ChatGPT Safer |url=https://time.com/6247678/openai-chatgpt-kenya-workers/ |access-date=2024-02-13 |magazine=TIME |language=en}}</ref>\n\nThe foundation model will then be hosted online either via the developer or via an external organization. Once released, other parties can create applications based on the foundation model, whether through fine-tuning or wholly new purposes. People can then access these applications to serve their various means, allowing one foundation model to power and reach a wide audience.\n\n== Release strategies ==\nAfter a foundation model is built, it can be released in one of many ways. There are many facets to a release: the asset itself, who has access, how access changes over time, and the conditions on use.<ref>{{Cite web |last1=Liang |first1=Percy |last2=Bommasani |first2=Rishi |last3=Creel |first3=Kathleen |date=May 17, 2022 |title=The Time is Now to Develop Community Norms for the Release of Foundation Models |url=https://crfm.stanford.edu/2022/05/17/community-norms.html |website=Stanford CRFM}}</ref> All these factors contribute to how a foundation model will affect downstream applications.<ref>{{Citation |last=Solaiman |first=Irene |title=The Gradient of Generative AI Release: Methods and Considerations |date=2023-02-05 |arxiv=2302.04844 |author-link=Irene Solaiman}}</ref> In particular, the two most common forms of foundation model release are through APIs and direct model downloads.\n\nWhen a model is released via an [[API]], users can query the model and receive responses, but cannot directly access the model itself. Comparatively, the model could be directly downloadable for users to access and modify. Both release strategies are often classified as an open release. The exact definition of an open release is disputed, but widely accepted requirements are provided by the [[Open Source Initiative]].\n\nSome open foundation models are: [[PaLM|PaLM 2]], [[LLaMA|Llama 2]], [[IBM Granite|Granite]], and [[Mistral AI|Mistral]]. While open foundation models can further research and development more easily, they are also more susceptible to misuse. Open foundation models can be downloaded by anyone, and particularly powerful models can be fine-tuned to intentionally or unintentionally cause harm.\n\nDuring a closed release, the foundation model cannot be accessed by the public, but is used internally by an organization. Such releases are considered safer, but offer no additional value to the research community or the public at large.\n\nSome foundation models like [[Google DeepMind]]'s Flamingo<ref>{{Citation |last1=Alayrac |first1=Jean-Baptiste |title=Flamingo: a Visual Language Model for Few-Shot Learning |date=2022-11-15 |arxiv=2204.14198 |last2=Donahue |first2=Jeff |last3=Luc |first3=Pauline |last4=Miech |first4=Antoine |last5=Barr |first5=Iain |last6=Hasson |first6=Yana |last7=Lenc |first7=Karel |last8=Mensch |first8=Arthur |last9=Millican |first9=Katie}}</ref> are fully closed, meaning they are available only to the model developer; others, such as [[OpenAI]]'s [[GPT-4]], are limited access, available to the public but only as a [[Black box (disambiguation)|black box]]; and still others, such as [[Meta Platforms|Meta]]'s Llama 2 are open, with broadly available model weights enabling downstream modification and scrutiny.\n\n==References==\n{{reflist}}\n\n{{Natural Language Processing}}\n{{Differentiable computing}}\n{{Existential risk from artificial intelligence}}\n\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Computational fields of study]]\n[[Category:Language modeling]]\n[[Category:Unsupervised learning]]\n[[Category:Deep learning]]"}